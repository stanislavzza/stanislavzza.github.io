<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="David Eubanks">

<title>Introduction – The Kappa Zoo</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-fcd204c8655bd031ced4918abb783b1b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-XP6WMESY52"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-XP6WMESY52', { 'anonymize_ip': true});
</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">
      The Kappa Zoo
      </li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">The Kappa Zoo</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-intro" id="toc-sec-intro" class="nav-link active" data-scroll-target="#sec-intro"><span class="header-section-number">1</span> Quantifying Agreement</a>
  <ul class="collapse">
  <li><a href="#sec-intro-overview" id="toc-sec-intro-overview" class="nav-link" data-scroll-target="#sec-intro-overview"><span class="header-section-number">1.1</span> Overview</a></li>
  <li><a href="#sec-intro-knowledge" id="toc-sec-intro-knowledge" class="nav-link" data-scroll-target="#sec-intro-knowledge"><span class="header-section-number">1.2</span> Seeking Knowledge</a></li>
  <li><a href="#sec-intro-confusion" id="toc-sec-intro-confusion" class="nav-link" data-scroll-target="#sec-intro-confusion"><span class="header-section-number">1.3</span> The Confusion Matrix</a></li>
  <li><a href="#sec-intro-agreement" id="toc-sec-intro-agreement" class="nav-link" data-scroll-target="#sec-intro-agreement"><span class="header-section-number">1.4</span> Agreement Statistics</a></li>
  <li><a href="#chance-correction" id="toc-chance-correction" class="nav-link" data-scroll-target="#chance-correction"><span class="header-section-number">1.5</span> Chance Correction</a></li>
  </ul></li>
  <li><a href="#the-kappa-zoo" id="toc-the-kappa-zoo" class="nav-link" data-scroll-target="#the-kappa-zoo"><span class="header-section-number">2</span> The Kappa Zoo</a></li>
  <li><a href="#machine-learning" id="toc-machine-learning" class="nav-link" data-scroll-target="#machine-learning"><span class="header-section-number">3</span> Machine Learning</a></li>
  <li><a href="#sec-intro-tap" id="toc-sec-intro-tap" class="nav-link" data-scroll-target="#sec-intro-tap"><span class="header-section-number">4</span> The t-a-p model</a>
  <ul class="collapse">
  <li><a href="#example-wine-judging" id="toc-example-wine-judging" class="nav-link" data-scroll-target="#example-wine-judging"><span class="header-section-number">4.1</span> Example: Wine Judging</a></li>
  </ul></li>
  <li><a href="#so-called-truth" id="toc-so-called-truth" class="nav-link" data-scroll-target="#so-called-truth"><span class="header-section-number">5</span> So-called Truth</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>David Eubanks </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="sec-intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Quantifying Agreement</h1>
<p>This chapter gives a brief background on rater agreement with a focus on the goals of the project and how legacy methods fall short. It contrasts psychological measurement methods to machine learning algorithms. A general model is introduced that combines the best features of both of those cultures, and an example is given.</p>
<section id="sec-intro-overview" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="sec-intro-overview"><span class="header-section-number">1.1</span> Overview</h2>
<p>Humans often use consensus to assess knowledge. In the introduction to an article on this topic <span class="citation" data-cites="dawid1979maximum">Dawid &amp; Skene (<a href="#ref-dawid1979maximum" role="doc-biblioref">1979</a>)</span> describe problems in using such consensus.</p>
<blockquote class="blockquote">
<p>When a patient’s historyis taken by different clinicians, different replies may be obtained to the same question. This may occur for a number of reasons; perhaps slightly different wording is used in each case, or perhaps the question is one the patient finds difficult to answer satisfactorily and so changes his reply from time to time. Similarly, in classifying a facet (sign or symptom) for type, severity, extent or duration, the patient and the clinicians may have different interpretations of the underlying scale of measurement. Such facets are said to be subject to observer error in that the response recorded may not be the “true” response as defined by some standard description of the facet or as implied by a consensus of medical opinion.</p>
</blockquote>
<p>We might seek a second opinion for a diagnosis, ask around for restaurant recommendations, or look for online reviews of a product. Intiuitively, we put more weight on opinions that have more agreement. This chapter describes how agreement or disagreement can be quantified. As in the quote above, we may have an idea of metaphysical truth that is obscured by error in the reported observations.</p>
<p>In behavioral science, the focus of these statistics is on human observers whom we’ll call “raters.” Raters independently assign “ratings” to “subjects.” This describes a familiar case like a product review with a five-star rating system (although those ratings may not be independent). However, the same idea applies to any categorization task, like classifying images of cats and dogs, or diagnosing diseases from medical images. A fascinating example comes from an article in <a href="https://www.theatlantic.com/magazine/archive/2024/09/decoding-voynich-manuscript/679157/">The Atlantic Magazine</a>, here describing an analysis of a very old text called the <a href="https://voynich.nu/index.html">Voynich manuscript</a>.</p>
<blockquote class="blockquote">
<p>Davis magnified the handwriting, she noticed subtle variations. In certain places, the script was more cramped or more likely to slant as it crossed the page. She tested this observation by picking a letter that didn’t appear often and tracking it across the manuscript’s pages. Its style, she saw, varied among groups of pages but not within those groups. This suggested that the differences—larger or smaller loops, straighter or curvier crossbars, longer or shorter feet—were the product of different scribes rather than of one scribe writing the same letter in different ways.</p>
</blockquote>
<p>See <a href="https://ceur-ws.org/Vol-3313/keynote2.pdf">Davis’s paper</a> for details<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. For our purposes, the “raters” are scholars who categorize writing styles, the “subjects” are sections (folios) of the manuscript they are reviewing, and the presumed different scribes are the categories we generically refer to as “ratings.”</p>
<div id="tbl-manuscript" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-manuscript-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Selected rows from a table at the website for the manuscript: <a href="https://voynich.nu/index.html" class="uri">https://voynich.nu/index.html</a>, comparing an early attempt to classify writing “hands” (Currier) to the recent classification (Davis). Hypothesized identity is unique to each column with the numbers denoting individual scribes.
</figcaption>
<div aria-describedby="tbl-manuscript-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Folio(s)</th>
<th>Prescott Currier</th>
<th>Lisa Fagin Davis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>f41, f48, f57r</td>
<td>2</td>
<td>5</td>
</tr>
<tr class="even">
<td>fRos (obverse)</td>
<td>3</td>
<td>2</td>
</tr>
<tr class="odd">
<td>fRos (main)</td>
<td>3</td>
<td>4</td>
</tr>
<tr class="even">
<td>f87, f88, f93, 96</td>
<td>4</td>
<td>1</td>
</tr>
<tr class="odd">
<td>f94, f95</td>
<td>5</td>
<td>3</td>
</tr>
<tr class="even">
<td>f103, f104, f106</td>
<td>X</td>
<td>3</td>
</tr>
<tr class="odd">
<td>f105</td>
<td>Y</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>In <a href="#tbl-manuscript" class="quarto-xref">Table&nbsp;1</a> it’s recorded that Currier thought that the obverse and main folio fRos were in a single hand, whereas Davis has two scribes. Conversely, Davis has the last three rows as the same author while Currier has three. This is an example of how science works toward consensus by placing observations into categories. Agreements are good for pointing to what might be real, and disagreements are for honing or discarding theories.</p>
<p>The statistical question for data like this is to compare variation within groups to variation between groups. One application is an idea called “signal-to-noise ratio,” which is a measure of how much of the variation in the data is due to the thing we’re interested in (signal) versus how much is due to random variation (noise). In the manuscript example, the signal is the commonality of handwriting within contiguous sections, and the noise is variation within each section. Reliability of measures is the same idea in different clothing, where we might formulate reliability as between-subject variation divided by total variation <span class="citation" data-cites="shrout1979intraclass">(<a href="#ref-shrout1979intraclass" role="doc-biblioref">Shrout &amp; Fleiss, 1979</a>)</span>. As an example, college grade averages (GPA) can have a reliability of about 0.8, meaning that 80% of the variation in grades is due to differences between students, and 20% is due to variation in how individual students earn grades. This makes GPA useful as a statistical measure, for example in predicting career outcomes.</p>
<p>The signal-to-noise approach usually treats a rating as an ordinary number (scalar), so a 1-5 rating scale is placed on the number line as a measure. If we start from the idea of rater agreement, however, this number-line approach is not as useful as counting agreement on categories. In the development below, we’ll be concerned with ratings as categories, and usually binary choices between some Class 1 or Class 0, e.g.&nbsp;“Does this patient have Covid?”. It is useful to start with the idea of a “true” classification.</p>
</section>
<section id="sec-intro-knowledge" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="sec-intro-knowledge"><span class="header-section-number">1.2</span> Seeking Knowledge</h2>
<p>It turns out that the questions “what is true?” and “what is knowledge, anyway?” are difficult to answer. There is a great body of work in epistemology, the philosophical study of knowledge, from which we can take a clever idea to get started.</p>
<p>To start, we have to imagine that “truth” exists. Since we are talking about classifications of subjects into one of two categories, we must commit to the idea that these categories (words that humans made up) <em>might</em> correspond to something in objective reality (whatever that is). We can easily make up descriptions that are fantasies, like unicorns or flying whales, and the goal is to have a way to distinguish these phantasms from real things like penguins and taxes. To do so, we have to imagine that by some mysterious means, our intellects and discourse can access Nature and name things.</p>
<p>Words are slippery things, however. Although there are definitions, the real meaning of “pizza” is (ironically) ineffable; one could write <a href="https://www.amazon.com/Modernist-Pizza-Nathan-Myhrvold/dp/1734386126">volumes</a> on the subject. Let’s take Platos’s definition of a man as a featherless biped. Our goal is a set of classifications “man” or “not man” generated from independent observers (we call them raters) who each inspect some objects in the world (we call them subjects) and assign “man” or “not man” to each. Then we’ll take that data and apply the between- and within-variance idea.</p>
<p>Therefore the definition isn’t enough. We also need a procedure for generating the data the way we want. To ensure independence of raters, we don’t want them to collaborate on decisions, for example. In the sciences, procedures can be quite elaborate, as in detecting and classifying Higgs bosons, which requires a huge particle accelerator.</p>
<p>The story is that <a href="https://en.wikipedia.org/wiki/Diogenes">Diogneses the Cynic</a> tossed a plucked chicken at Plato’s feet and declared “here’s your man.” The example drives a wedge between the definition “featherless biped” and what is actually meant by “man.” Outside of math and science, definitions and procedures are fuzzy in this way. Rater agreement statistics attempt to defuzz the ratings, to understand the reality behind them.</p>
<p>Here’s where the idea from epistemology is key, because it gets to the nature of the fuzziness of classification. First we define knowledge as <a href="https://plato.stanford.edu/entries/knowledge-analysis/">justified true belief</a> (JTB), and then sort out where the fuzziness is.</p>
<p>There are three parts to JTB, and we already talked about truth; as a prerequisite we must believe that true statements can be made about the world, which means accessing something real and describing it. The third piece of JTB is belief, which we’ll take to be the classification assigned by a rater. If our rater looks at a horse and writes down “not unicorn,” we’ll assume that the classification represents what the rater believes. This runs into problems if we want to consider adversarial raters, like someone paid to rate consumer products, or an angry customer rating a service poorly for extraneous reasons. We won’t consider those possibilities here.</p>
<p>The first piece of JTB is where the fuzziness lives: justification. Outside of math, justification in words isn’t complete<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. In the case of Diogenes versus Plato, an follower of the latter might reason that the plucked chicken meets the great philosopher’s definition, and therefore is a man. Others might use common sense to conclude that it’s not a man. There is more than one way to justify belief, but not all of them result in true classifications.</p>
<p>The JTB idea was dealt a blow by <a href="https://plato.stanford.edu/entries/knowledge-analysis/#GettProb">Gettier</a>, who posed a number of examples where justification goes awry, but the idea is much older. Consider this example from Dharmottara in 779 AD<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<blockquote class="blockquote">
<p>A desert traveller is searching for water. He sees, in the valley ahead, a shimmering blue expanse. Unfortunately, it’s a mirage. But fortunately, when he reaches the spot where there appeared to be water, there actually is water, hidden under a rock. Did the traveller <em>know</em>, as he stood on the hilltop hallucinating, that there was water ahead?</p>
</blockquote>
<p>The traveller had a belief that turned out to be true, but the justification was incorrect. The philosphers are concerned with whether or not this constitutes knowledge<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, but we’re more interested in something simpler. That is to categorize the ways truth and belief can happen. If they don’t coincide (if we believe something false), then we’ll assume that the justification was incorrect. As the Gettier problems and the discussion they generated demonstrate, there are a lot of complicated ways in which justification can go awry.</p>
<p>In the literature of rater agreement, one can find similar language. In <span class="citation" data-cites="aickin1990maximum">Aickin (<a href="#ref-aickin1990maximum" role="doc-biblioref">1990</a>)</span>, we find “The <span class="math inline">\(\alpha\)</span> agreement parameter is defined as the proportion of a population of items that are classified identically ‘for cause’ by two classifiers, the remaining items being classified at random […].” The author suggests that some subjects are more difficult to rate than others, which hints at modeling each subject’s probability of being in a category, <em>in truth</em>. This idea was explored in <span class="citation" data-cites="dawid1979maximum">Dawid &amp; Skene (<a href="#ref-dawid1979maximum" role="doc-biblioref">1979</a>)</span>, who also allowed that raters might have different proficiencies.</p>
<p>Another instance comes from the derivation of a rater agreement statistic called AC1 <span class="citation" data-cites="gwet2008computing">Gwet (<a href="#ref-gwet2008computing" role="doc-biblioref">2008</a>)</span>. The author notes that “the assumption [is] that any rating that is not random will automatically lead to a correct classification, while a random rating leads to a correct classification with probability 1/2.” The idea is that we conceptually separate accurate ratings from random ones, which then means we must consider the probability distribution of the random ones. The random assignments will sometimes be correct, but for the wrong reason–just like the Gettier problems.</p>
</section>
<section id="sec-intro-confusion" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="sec-intro-confusion"><span class="header-section-number">1.3</span> The Confusion Matrix</h2>
<p>Diagnosing illness is an important classification problem, and the simplest version is a blood test that returns a positive or negative result, perhaps scanning for an antibody. There are four possibilities, which can be organized into what is commonly called a <em>confusion matrix.</em> Suppose that we consider a statement S about the world, like “this patient has the flu.” Using a definition and process we reach a conclusion, so that we believe S to be true or false. This generates the classifications we’re interested in: this is what the raters do, generating lots of S or not-S data on cases.</p>
<p>The truth is often inaccessible, but recall that we assume that it exists, so that the statement really is true or false: each patient really does have the flu or does not. Here are the four possibilities:</p>
<div id="tbl-confusion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-confusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Sample confusion matrix
</figcaption>
<div aria-describedby="tbl-confusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 36%">
<col style="width: 31%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>S is true</th>
<th>S is false</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Believe S is true</strong></td>
<td>True positive cases</td>
<td>False positive cases</td>
</tr>
<tr class="even">
<td><strong>Believe S is false.</strong></td>
<td>False negative cases</td>
<td>True negative cases</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Notice that there are four spots in the table where data can go: if we had perfect knowledge we could count the number of true positive cases, etc. If we converted these numbers to proportions by dividing by the table, we have three parameters left. This foreshadows what comes later: the heart of the classification problem (in its simple form) is a three-parameter problem<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>The limitation of casting the confusion matrix into a single-parameter statistic was noted by <span class="citation" data-cites="cicchetti1990high">Cicchetti &amp; Feinstein (<a href="#ref-cicchetti1990high" role="doc-biblioref">1990</a>)</span>, who suggested “two separate indexes of proportionate agreement in the observers’ positive and negative decisions. […] Why do we want a single index?” Another example comes from the motivation for the AC1 rater statistic <span class="citation" data-cites="gwet2008computing">Gwet (<a href="#ref-gwet2008computing" role="doc-biblioref">2008</a>)</span>. It separately considers rater true positives and true negatives, but then assumes that these are identical for each rater, so that the result is a single parameter.</p>
<p>As a concrete illustration, consider the wine judging data used in <span class="citation" data-cites="hodgson2008examination">Hodgson (<a href="#ref-hodgson2008examination" role="doc-biblioref">2008</a>)</span> (data from the author in personal communication). The first five rows look like this:</p>
<div id="tbl-wine-data" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-wine-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Wine ratings showing rows 1-5 of 183, taken from the study by Hodgson (personal communication).
</figcaption>
<div aria-describedby="tbl-wine-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Wine</th>
<th>J1</th>
<th>J2</th>
<th>J3</th>
<th>J4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr class="even">
<td>2</td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr class="odd">
<td>3</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>4</td>
</tr>
<tr class="even">
<td>4</td>
<td>3</td>
<td>4</td>
<td>3</td>
<td>1</td>
</tr>
<tr class="odd">
<td>5</td>
<td>4</td>
<td>2</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The 1-4 scale is ordinal, meaning that we imagine quality to increase with the numerical value. For this discussion, we will reduce the scale to a binary classification: acceptable wine (ratings 2-4) or wine to avoid (rating 1). This simplifies the table to.</p>
<div id="tbl-wine-binary" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-wine-binary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Simplified wine ratings, where 1 means “acceptable” and 0 means “avoid.”
</figcaption>
<div aria-describedby="tbl-wine-binary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Wine</th>
<th>J1</th>
<th>J2</th>
<th>J3</th>
<th>J4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>4</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>5</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Intuitively there is more reason to believe wines 1 and 2 are acceptable than with 3 or 4, for which one judge found them not metal-worthy. It seems safe to avoid wine 5, since all but one of the judges found it not to be metal-worthy.</p>
<p>We can put this information into the confusion matrix, but we’re missing information.</p>
<div id="tbl-wine-confusion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-wine-confusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: Confusion matrix for the first five wines, with missing information as question marks.
</figcaption>
<div aria-describedby="tbl-wine-confusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>true value of 2-4</th>
<th>true value of 1</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>rating of 2-4</strong></td>
<td>?</td>
<td>?</td>
<td>15</td>
</tr>
<tr class="even">
<td><strong>rating of 1</strong></td>
<td>?</td>
<td>?</td>
<td>5</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>There is some awkwardness in imagining that the physical universe encompasses a reality including true wine ratings. It seems absurd on the face of it that the physical laws of the universe imply “wine science.” It’s useful to think of it in reverse: this reification of the ratings is a <em>hypothesis</em> that may be rejected by the statistics we derive from the data. If we find that raters are assigning ratings at random, there’s no evidence here for the reality of wine asthetics. On the other hand, strong indications of non-randomness need to be explained somehow.</p>
<p>Taking the leap that all non-randomness is causal, and physical reality has a monopoly on causality, then <em>something</em> concrete in the world is behind the rating consistency. We could imagine the chemical similarities and differences of wines and their effects on human anatomy, and so on. Being “real” in this case doesn’t mean that the universe appreciates wine, only that the universe includes physical blobs called humans, and they have mechanical consistencies that correlate observations in this case. The confusion matrix is also used for calculating the causal effect of an experiment. In a medical context, the rows might be treatment categories and the columns patient outcomes. See <span class="citation" data-cites="eubankscause">Eubanks (<a href="#ref-eubankscause" role="doc-biblioref">2014</a>)</span> for more on that.</p>
<p>By using the rating counts for each subject instead of the total, we can–surprisingly–estimate the whole confusion matrix. We can even estimate the true classification values for each subject. For that level of detail see <a href="./hierarchical.html">Chapter 4: Hierarchical Models</a>.</p>
</section>
<section id="sec-intro-agreement" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="sec-intro-agreement"><span class="header-section-number">1.4</span> Agreement Statistics</h2>
<p>Signal and noise as variability between and variability within subjects translates into rater agreement within subjects versus between subjects. The usual way to turn this idea into a number is to count the number of actual agreements and divide by the total number possible.</p>
<div id="tbl-agreement" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-agreement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6: Rater agreements count, showing the maximum agreements for N raters, the number matches that are evidence versus logical implications and the ratio of evidence to matches.
</figcaption>
<div aria-describedby="tbl-agreement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Number of raters</th>
<th>Maximum agreements</th>
<th>Evidentiary agreements</th>
<th>Implied agreements</th>
<th>Evidence / Maximum</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>2</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>3</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>.67</td>
</tr>
<tr class="even">
<td>4</td>
<td>6</td>
<td>3</td>
<td>3</td>
<td>.5</td>
</tr>
<tr class="odd">
<td>5</td>
<td>10</td>
<td>4</td>
<td>6</td>
<td>.4</td>
</tr>
<tr class="even">
<td>N</td>
<td><span class="math inline">\(N(N-1)/2\)</span></td>
<td><span class="math inline">\(N-1\)</span></td>
<td><span class="math inline">\(N(N-1)/2 - N + 1\)</span></td>
<td><span class="math inline">\(2/N\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The maximum agreements in the table come from the combinatorics formula “N choose 2,” counting the number of possible pairs out of N. If a group of 5 people meet and all shake hands, it’s 10 handshakes. The number of pairs grow much more rapidly than the number of raters does<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
<p>There’s a rule in thumb in statistics that it’s preferable to use standard deviation instead of variance when understanding the data. That’s because the units of standard deviation are the same as the original data, whereas the variance has squared units. The same effect is going on with counting agreement. In some sense, what we want looks more like the square root of agreement.</p>
<p>Intuitively, three raters agreeing doesn’t seem like three times as much evidence as two agreeing. And if we look at where those agreements are coming from, most of them are purely logical. Agreement is transitive, meaning if A agrees with B and B agrees with C, then A agrees with C. This is a logical necessity, and it’s not evidence of anything. The Evidentiary and Implied columns of the table record the distinction between the two types of matches. As the number of raters <span class="math inline">\(N\)</span> increases, the logically necessary agreements greatly outnumber the evidentiary ones, so the ratio in the last column of the table decreases toward zero.</p>
<p>If we only counted the evidentiary agreements, the maximum for <span class="math inline">\(N\)</span> raters would be <span class="math inline">\(N - 1\)</span>, which is roughly proportional to the square root of the total number of agreements. We’ll come back to that idea later. However, the well-trodden path is to estimate the probability of agreement over all the subjects, which leads to the kappa statistics.</p>
<p>For the five wine ratings that are reduced to the binary category “avoid” or “don’t avoid” in <a href="#tbl-wine-binary" class="quarto-xref">Table&nbsp;4</a>, we can calculate the agreement as follows.</p>
<div id="tbl-wine-agreement" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-wine-agreement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7: Agreement calculation for the first five wines, showing the maximum possible agreements, the actual agreements, and the agreement proportion out of the maximum.
</figcaption>
<div aria-describedby="tbl-wine-agreement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Wine</th>
<th>Raters</th>
<th>Possible</th>
<th>Actual</th>
<th>Agreement</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>4</td>
<td>6</td>
<td>6</td>
<td>1</td>
</tr>
<tr class="even">
<td>2</td>
<td>4</td>
<td>6</td>
<td>6</td>
<td>1</td>
</tr>
<tr class="odd">
<td>3</td>
<td>4</td>
<td>6</td>
<td>3</td>
<td>.5</td>
</tr>
<tr class="even">
<td>4</td>
<td>4</td>
<td>6</td>
<td>3</td>
<td>.5</td>
</tr>
<tr class="odd">
<td>5</td>
<td>4</td>
<td>6</td>
<td>3</td>
<td>.5</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>For the fifth vintage in <a href="#tbl-wine-agreement" class="quarto-xref">Table&nbsp;7</a>, there are more matches with the zeros than there are with the ones; both are counted. It is at this point what seemed like a good starting point begins to turn into a muddle, because since both the zeros and the ones can create agreement, how do we know what’s <em>good enough</em> agreement? What’s the worst agreement possible?</p>
<p>A group of <span class="math inline">\(n_1\)</span> raters who agree on the 1 ratings produces a number of agreements about proportional to <span class="math inline">\(n_1^2\)</span>, and similarly the <span class="math inline">\(n_0\)</span> raters of 0 agreements produce about <span class="math inline">\(n_0^2\)</span> agreements. Together that’s around <span class="math inline">\(n_1^2 + n_0^2\)</span> agreements, which is less than or equal to <span class="math inline">\((n_1 + n_0)^2\)</span>, which is about the number of agreements if everyone agreed on a single category. So we get more agreement when everyone agrees than we do when the raters split into groups. That means that the <em>least</em> agreement happens when the raters are evenly split, giving us a floor for agreement.</p>
<div id="tbl-min-agreement" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-min-agreement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8: Minimum rater agreement rates
</figcaption>
<div aria-describedby="tbl-min-agreement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Number of raters</th>
<th>Maximum agreements</th>
<th>Minimum agreements</th>
<th>Minimum rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>4</td>
<td>6</td>
<td>2</td>
<td>.33</td>
</tr>
<tr class="even">
<td>6</td>
<td>15</td>
<td>6</td>
<td>.40</td>
</tr>
<tr class="odd">
<td>8</td>
<td>28</td>
<td>12</td>
<td>.43</td>
</tr>
<tr class="even">
<td>10</td>
<td>45</td>
<td>20</td>
<td>.44</td>
</tr>
<tr class="odd">
<td>even N</td>
<td><span class="math inline">\(N(N-1)/2\)</span></td>
<td><span class="math inline">\(N(N/2-1)/2\)</span></td>
<td><span class="math inline">\((N/2-1)/(N - )\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>For six or more raters, there’s at least a 40% agreement rate, even when there’s the least possible amount of agreement. It’s annoying that the minimum rate of agreement changes with the number of raters. For real cases when we want to use these ideas, we might have different numbers of raters for different subjects, complicating an average minimum rate, which defines the worst case agreement.</p>
</section>
<section id="chance-correction" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="chance-correction"><span class="header-section-number">1.5</span> Chance Correction</h2>
<p>We saw in the previous section that the minimum frequency of agreement among raters is quite large, tending toward one half of the maximum possible. Whatever statistic we create as a measure of agreement needs to take that into account if we are to make sense of the results. We’ll get maximum agreement if all the raters choose the same category (of the two choices), and we’ll get minimum agreement if they split evenly between the categories, as if they each flipped a coin to decide. So there’s an association between worst-case random raters and the minimum agreement rate. This is the basis of the chance correction in the S statistic, which assumes coin-flipping raters as the worst-case benchmark from which to measure agreement.</p>
<p>The general formula for taking into account a worst-case (random) match rate <span class="math inline">\(m_r\)</span> first computes the practical range of our scale, which is <span class="math inline">\(1 - m_r\)</span>, since the statistic can’t be less than <span class="math inline">\(m_r\)</span>. The amount of agreement observed <span class="math inline">\(m_o\)</span> is then pinned to this scale as a “chance-corrected” match rate <span class="math inline">\(\kappa\)</span> (kappa) with</p>
<p><span class="math display">\[\kappa = \frac{m_o - m_r}{1 - m_r}\]</span></p>
<p>The S statistic is a special case of this formula, where <span class="math inline">\(m_r = .5\)</span>. The sample of wine ratings can be used to illustrate. Recall that earlier we simplified the judges’ scale of “no medal” to “gold medal” to a binary classification between “no medal” (coded as zero) or “any medal” (coded as one). There were 30 maximum agreements among the four raters over the five wines (six per wine), and 21 actual agreements. The minimum is when the ratings are evenly split on each case, generating two agreements for each wine, or 10 total. An S-like statistic would be calculated like this:</p>
<p><span class="math display">\[
\begin{aligned}
m_o &amp;= 21/30 = .70 \\
m_r &amp;= 10/30 = .33 \\
\kappa_s &amp;= \frac{.70 - .33}{1 - .33} = .47
\end{aligned}
\]</span> The kappa is now on a scale of zero (worse) to one (perfect agreement). Interpreting what the value means is difficult because it’s on the wrong scale as noted above. A value close to one half sounds mediocre, but not terrible, but we’re really looking for something like the square root of the agreement rate, which is .69. That value corresponds better to the intuition that the agreement in the table looks pretty good.</p>
<p>The coin-flipping assumption for randomness can be replaced with other assumptions. This has the effect of increasing the assumed worst-case agreement rate and hence decreasing kappa values. Perhaps the most popular choice is to reason that the raters are more likely to randomly pick the most common category so the coin should be weighted according to the rating distribution.</p>
<p>For example, the table of simplified wine ratings has 15 ratings of 1 and 5 of 0, or 75% 1-ratings. The worst case match rate is then the product of the proportions of 1 ratings, or <span class="math inline">\(m_r = (.75)(.75) + (.25)(.25) = .625\)</span>. This is the assumption of the Fleiss kappa. Note that the actual worst case match rate is still close to one half (because of the math), so it’s possible to get a negative Fleiss kappa if the assumption about the distribution of randomness fails.</p>
<p>One calculation under this proportionality assumption is:</p>
<p><span class="math display">\[
\begin{aligned}
m_o &amp;= 21/30 = .70 \\
m_r &amp;= .625 \\
\kappa_f &amp;= \frac{.70 - .625}{1 - .625} = .20
\end{aligned}
\]</span>The actual calculations of these statistics are complicated by the choice of using exact combinatorical counts or long-run estimates. This distinction need not concern us in here.</p>
<p>The deflation of the rater agreement statistic in the Fleiss calculation above makes it clear that assumptions about random agreement are critical. A researcher may justifiably wonder which value is correct: are the wine judges in pretty good agreement, or is the judging mostly random? For historical reasons, rather than treating rater disagreement as a parameter to be discovered, the field has evolved to propose many variations of the kappa formula with different assumptions about the worst-case agreement rate.</p>
<p>A limitation of most of the kappa statistics is that they make the simplifying assumption that a pair of raters reaches agreement randomly if <em>both</em> raters are random. This omits the possibility that one rater is accurate and the other isn’t.</p>
</section>
</section>
<section id="the-kappa-zoo" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> The Kappa Zoo</h1>
<p>There are by now several statistics advertised as rater agreement statistics, including Cohen’s kappa, Fleiss’s kappa, Scott’s pi, Krippendorff’s alpha, S, and AC1. For convenience, these statistics will be referred to generically as “kappas.” Researchers who want a simple answer are faced with a bewildering set of options and claims about them. In <span class="citation" data-cites="vach2023gwet">Vach &amp; Gerke (<a href="#ref-vach2023gwet" role="doc-biblioref">2023</a>)</span> we get a sense of the situation.</p>
<blockquote class="blockquote">
<p>Gwet’s AC1 has been proposed as an alternative to Cohen’s kappa in evaluating the agreement between two binary ratings. This approach is becoming increasingly popular, and researchers have been criticized for still using Cohen’s kappa. However, a rigorous discussion of properties of Gwet’s AC1 is still missing.</p>
</blockquote>
<p>The authors illustrate both the need for clarification and the faddishness that research communities can adopt when lacking real criteria. The kappas are ill-suited to answer such questions, and the proliferation of agreement statistics makes it difficult to compare results across studies or to know which one to use. Given pressures to publish results, there may be a tendency to use the statistic that gives the highest value, or to use the one that is most familiar. See <span class="citation" data-cites="button2020inter">Button et al. (<a href="#ref-button2020inter" role="doc-biblioref">2020</a>)</span> for a discussion of rater agreement statistics in the context of the “crisis in confidence in psychological research.”</p>
<p>It’s not just difficult to know what kappa to use, there are no meaningful guides to interpreting the results. “There is a wide distinction in the elucidation of Kappa values, and several efforts have been made to assign practical meaning to calculated Kappa values,” wrote the authors of <span class="citation" data-cites="chaturvedi2015evaluation">Chaturvedi &amp; Shweta (<a href="#ref-chaturvedi2015evaluation" role="doc-biblioref">2015</a>)</span>, who mention a widely-used heuristic found in <span class="citation" data-cites="landis1977measurement">Landis &amp; Koch (<a href="#ref-landis1977measurement" role="doc-biblioref">1977</a>)</span> that proposed a translation between numerical values of kappa and qualitative descriptions of agreement, such as <span class="math inline">\(\kappa \ge .81\)</span> is “almost perfect.” The categories are arbitrary, do not translate well between different agreement statistics (which can give different values for the same data) and their assumptions, and do not provide insight into how to improve ratings.</p>
<p>The agreement statistics for ratings have problems that should now be evident. The goal is to understand a three-dimensional relationship between ratings and true values, but the statistics are single parameters. A second parameter, the worst case baseline, is buried in an assumption, which varies by kappa, and is not tested for fit to the data. The result is a bewildering array of choices for rater agreement measures. As a result of this confusion, different cultures have emerged. If getting published is the goal, then higher agreement rates are more desirable, so a researcher can shop around for the “best” one.</p>
<p>A research agenda was suggested in <span class="citation" data-cites="landis1977measurement">Landis &amp; Koch (<a href="#ref-landis1977measurement" role="doc-biblioref">1977</a>)</span> that can be paraphrased as understanding (1) the true category of each subject, (2) the accuracy of raters, (3) truth and accuracy within sub-populations of subjects, (4) conditions that cause disagreement, and (5) what distinguishes “for cause” agreement and random agreement. Some of this can be accomplished by taking a fresh look at the kappas.</p>
</section>
<section id="machine-learning" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Machine Learning</h1>
<p>The geyser of data produced in the information age has led to new methods of analyzing it that fall generally in the description of “machine learning (ML).” Assigning categories to subjects is a common task in machine learning, and the field has developed a number of methods to do so, including neural networks, support vector machines, and random forests. The goal is to assign categories to subjects in a way that generalizes to new subjects. The methods are often evaluated by comparing the predicted categories to the true categories, and the results are summarized in a confusion matrix. See <span class="citation" data-cites="carpenter2008multilevel">Carpenter (<a href="#ref-carpenter2008multilevel" role="doc-biblioref">2008</a>)</span> for a good example of this literature.</p>
<p><span class="citation" data-cites="breiman">Breiman (<a href="#ref-breiman" role="doc-biblioref">2001</a>)</span> rather famously drew a line between classical statistics and machine learning algorithms, with the glove thrown down in the paper’s abstract:</p>
<blockquote class="blockquote">
<p>There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.</p>
</blockquote>
<p>There are differences in the methods and philosophies of the two cultures. The kappa approach is implicitly backward-looking, asking “how accurate were these ratings,” with the assumption that the measured accuracy (a kappa) will carry forward to future instances. The ML approach is more forward-looking, with a suite of tools like cross-validation and a vocabulary (bias-variance trade-off) to measure generalizability. As we have seen, the kappa approach is to reason out a plausible chance-correction calculation and use it to produce the single-parameter kappa. The ML approach is to estimate parameters with regression models. The kappas are grounded in the psychology of human classification and come with philosophical links to epistemology, which I described earlier in <a href="#sec-intro-knowledge" class="quarto-xref">Section&nbsp;1.2</a>. The ML algorithms are just statistics and code, usually barren of philosophical considerations.</p>
<p>For rater agreement algorithms, ML researchers typically use Bayesian models with explicit likelihood models, which one could think of as a classical statistical approach, albeit in a Bayesian context. The kappas, by contrast, are by statistical standards rather <em>ad hoc</em>. For example, we will see in the following chapters that the most popular kappas are equivalent to distributional models with implicit assumptions that are not declared or tested. For example, the Fleiss kappa implicitly assumes that raters are unbiased in a certain sense. This seems not to be known, which implies that the kappas have not been theoretically developed in the same way that, say, a <span class="math inline">\(t\)</span>-test has. In this context, the t-a-p models overlap are ML models in spirit, but with the added ingredient of the philosophical motivation that’s not always obvious in the ML literature, which can also seem <em>ad hoc</em> in model choices.</p>
<p>I learned in <span class="citation" data-cites="passonneau2014benefits">Passonneau &amp; Carpenter (<a href="#ref-passonneau2014benefits" role="doc-biblioref">2014</a>)</span> that ML-like algorithms for rater agreement date back at least to <span class="citation" data-cites="dawid1979maximum">Dawid &amp; Skene (<a href="#ref-dawid1979maximum" role="doc-biblioref">1979</a>)</span>, the title of which paper is “Maximum likelihood estimation of observer error-rates using the EM algorithm.” The abbreviation is for an “expectation-maximization” approach, which splits the unknown coefficients into two groups, and we alternately use estimates of one group to improve the estimates of the other. This is very much in the spirit of ML, even though it emerged from classical statistics as a hack to solve difficult maximum likelihood problems. While ML researchers seem to be aware of the kappas <span class="citation" data-cites="passonneau2014benefits">(<a href="#ref-passonneau2014benefits" role="doc-biblioref">Passonneau &amp; Carpenter, 2014</a>)</span>, I have not seen evidence of the reverse, for example in surveys of rates and proportions <span class="citation" data-cites="fleiss2013statistical">Fleiss et al. (<a href="#ref-fleiss2013statistical" role="doc-biblioref">2013</a>)</span> and <span class="citation" data-cites="agresti2003categorical">Agresti (<a href="#ref-agresti2003categorical" role="doc-biblioref">2003</a>)</span>.</p>
<p>The advantage of the ML approaches over the kappas is that the three parameter models avoid the confusions of the kappa zoo. But because they lack philosophical grounding, they run into an embarrassment of riches: three parameters is sometimes <em>too many</em> parameters for a model, so there can be multiple solutions. There are work-arounds, but it’s the same species of <em>ad hoc</em> reasoning that causes the problems of the kappa zoo. I propose that the t-a-p model approach, with its explicit epistemological foundation, combines the best of both cultures to avoid some of these problems.</p>
<p>In the following chapters, I will reference the relevant ML models as appropriate.</p>
</section>
<section id="sec-intro-tap" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> The t-a-p model</h1>
<p>We can now describe a regression model that allows us to estimate the four proportions that appear in the confusion matrix (see <a href="#sec-intro-confusion" class="quarto-xref">Section&nbsp;1.3</a>). Since the four cells sum to one, there are three free parameters to estimate.</p>
<table class="caption-top table">
<caption>: Confusion matrix with entries to be filled in by the model estimates. The four numbers are proportions and sum to one, leaving three free parameters to estimate. C0 = class zero, and C1 = class one, standing in for any binary categories we might choose.</caption>
<thead>
<tr class="header">
<th></th>
<th>True C1</th>
<th>True C0</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Classified C1</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Classified C0</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Two philosophical assumptions are needed to get started:</p>
<ol type="1">
<li><p>The classifications are real in the sense that true values exist.</p></li>
<li><p>The true values have a binary causal effect on the classification process.</p></li>
</ol>
<p>These assumptions are provisional like hypothesis in statistics; the evidence will provide some support for the assumptions, ranging from no support at all to quite good. The reality of the truth values posed in assumption one is not like Plato’s cave shadows. In the case of wine judging, we can’t say there is a universal ideal for good wine, but if rater agreement is high we can say that some physical causal process exists for translating the observable subject (tasting the wine) into a category. So in that sense the category exists as part of the world. The translation is imperfect, because the conditions are not always perfect for the cause to happen, as with the Gettier problems<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</p>
<p>This may seem fiddly, but it gives us a place to start. For any given subject to be rated (e.g.&nbsp;object to be classified) we provisionally assume that there’s a latent truth value that we can never know, but might find evidence for. The second assumption then allows us to provisionally assume that the causal effect of the true value in the context of the classification process has the following nature:</p>
<ul>
<li><p>The rater either assigns the correct class due to the causal pathway operating to connect the observation to the class (justified true knowledge), or</p></li>
<li><p>Something goes wrong with the causal pathway (the conditions weren’t quite right, etc.) and the classification is assigned non-causally, which is to say randomly.</p></li>
</ul>
<p>The key to this is that the cause either works to connect the true value to the rater’s assigned value, or it fails completely. There’s no “partial cause.” When it fails, the rating is generated from a random process called a Bernoulli trial. It’s the simplest possible type of randomness, taking only two values with some fixed probability, like flipping a weighted coin.</p>
<div class="cell" data-layout-align="default">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>%%{init: {'themeCSS': '.node text { font-family: cursive; }'}}%%</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>%%| label: fig-tap-concept</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: Conceptual map of t-a-p model</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>flowchart TB</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  A(Rating) --&gt; |"proportion a"|B(Accurate)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  A --&gt; |"proportion 1-a"|C(Inaccurate)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  B --&gt; |"proportion t"|D[True Class 1]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  B --&gt; |"proportion 1-t"|F[True Class 0]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  C --&gt; |"proportion 1-p"|H[Random Class 0]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  C --&gt; |"proportion p"|G[Random Class 1]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">%%{init: {'themeCSS': '.node text { font-family: cursive; }'}}%%
%%| label: fig-tap-concept
%%| fig-cap: Conceptual map of t-a-p model
flowchart TB
  A(Rating) --&gt; |"proportion a"|B(Accurate)
  A --&gt; |"proportion 1-a"|C(Inaccurate)
  B --&gt; |"proportion t"|D[True Class 1]
  B --&gt; |"proportion 1-t"|F[True Class 0]
  C --&gt; |"proportion 1-p"|H[Random Class 0]
  C --&gt; |"proportion p"|G[Random Class 1]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Let’s take inventory of the three parameters that come from the reasoning displayed in the diagram.</p>
<ul>
<li><p>Among the subjects being rated there is a fraction <span class="math inline">\(t\)</span> that are <em>in reality</em> Class 1, with the remaining <span class="math inline">\(1-t\)</span> being Class 0.</p></li>
<li><p>Among the ratings there is a fraction <span class="math inline">\(a\)</span> that are accurate ratings (justified true knowledge) where the causal connection worked.</p></li>
<li><p>For the remaining <span class="math inline">\(1-a\)</span> ratings, the causal connection failed (as with a Gettier problem), and there is some probability <span class="math inline">\(p\)</span> that describes the frequency that ratings are randomly assigned to Class 1. The remaining <span class="math inline">\(1-p\)</span> are assigned Class 0.</p></li>
</ul>
<p>All three of these parameters are proportions ranging from zero to one, and can be treated as probabilities that we estimate from appropriate regression models. Setting it up this way, instead of the usual machine learning parameterization avoids the most significant problem with non-identifiability (multiple solutions), which is class-switching. That happens when the model can fit the data, but is agnostic about which class is which, and so attempts to fit the model both ways at once.</p>
<div id="fig-tap-tree" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tap-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/simpletap.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tap-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: tap diagram
</figcaption>
</figure>
</div>
<p>The diagram in <a href="#fig-tap-tree" class="quarto-xref">Figure&nbsp;1</a> shows probabilistic links between states of the world and rater classifications for a single rating. Upper case letters will be used here for binary states, with</p>
<ul>
<li><span class="math inline">\(C_{ij}\)</span> being the rating assigned to the <span class="math inline">\(i\)</span>th subject by the <span class="math inline">\(j\)</span>th rater. The value of <span class="math inline">\(C_{ij}\)</span> is determined at the bottom of the diagram, contingient on the classification process.</li>
<li><span class="math inline">\(A_{ij}\)</span> is 1 if that rating was accurate (JTB), or 0 otherwise.</li>
<li><span class="math inline">\(T_i\)</span> is the true class (zero or one) of the <span class="math inline">\(i\)</span>th subject.</li>
<li><span class="math inline">\(P_{ij}\)</span> matters only if the <span class="math inline">\(i,j\)</span> rating was inaccurate, (<span class="math inline">\(A_{ij} = 0\)</span>). In that case, a random assignment of zero or one is made.</li>
</ul>
<p>These binary events are assumed to be independent of one another, except that the <span class="math inline">\(T_i\)</span> true classification is fixed over all ratings. We’ll also assume that each of these binary outcomes has a fixed average value. In the notation, these are lower-case letters corresponding to the ones in <a href="#fig-tap-tree" class="quarto-xref">Figure&nbsp;1</a>.</p>
<ul>
<li><span class="math inline">\(c\)</span> is the proportion of Class 1 ratings assigned.</li>
<li><span class="math inline">\(t\)</span> is the proportion of true Class 1 cases.</li>
<li><span class="math inline">\(a\)</span> is the fraction of classifications that are accurate.</li>
<li><span class="math inline">\(p\)</span> is the proportion of randomly-assigned classifications that are Class 1.</li>
</ul>
<p>These averages replace the individual binary states to comprise the average t-a-p model.</p>
<div id="fig-tap-avg" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tap-avg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/avgtap.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tap-avg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: tap diagram
</figcaption>
</figure>
</div>
<p>In <a href="#fig-tap-avg" class="quarto-xref">Figure&nbsp;2</a>, the variables <span class="math inline">\(c, a, t, p\)</span> are all probabilities, with a bar over the symbol to denote its complement, i.e.&nbsp;<span class="math inline">\(\bar{t} := 1-t\)</span>. At the end of each branch is the classification (zero or one) and its probability.</p>
<p>To illustrate the ideas here, consider a judge (the rater) tasting one of the wines in a competition (the subject). Because the classification is binary, assume that the rating is either “1 = acceptable” or “0 = not acceptable.” The model assumes that each wine being tasted has a true value of acceptability. Over all the wines, the proportion of acceptable wines is <span class="math inline">\(t\)</span>. Suppose this one is, in fact, acceptable, so that <span class="math inline">\(T_i = 1\)</span> in <a href="#fig-tap-tree" class="quarto-xref">Figure&nbsp;1</a>. There’s some probability <span class="math inline">\(a\)</span>, which we’ll call <em>accuracy</em>, that the judge’s perceptive powers will reveal this true quality of the wine. If this happens, then <span class="math inline">\(A_{ij] = 1\)</span>, recording an accurate rating, and the resulting classification is necessarily Class 1. That event would be tracing down the left side of the tree diagram, and over all judges and wines, the probability of that is <span class="math inline">\(at\)</span>. If the judge’s perspicacity desserts him, and he makes an inaccurate rating, this doesn’t mean he automatically gets the wrong answer! Recall the Gettier-like problems, where we can accidentally get the correct answer even though our reasoning is flawed. Instead, there’s a random chance <span class="math inline">\(p\)</span> of assigning a Class 1 rating. The overall probability that a random (inaccurate) Class 1 rating is assigned is the branch ending with <span class="math inline">\(\bar{a}p = (1-a)p\)</span> at bottom right in <a href="#fig-tap-avg" class="quarto-xref">Figure&nbsp;2</a>.</p>
<p>It’s a critical point that the true classifcation of each subject (<span class="math inline">\(T_i\)</span>) is the <em>same</em> regardless of who’s rating it. So when four judges all rate the same wine, they are all either on the left side of the diagram (if the wine is acceptable in reality) or on the right side (if not). The ratings are determined only by <span class="math inline">\(a\)</span> and <span class="math inline">\(p\)</span> at that point. It’s this commonality of truth that allows us to study within-subject variation versus between-subject variation.</p>
<p>For a wine chosen at random, we can compute the probabilities of rater classifications. An acceptable wine will be classified accurately with a proportion of <span class="math inline">\(ta\)</span>, multiplying the probabilities along the leftmost edge from top to bottom. An inaccurate rating of <span class="math inline">\(\hat{C_1}\)</span> (acceptability) can come from either the left or right side of the diagram, and if we add those together we get <span class="math inline">\(t\bar{a}p + \bar{t}\bar{a}p\)</span>, and since <span class="math inline">\(t + \bar{t} = 1\)</span> that expression reduces to <span class="math inline">\(\bar{a}p\)</span>.</p>
<p>Assuming we can estimate the three parameters from the data, we can then populate the confusion matrix by tracing the diagram down to each of the four outcomes, and multiplying probabilities as we go.</p>
<div id="tbl-tap-confusion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-tap-confusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9: The t-a-p model’s correspondence to the confusion matrix. Terms in parentheses are inaccurate ratings.
</figcaption>
<div aria-describedby="tbl-tap-confusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 28%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>True C1</th>
<th>True C0</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Classified C1</td>
<td><span class="math inline">\(ta + (t\bar{a}p)\)</span></td>
<td><span class="math inline">\((\bar{t}\bar{a}p)\)</span></td>
</tr>
<tr class="even">
<td>Classified C0</td>
<td><span class="math inline">\((t\bar{a}\bar{p})\)</span></td>
<td><span class="math inline">\(\bar{t}a + (\bar{t}\bar{a}\bar{p})\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The entries in <a href="#tbl-tap-confusion" class="quarto-xref">Table&nbsp;9</a> demonstrate that if the t-a-p model fits the data and we are able to estimate the three parameters, it is a general answer to the rater agreement question. The later sections show how S, Fleiss kappa, and other statistics are special cases of t-a-p models.</p>
<section id="example-wine-judging" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="example-wine-judging"><span class="header-section-number">4.1</span> Example: Wine Judging</h2>
<p>The sections above used five rows of the wine judging data for illustration. We now use the whole data set of 183 wines each rated by four judges to estimate the t-a-p parameters. The included app was used (see Chapter 7) for the analysis.</p>
<p>The binary classification illustrated above was to convert the original scale (1 = no medal, 2 = bronze, 3 = silver, 4 = gold medal) to a binary outcome where 1 = any medal (bronze, silver, gold), and 0 = no medal. We called the Class 1 category “acceptable.” Using the app to analyze that binary comparison we get the following output.</p>
<div id="fig-wine-binary-distro" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wine-binary-distro-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/wine tap.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wine-binary-distro-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Wine rating t-a-p analysis comparing C1 = ratings 1,2,3 (in-class) to C0 = rating 0.
</figcaption>
</figure>
</div>
<p>The acceptable wines are the primary (in-) class, and the plot shows the distribution of ratings for acceptability. Since there are four judges, the possibilities are:</p>
<ul>
<li><p>None of them could find the wine acceptable. This is the 0 at the left end of the plot. It happens for about 6% of the wines.</p></li>
<li><p>All four of them could find the wine acceptable. This is the 4 (four judges) at the right end of the plot. That happened for about 49% of the wines.</p></li>
<li><p>Something in between, where the judges disagree. In that case we can have from one to three ratings of “acceptable,” corresponding to those points on the plot.</p></li>
</ul>
<p>The lollipops (black lines with dots on top) in <a href="#fig-wine-binary-distro" class="quarto-xref">Figure&nbsp;3</a> show the empirical data straight out of the ratings. The dashed blue line is the model fit, showing what the distribution would be if the model were exactly correct. Here, “model” means applying the estimated parameters to generate the distribution.</p>
<p>The estimated parameters are found at the top of the plot:</p>
<ul>
<li><p><span class="math inline">\(t\)</span> = .73 estimates that 73% of wines are acceptable <em>in reality.</em> This is more than the rate of unanimous agreement, which we saw above was only 49%.</p></li>
<li><p><span class="math inline">\(a\)</span> = .54 estimates that the judges make accurate ratings (justified true belief) more than half the time. The rest of the ratings are assumed to be random.</p></li>
<li><p><span class="math inline">\(p\)</span> = .78 estimates that when random ratings are made, judges choose “acceptable” 78% of the time. Notice that this is close to the estimated actual rate of 73% (the estimated value of <span class="math inline">\(t\)</span> above). This turns out to be a desirable quality in judges. See Chapter 3 for details, especially the section on the Fleiss kappa.</p></li>
</ul>
<p>Note that the randomness of inaccurate ratings is not a conscious choice of raters. They aren’t giving up and flipping a coin. They still have a reasonable basis for making a rating, and may be quite confident about it. The assumption is that something Gettier-like has gone wrong with the causal process that links the input (wine) to output (rating), which flips the process from deterministic to stochastic. As with any regression model, the world doesn’t have to actually be that way; it’s just an assumption to allow us to create the model. Then we check to see if the data matches the model. In this case, the data matches the model pretty well, as we can visually assess by comparing the blue dashed line to the lollipops. More formal tests of agreement will have to wait until later.</p>
<p>The wine ratings comprise four categories that are naturally ordered from 1 = “no medal” to 4 = “gold medal.” That kind of data is common: from surveys with Strongly Disagree to Strongly Agree scales, from consumer ratings, and many other sources. We humans seem to have a natural facility to think of descriptions along an imaginary intensity scale. This data type is called “ordinal,” meaning we can put the categories in order.</p>
<p>The t-a-p model is designed to work with binary data. It’s possible to switch to a multinomial model, but it’s common to analyze ordinal scales using binary distinctions. I’ll show how that works with the wine data. Along the 1,2,3,4 scale, there are three sequential cut-points where the commas are. The first one we can denote “1|2”, which splits the scores into two sets: an in-class {1} and and out-class {2,3,4}. This is the same thing as the Class 1 and Class 0 distinction from earlier. The second cut-point is at 2|3, splitting the data into ratings {1,2} and {3,4}. The plot shows the fraction of ratings in the in- and out-class for each cut-point.</p>
<div id="fig-wine-ordinal-distro" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wine-ordinal-distro-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/wine cutpoints.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wine-ordinal-distro-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Wine ratings divided into binary groups by cutpoint.
</figcaption>
</figure>
</div>
<p>The 3|4 bar of the plot shows us that 88% of ratings are in {1,2,3}, so the remaining 12% are 4s (gold medal). As a validity check, there are 183 wines, each rated by four judges, so that should be 732 ratings. That looks like the height on the bars, so it checks out.</p>
<p>For a given cut-point, a wine rating will either be in the in-class or out-class This converts the scale to binary, and then we can estimate the model parameters. We repeat the t-a-p model for each cut-point to create a pattern like the one in the plot below.</p>
<div id="fig-wine-ordinal-results" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wine-ordinal-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/wine ordinal.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wine-ordinal-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: t-a-p parameter estimates for ordinal scale based on cutpoints
</figcaption>
</figure>
</div>
<p>The plot in <a href="#fig-wine-ordinal-results" class="quarto-xref">Figure&nbsp;5</a> shows each t-a-p parameter for each cut-point. As a reference, the Fleiss kappa estimates are also included as dotted lines. The significance of the Fleiss kappa is discussed later.</p>
<p>The accuracy parameter <span class="math inline">\(a\)</span> at the top shows good accuracy for the lowest cut-point, meaning that the judges were good at distinguishing the least worthy wines from the rest. This is the case we analyzed earlier when we called the 1 rating unacceptable. As the quality rating increases, moving to the right on the top plot, accuracy decreases to less than half its value for the first cut-point (focus on the solid line). This would be the case if poor wines have more basis in physiology (sourness, etc.) and as the assessments become more aesthetic, they become more individualized and have less group agreement.</p>
<p>The second plot, showing estimates for <span class="math inline">\(p\)</span> show the probability with which judges place a wine randomly into the in-class for inaccurate ratings. This will happen less often for the left-most cut-points, since accuracy is higher there. The dotted line is useful here: it shows what the <span class="math inline">\(p\)</span> parameter would look like if the raters assigned ratings proportionally when making inaccurate classifications. For example, we noted earlier that 88% of the ratings are 1-3 (rightmost bar of the previous plot), so for the 3|4 cut-point, proportional random ratings would assign 88% of the inaccurate ratings into the {1,2,3} in-class. That’s where the dotted line is, at 88%. The actual parameter estimate (solid line) is at about 84%, meaning that judges are probably too conservative about assigning the gold medal category. Accuracy is low for the gold medals, and when the judges rate inaccurately, the ratings are slightly biased toward the lower ratings.</p>
<p>The bottom plot in <a href="#fig-wine-ordinal-results" class="quarto-xref">Figure&nbsp;5</a> estimates the true proportions for each cut-point, after taking into account the other two parameters. For the 3|4 cut-point on the right, it shows a proportion of {1,2,3} wines of about 65%. This is much lower than the 88% of ratings that are {1,2,3}. That’s the combined effect of inaccurate ratings at the top end of the scale combined with the bias toward lower ratings for inaccurate ratings. Looking at the two ends of the scale for the <span class="math inline">\(t\)</span> plot, we can estimate that about 26% of wines are truly in the 1 = no medal category, and about 35% are in the 4 = gold medal category (1 - .65 = .35). That 35% figure comes from reading the estimated value of <span class="math inline">\(t\)</span> at the 3|4 cut point, which is about 65%, and subtracting from one.</p>
<p>It’s possible to tell good wine from bad wine pretty reliably in this data set, but beyond that individual tastes may not be discerning enough to sort out four levels of wine quality. The scale could possibly be reduced to three ratings, or else keep the existing scale but collapse the 3 and 4 ratings into a single category before reporting the results. The effect of the existing rating system is leaving a lot of probably excellent wines with poorer ratings than they deserve. An alternative approach is to try to improve the accuracy of the higher ratings, which can be facilitated by reducing the rater bias against the highest award. One study showed that it’s possible to improve reliability in college grade assignment through feedback <span class="citation" data-cites="millet2010improving">Millet (<a href="#ref-millet2010improving" role="doc-biblioref">2010</a>)</span>. This method might work more generally for reducing rater bias.</p>
</section>
</section>
<section id="so-called-truth" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> So-called Truth</h1>
<p>In statistics we commonly make assumptions that we know aren’t true, because a simple approximation is often better than an intractable exactitude for understanding the world. So it is with the assumptions about true ratings I listed above. Independent observation is a powerful way to learn about the world, but it’s not epistemological fairy dust. If the data points to a set of conclusions about the world, we still need a plausible causal explanation to generalize the findings. The t-a-p models and their kin don’t give us must structure; their truth <a href="https://poets.org/poem/song-myself-51">contains multitudes</a>. For example, suppose we ask raters a series of questions like “Is red better than blue?” or “Is Star Wars better than The Godfather?” If we ask enough such questions, we’ll end up with a data set that has plausible “true” values where A &gt; B, B &gt; C, and C &gt; A, violating our intuition about how “better” is transitive. For a biological example, see <a href="https://en.wikipedia.org/wiki/Ring_species">ring species</a>. The point is that we can’t assume any logical structure to “truth” values produces by these agreement models. It’s good to be cautious when understanding and communicating results by not making universal statements from empirical data.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-agresti2003categorical" class="csl-entry" role="listitem">
Agresti, A. (2003). <em>Categorical data analysis</em> (Vol. 482). John Wiley &amp; Sons.
</div>
<div id="ref-aickin1990maximum" class="csl-entry" role="listitem">
Aickin, M. (1990). Maximum likelihood estimation of agreement in the constant predictive probability model, and its relation to cohen’s kappa. <em>Biometrics</em>, 293–302.
</div>
<div id="ref-breiman" class="csl-entry" role="listitem">
Breiman, L. (2001). <span class="nocase">Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)</span>. <em>Statistical Science</em>, <em>16</em>(3), 199–231. <a href="https://doi.org/10.1214/ss/1009213726">https://doi.org/10.1214/ss/1009213726</a>
</div>
<div id="ref-button2020inter" class="csl-entry" role="listitem">
Button, C. M., Snook, B., &amp; Grant, M. J. (2020). Inter-rater agreement, data reliability, and the crisis of confidence in psychological research. <em>Quant Methods Psychol</em>, <em>16</em>(5), 467–471.
</div>
<div id="ref-carpenter2008multilevel" class="csl-entry" role="listitem">
Carpenter, B. (2008). Multilevel bayesian models of categorical data annotation. <em>Unpublished Manuscript</em>, <em>17</em>(122), 45–50.
</div>
<div id="ref-chaturvedi2015evaluation" class="csl-entry" role="listitem">
Chaturvedi, S., &amp; Shweta, R. (2015). Evaluation of inter-rater agreement and inter-rater reliability for observational data: An overview of concepts and methods. <em>Journal of the Indian Academy of Applied Psychology</em>, <em>41</em>(3), 20–27.
</div>
<div id="ref-cicchetti1990high" class="csl-entry" role="listitem">
Cicchetti, D. V., &amp; Feinstein, A. R. (1990). High agreement but low kappa: II. Resolving the paradoxes. <em>Journal of Clinical Epidemiology</em>, <em>43</em>(6), 551–558.
</div>
<div id="ref-dawid1979maximum" class="csl-entry" role="listitem">
Dawid, A. P., &amp; Skene, A. M. (1979). Maximum likelihood estimation of observer error-rates using the EM algorithm. <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em>, <em>28</em>(1), 20–28.
</div>
<div id="ref-eubankscause" class="csl-entry" role="listitem">
Eubanks, D. A. (2014). Causal interfaces. <em>Arxiv.org Preprint</em>. <a href="http://arxiv.org/abs/1404.4884v1">http://arxiv.org/abs/1404.4884v1</a>
</div>
<div id="ref-fleiss2013statistical" class="csl-entry" role="listitem">
Fleiss, J. L., Levin, B., &amp; Paik, M. C. (2013). <em>Statistical methods for rates and proportions</em>. john wiley &amp; sons.
</div>
<div id="ref-gwet2008computing" class="csl-entry" role="listitem">
Gwet, K. L. (2008). Computing inter-rater reliability and its variance in the presence of high agreement. <em>British Journal of Mathematical and Statistical Psychology</em>, <em>61</em>(1), 29–48.
</div>
<div id="ref-hodgson2008examination" class="csl-entry" role="listitem">
Hodgson, R. T. (2008). An examination of judge reliability at a major US wine competition. <em>Journal of Wine Economics</em>, <em>3</em>(2), 105–113.
</div>
<div id="ref-landis1977measurement" class="csl-entry" role="listitem">
Landis, J. R., &amp; Koch, G. G. (1977). The measurement of observer agreement for categorical data. <em>Biometrics</em>, 159–174.
</div>
<div id="ref-millet2010improving" class="csl-entry" role="listitem">
Millet, I. (2010). Improving grading consistency through grade lift reporting. <em>Practical Assessment, Research, and Evaluation</em>, <em>15</em>(1).
</div>
<div id="ref-passonneau2014benefits" class="csl-entry" role="listitem">
Passonneau, R. J., &amp; Carpenter, B. (2014). The benefits of a model of annotation. <em>Transactions of the Association for Computational Linguistics</em>, <em>2</em>, 311–326.
</div>
<div id="ref-shrout1979intraclass" class="csl-entry" role="listitem">
Shrout, P. E., &amp; Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing rater reliability. <em>Psychological Bulletin</em>, <em>86</em>(2), 420.
</div>
<div id="ref-vach2023gwet" class="csl-entry" role="listitem">
Vach, W., &amp; Gerke, O. (2023). Gwet’s AC1 is not a substitute for cohen’s kappa–a comparison of basic properties. <em>MethodsX</em>, 102212.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>An earlier paper on this was published under the auspices of the NSA, and has a stamp declaring it unclassified. See <a href="https://www.nsa.gov/portals/75/documents/news-features/declassified-documents/tech-journals/application-of-cluster-analysis.pdf" class="uri">https://www.nsa.gov/portals/75/documents/news-features/declassified-documents/tech-journals/application-of-cluster-analysis.pdf</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>It’s a mystery why humans can agree on mathematical justifications at such a high rate. If agreement indicates reality, then math must be real in some way.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>I found this on wikipedia here <a href="https://en.wikipedia.org/wiki/Gettier_problem" class="uri">https://en.wikipedia.org/wiki/Gettier_problem</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>which seems like an infinite regress of JTB inquiry.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Confusion matrices are at the heart of measures of causality too, for reasons that are not coincidental. See <span class="citation" data-cites="eubankscause">Eubanks (<a href="#ref-eubankscause" role="doc-biblioref">2014</a>)</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>The process of a group of people <em>reaching</em> agreement does not seem to be transitive! It may well be three times as difficult for three people to agree on a movie to watch as two people. The rater agreement models get around this by assuming that raters don’t talk to (argue with) each other, but reach conclusions independently before comparing notes.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>For much more on this idea see “Causal Interfaces.”<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/kappazoo\.com\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>