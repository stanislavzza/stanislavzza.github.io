<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="David Eubanks">

<title>Chapter 3: Kappa Statistics – The Kappa Zoo</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-fcd204c8655bd031ced4918abb783b1b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-XP6WMESY52"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-XP6WMESY52', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">
      The Kappa Zoo
      </li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">The Kappa Zoo</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#naive-raters-the-s-statistic" id="toc-naive-raters-the-s-statistic" class="nav-link" data-scroll-target="#naive-raters-the-s-statistic"><span class="header-section-number">2</span> Naive Raters: The S-Statistic</a></li>
  <li><a href="#unbiased-raters-the-fleiss-kappa" id="toc-unbiased-raters-the-fleiss-kappa" class="nav-link" data-scroll-target="#unbiased-raters-the-fleiss-kappa"><span class="header-section-number">3</span> Unbiased Raters: the Fleiss Kappa</a></li>
  <li><a href="#ac-1" id="toc-ac-1" class="nav-link" data-scroll-target="#ac-1"><span class="header-section-number">4</span> AC 1</a></li>
  <li><a href="#cohens-kappa" id="toc-cohens-kappa" class="nav-link" data-scroll-target="#cohens-kappa"><span class="header-section-number">5</span> Cohen’s Kappa</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">6</span> Discussion</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Chapter 3: Kappa Statistics</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>David Eubanks </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>The formula for chance-corrected measure of agreement (generically a “kappa”) compares observed match rates to the expectation of random match rates. The kappas vary in how they estimate the random match rates. For two ratings to match, two raters <span class="math inline">\(j,k\)</span> of the same subject <span class="math inline">\(i\)</span> must agree in their assignment of either Class 1 or Class 0 classifications. In other words, the binary random variables must agree: <span class="math inline">\(C_{ij} = C_{ik}\)</span>. A generic formula that includes the most common kappas is</p>
<p><span class="math display">\[
\kappa = \frac{m_o - m_c}{1 - m_c},
\]</span>where <span class="math inline">\(m_o\)</span> is the observed proportion of agreements and <span class="math inline">\(m_c\)</span> is the expected proportion of agreements under chance. The assumption about <span class="math inline">\(m_c\)</span> is a defining feature of the various kappa statistics. The most general treatment of such statistics is the Krippendorff alpha <span class="citation" data-cites="krippendorff2018content">(<a href="#ref-krippendorff2018content" role="doc-biblioref">Krippendorff, 2018, pp. 221–250</a>)</span></p>
<p>The various kappas differ in the assumption made about the chance correction probability <span class="math inline">\(m_c\)</span>. Commonly, the assumption is that <span class="math inline">\(m_c = x^2 + \bar{x}^2\)</span> for some probability <span class="math inline">\(x\)</span>. This simple formulation makes sense when both raters are guessing, but the actual case is more complicated because a match “by chance” could be a case where one rating was accurate and the other was a guess. This distinction isn’t generally made in the derivations of the kappas, although the AC1 paper discusses the issue, and hints at a full three-parameter model. It’s ironic that the confusion about kappas is disagreement about the probability of agreement by chance.</p>
<p>The Fleiss kappa <span class="citation" data-cites="fleiss1971measuring">(<a href="#ref-fleiss1971measuring" role="doc-biblioref">Fleiss, 1971</a>)</span> uses the fraction of Class 1 ratings <span class="math inline">\(c\)</span> to create <span class="math inline">\(m_c = c^2 + \bar{c}^2\)</span>. The S statistic <span class="citation" data-cites="bennett1954communications">(<a href="#ref-bennett1954communications" role="doc-biblioref">Bennett et al., 1954</a>)</span>, also called the Guilfords’ G or G-index<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="citation" data-cites="holley1964note">(<a href="#ref-holley1964note" role="doc-biblioref">Holley &amp; Guilford, 1964</a>)</span>, is a kappa that assumes <span class="math inline">\(m_c = 1/2\)</span> when there are two categories. The AC1 kappa has a different form, assuming that <span class="math inline">\(m_c = 2c\bar{c}\)</span> <span class="citation" data-cites="gwet2008computing">(<a href="#ref-gwet2008computing" role="doc-biblioref">Gwet, 2008</a>)</span>. The Cohen kappa is a variation where each rater gets a guessing distribution, so <span class="math inline">\(m_c = x_1x_2 + \bar{x_1}\bar{x_2}\)</span> <span class="citation" data-cites="cohen1960coefficient">(<a href="#ref-cohen1960coefficient" role="doc-biblioref">Cohen, 1960</a>)</span>. Because of the extra parameter, discussion of Cohen’s kappa is found in the chapter on hierarchical models rather than being included here.</p>
<p>Consider two raters classifying an observation. In the t-a-p model we can express the expected value of observed matches <span class="math inline">\(m_o\)</span> as the sum of three kinds of agreement: (1) <span class="math inline">\(m_a\)</span> is when both raters are accurate (and hence agree), (2) <span class="math inline">\(m_i\)</span> when both raters are inaccurate (guessing) and agree, and (3) <span class="math inline">\(m_x\)</span> is the mixed case when one rater is accurate and the other is inaccurate but they agree. The second two of these have expressions that include the guessing rate <span class="math inline">\(m_c\)</span>. Following that thinking we have the following expectations for rates:</p>
<p><span id="eq-match-rates"><span class="math display">\[
\begin{aligned}
m_a &amp;= a^2 &amp; \text{(both accurate)}\\
m_r &amp;= p^2 + \bar{p}^2 &amp; \text{(random ratings)}\\
m_i &amp;= \bar{a}^2m_r = a^2m_r - 2am_r + m_r &amp;\text{(both inaccurate)}\\
m_x &amp;= 2a\bar{a}(tp + \bar{t}\bar{p}) &amp;\text{(mixed accurate and inaccurate)}\\
m_o &amp;= m_a + m_i + m_x &amp;\text{(observed match rate)}\\
    &amp;= a^2+a^2m_r + m_r - 2am_r + 2a\bar{a}(tp + \bar{t}\bar{p})\\
\end{aligned}
\tag{1}\]</span></span></p>
<p>For <span class="math inline">\(m_a\)</span>, both ratings must be accurate, in which case they automatically agree. For <span class="math inline">\(m_i\)</span>, both must be inaccurate (probability <span class="math inline">\(\bar{a}^2\)</span>) and then match randomly (probability <span class="math inline">\(m_r\)</span>). For <span class="math inline">\(m_x\)</span>, one rater must be accurate and the other inaccurate, in which case they agree if the accurate rater chooses the category that the inaccurate rater guesses. The various kappa derivations usually ignore these mixed matches in favor of using <span class="math inline">\(m_r\)</span> as the chance match rate, which we called <span class="math inline">\(m_c\)</span> in the kappa formula. This amounts to choosing <span class="math inline">\(p\)</span> since <span class="math inline">\(m_r = p^2 + \bar{p}^2\)</span>.</p>
<p>The various match rates in <a href="#eq-match-rates" class="quarto-xref">Equation&nbsp;1</a> create a vocabulary for understanding some of the kappa statistics. The easiest one to analyze is the S-statistic (it is sometimes called the G-index).</p>
</section>
<section id="naive-raters-the-s-statistic" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Naive Raters: The S-Statistic</h1>
<p>The S-statistic <span class="citation" data-cites="bennett1954communications">(<a href="#ref-bennett1954communications" role="doc-biblioref">Bennett et al., 1954</a>)</span> makes the assumption that if there are two categories to choose from, then the chance of a random match between two raters is <span class="math inline">\(m_c = m_r = 1/2\)</span>. If we assume this means two <em>inaccurate</em> raters in the t-a-p model, we have <span class="math inline">\(p=1/2\)</span>. I call this the naive rater assumption, because it assumes that inaccurate raters are not influenced by the actual proportions of the two categories. For example, if an inexperienced doctor repeatedly diagnosed patients as having a very rare condition, this would be “naive” in the meaning here.</p>
<p>Substituting <span class="math inline">\(p=1/2\)</span> into the formulas of <a href="#eq-match-rates" class="quarto-xref">Equation&nbsp;1</a> results in the kappa</p>
<p><span class="math display">\[
\begin{aligned}
\kappa_s &amp;= \frac{m_o - m_c}{1 - m_c} \\
&amp;= \frac{a^2+a^2/2 + 1/2 - a + 2a\bar{a}(t/2 + \bar{t}/2) - 1/2}{1 - 1/2} \\
&amp;= 2(3a^2/2 + 1/2 + -a + a - a^2 - 1/2) \\
&amp;= a^2
\end{aligned}
\]</span></p>
<p>In this case, the intuition from the introductory chapter that we’re interested in something like the square root of rater agreement is exactly right. If the raters really do assign random ratings as if flipping a coin, then the resulting kappa derived from the data will have as its expectation the accuracy squared. Actual results will also have estimation error, depending on sample size and how unlucky you are.</p>
<p>I called the <span class="math inline">\(p=1/2\)</span> error structure “naive raters,” but there’s another way to think about it. The choice of <span class="math inline">\(p=1/2\)</span> is the “most random” in the sense that this is the choice that gives the largest entropy: one bit per inaccurate rating. For example, it would be odd if we chose <span class="math inline">\(p=1/3\)</span> without some justification, like historical data. The choice of <span class="math inline">\(1/2\)</span> puts the least load on causal explanations. I’ll return to this theme below in the context of the Fleiss Kappa.</p>
</section>
<section id="unbiased-raters-the-fleiss-kappa" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Unbiased Raters: the Fleiss Kappa</h1>
<p>The Fleiss kappa is designed to work with ratings where the number of raters per subject can vary, and with a rating scale of arbitrary length. It assumes an asymptotic form for chance correction, so is most appropriate for large samples. I will only consider the binary scale case here for simplicity.</p>
<p>The baseline for random ratings for Fleiss is if we took all the ratings and randomly shuffled them between subjects. In this case, the match rate for two raters is given by the proportions of the ratings for the two classes as found in the data. For example, if Class 1 ratings comprise 20% of the total, then the random match rate is <span class="math inline">\(m_c = .2^2 + .8^2\)</span>. That’s the probability that either two random Class 1 ratings match or two random Class 0 ratings do. If <span class="math inline">\(c\)</span> is the expected proportion of Class 1 ratings, then <span class="math inline">\(m_c = c^2 + \bar{c}^2\)</span>. From the t-a-p diagram, we can see that <span class="math inline">\(c = t(a + \bar{a}p) + \bar{t}\bar{a}p = ta + \bar{a}p\)</span>. If inaccurate ratings assign Class 1 at the true rate so that <span class="math inline">\(t = p\)</span>, I’ll describe the raters as “unbiased.” In that case <span class="math inline">\(c = pa + p\bar{a} = p = t\)</span>; the rating proportions of Class 1 reflect the true rates, because the raters assign proportionate “guesses” for inaccurate ratings. Under this assumption, with <span class="math inline">\(t = p\)</span> and <span class="math inline">\(m_c = m_r = p^2 + \bar{p}^2\)</span>, kappa becomes</p>
<p><span class="math display">\[
\begin{aligned}\kappa_f &amp;= \frac{m_o - m_c}{1 - m_c} \\
&amp;= \frac{a^2+a^2m_r + m_r - 2am_r + 2a\bar{a}(p^2 + \bar{p}^2) - m_r}{1 - m_r} \\
&amp;= \frac{a^2+ m_r(a^2 + 1 - 2a + 2a - 2a^2 - 1)}{1 - m_r} \\
&amp;= \frac{a^2 - a^2m_r}{1-m_r}\\
&amp;= a^2.
\end{aligned}
\]</span></p>
<p>For the Fleiss kappa, it is also true that the expectation of kappa is the accuracy squared, this time if the condition <span class="math inline">\(t = p\)</span> is met by the raters represented in the data you have.</p>
<p>The Fleiss kappa formula can be thought of as first establishing a random baseline for matches. As described above, it’s by taking all the ratings for all the subjects, scrambling them up randomly, and choosing two at a time to see if they match. That rate is expected to be <span class="math inline">\(c^2 + \bar{c}^2\)</span>, the chance that two Class 1 ratings will match plus the chance that two Class 0 ratings will. Notice that <span class="math inline">\(1 = (c + \bar{c})^2 = c^2 + 2c\bar{c} + \bar{c}^2\)</span>, so the denominator of the kappa formula amounts to the expected rate of non-matches <span class="math inline">\(2c\bar{c}\)</span> in the ratings. The whole formula is then (observed matches - random matches)/(random non-matches).</p>
<p>A review of the properties of Fleiss kappa can be found in <span class="citation" data-cites="fleiss2013statistical">Fleiss et al. (<a href="#ref-fleiss2013statistical" role="doc-biblioref">2013</a>)</span>, chapter 18, including kappa’s equivalence to an intraclass correlation coefficient, defined as ICC(1,1) in <span class="citation" data-cites="shrout1979intraclass">Shrout &amp; Fleiss (<a href="#ref-shrout1979intraclass" role="doc-biblioref">1979</a>)</span>. Under the <span class="math inline">\(t = p\)</span> “unbiased” condition, rater accuracy <span class="math inline">\(a\)</span> is the correlation between the ratings and the true classifications: <span class="math inline">\(\sqrt{\kappa_f} = a = cor(C, T)\)</span>. Additionally, the Fleiss kappa is the intraclass correlation of the ratings. Derivations of these results are found in <a href="./correlation.html">Appendix A</a>, where there is also an alternative derivation of the <span class="math inline">\(a = \sqrt{\kappa_f}\)</span> result.</p>
<p>If the <span class="math inline">\(t = p\)</span> assumption is not true for your raters, then the resulting kappa will be biased. In some cases, kappa may be negative. This is a general problem for the kappas, since they make assumptions about the distribution of random raters without testing those assumptions.</p>
<p>Above, I said that the <span class="math inline">\(p\)</span> chosen by the Fleiss kappa is what we get if we randomly assign the ratings among subjects, but this is not plausible as a causal explanation unless we are testing for an incompetent lab assistant. It’s more plausible to assume some cause that’s inherent to the history of the classification, where inaccurate ratings are more likely to be assigned one class than the other. In the unbiased rater case, we could explain it by a possible unconsious allotment, where raters are less likely to assign a class they know is rare. This can be seen as a subtle violation of the assumption that ratings are independent. As we’ll see in Chapter 5’s discussion of dependent ratings, if we admit more dependence on group-level knowlege of class prevelance, rater accuracy is swamped by what we might call groupthink.</p>
</section>
<section id="ac-1" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> AC 1</h1>
<p>The AC1 version of kappa developed in <span class="citation" data-cites="gwet2008computing">Gwet (<a href="#ref-gwet2008computing" role="doc-biblioref">2008</a>)</span> uses a disaggregation of rating agreements found in Table 4, page 36, where the author distinguishes between ratings that are “certain” (the same as “for cause” as found in <span class="citation" data-cites="landis1977measurement">Landis &amp; Koch (<a href="#ref-landis1977measurement" role="doc-biblioref">1977</a>)</span>) versus random. This is similar to the justified true belief formula described in the <a href="./introduction.html">introductory chapter</a>, except that there’s no provision for being certain (believing a classification is true) and yet mistaken (the Gettier problem). The stated goal is to estimate the probability that two raters match when they both rate accurately (“with certainty”). This is <span class="math inline">\(a^2\)</span> in the t-a-p model (equation 16 in the paper). Random matches are those in which at least one of raters rates randomly. In terms of the t-a-p model, this assumes that the probability of a by-chance agreement is <span class="math inline">\(m_c = m_i + m_x\)</span>, the cases where either both raters match randomly or at least one makes an inaccurate rating and they match. The acknowledgement of partially inaccurate matches is a philosophical advance over previous derivations of kappa.</p>
<p>The AC1 kappa is derived from the usual formula (total match rate - estimated random match rate) / (1 - estimated random match rate) (see equation 17 in the paper). The problem, as usual with this approach, is to estimate the (partially) random match rate <span class="math inline">\(m_c = m_i + m_x\)</span>. Of course, neither <span class="math inline">\(m_i\)</span> nor <span class="math inline">\(m_x\)</span> are directly observable, so the author derives an approximation in two steps. First the probability of agreement between two raters, at least one of whom made an inaccurate rating, is assumed to be 1/2. In the t-a-p model, this amounts to</p>
<p><span id="eq-ac1"><span class="math display">\[
Pr[\text{match|random}] = \frac{m_i + m_x}{1-a^2} \approx 1/2.
\tag{2}\]</span></span></p>
<p>To remove the denominator requires multiplying by the probability of at least one random rating, which is then approximated by <span class="math inline">\(4c\bar{c}\)</span>, where <span class="math inline">\(c\)</span> is the probability of a rater assigning Class 1 to a subject, which can be estimated directly from the data. Since from the t-a-p diagram of conditional probabilities, <span class="math inline">\(c = ta +\bar{a}p\)</span>, the estimation entails assuming that</p>
<p><span class="math display">\[
Pr[\text{random}] = 1-a^2 \approx 4(ta + \bar{a}p)(1-ta-\bar{a}p)
\]</span></p>
<p>Multiplying these gives (page 37) <span class="math inline">\(\hat{m_c} = 2c\bar{c} = 2(ta + \bar{a}p)(1-ta-\bar{a}p)\)</span>. The approach here is ingenious and philosophically rich, but the limitations of a one-parameter index for rater agreement limit how much can be done.</p>
<p>Recall that the Fleiss kappa assumes <span class="math inline">\(m_c = c^2 + \bar{c}^2\)</span>, as compared to the AC1’s <span class="math inline">\(m_c = 2c\bar{c}\)</span>. The AC1 version is the complement of the Fleiss version: they sum to one since <span class="math inline">\(1 = (c + \bar{c})^2 = c^2 + 2c\bar{c} + \bar{c}^2\)</span>. In some sense, the assumptions about the two kappas are opposite: what Fleiss considers random, AC1 considers non-random, and vice-versa, at least in expectation. The Fleiss kappa assumes a benchmark match rate derived from randomly shuffling the raters (regardless of subject). The AC1’s assumption can be seen as shuffling the ratings and then finding the <em>non-match</em> rate (one minus the match rate) in the resulting data. The kappa then becomes (observed matches - random non-matches)/(random matches) whereas the Fleiss version is (observed matches - random matches)/(random non-matches). The two will agree if <span class="math inline">\(m_c = .5\)</span>, which is the S-Statistic’s assumption too.</p>
</section>
<section id="cohens-kappa" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Cohen’s Kappa</h1>
<p>Cohen’s kappa <span class="citation" data-cites="cohen1960coefficient">(<a href="#ref-cohen1960coefficient" role="doc-biblioref">Cohen, 1960</a>)</span> is the original kappa, and is still widely used. For criticism of the statistic see <span class="citation" data-cites="delgado2019cohen">Delgado &amp; Tibau (<a href="#ref-delgado2019cohen" role="doc-biblioref">2019</a>)</span>, and a comparison with AC1 in <span class="citation" data-cites="vach2023gwet">Vach &amp; Gerke (<a href="#ref-vach2023gwet" role="doc-biblioref">2023</a>)</span>. In its original form, it’s limited to two raters and a binary scale. It’s similar to the Fleiss kappa in that it uses empirical rating distributions to assume a chance match rate <span class="math inline">\(m_c\)</span>, however it has an extra complication. Cohen’s kappa assumes that each rater has an <em>individual parameter</em> for random ratings. In the t-a-p model, this amounts to assigning each rater <span class="math inline">\(j\)</span> a parameter <span class="math inline">\(p_j\)</span>. The original statistic only considered two raters, but that can be extended to any number of them. This situation is best treated as a hierarchical model that uses a more general approach to maximum likelihood than a binomial mixture (see <a href="./hierarchical.html">Chapter 5</a>). However, it’s possible to simply expand the simple t-a-p model to include two raters with individual parameters <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span>. If <span class="math inline">\(p_1 = p_2 = t\)</span>, we are back to the Fleiss model’s assumption about unbiased raters. If <span class="math inline">\(p_1 \ne p_2\)</span>, then at least one rater is biased, meaning that the distribution of their random ratings is different from the true distribution of classifications. Expanding the scope to multiple raters and multiple classifications (beyond binary) leads to the model from <span class="citation" data-cites="dawid1979maximum">Dawid &amp; Skene (<a href="#ref-dawid1979maximum" role="doc-biblioref">1979</a>)</span>, which is described in <a href="./hierarchical.html">Chapter 5</a>.</p>
</section>
<section id="discussion" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Discussion</h1>
<p>Both the worst-case match rate for binary ratings, <span class="math inline">\(p= 1/2\)</span>, and the proportional (unbiased) rate <span class="math inline">\(t=p\)</span> lead to kappas that have a nice relationship to accuracy in the t-a-p model, but only when the respective assumption about raters is true. Generally we don’t know what <span class="math inline">\(p\)</span> is for a given data set, however, so assuming either of those conditions is a leap of faith.</p>
<p>We might wonder if there are other kappas that have the nice property that accuracy is the square root. We can attempt to choose <span class="math inline">\(m_c\)</span> so that <span class="math inline">\(\kappa = a^2\)</span> via <span class="math inline">\(m_o - m_c = a^2(1-m_c)\)</span>. Solving for <span class="math inline">\(m_c\)</span> and using <span class="math inline">\(m_o - a^2 = m_i + m_x\)</span> leads to</p>
<p><span id="eq-a-square"><span class="math display">\[ \begin{aligned} m_c^* &amp;= \frac{m_i + m_x}{1 - a^2} \\ &amp;= \frac{\bar{a}^2m_r + 2a\bar{a}(tp + \bar{t}\bar{p})}{(1+a)\bar{a}} \\ &amp;= \frac{\bar{a}(p^2 + \bar{p}^2)  + 2a(tp + \bar{t}\bar{p})}{1+a} \\ \end{aligned}  \tag{3}\]</span></span></p>
<p>where the asterisk denotes the choice of the chance correction formula <span class="math inline">\(m_c\)</span> that makes <span class="math inline">\(\kappa = a^2\)</span>. In the first line of <a href="#eq-a-square" class="quarto-xref">Equation&nbsp;3</a>, the numerator is the expected proportion of matches where there is at least one inaccurate rating, and the denominator is the the rate of non-perfect rating pairs, where at least one of the raters is inaccurate (they may or may not match). We saw this above in the derivation of AC1 in <a href="#eq-ac1" class="quarto-xref">Equation&nbsp;2</a>. It turns out that the correct choice of <span class="math inline">\(m_c\)</span> is the conditional probability of a match given that at least one of the raters is inaccurate, rather than the unconditional probability of a match given that at least one of the raters is inaccurate. The difference is the <span class="math inline">\(1-a^2\)</span> denominator. The chance correction is therefore accounting for the accurate ratings by taking them out of the data altogether and then calculating inaccurate matches out of all remaining rating pairs as the probability of by-chance matching.</p>
<p>The formula in <a href="#eq-a-square" class="quarto-xref">Equation&nbsp;3</a> is useful for testing properties of kappa assumptions. We can use it to verify that <span class="math inline">\(p=1/2\)</span> (naive raters) and <span class="math inline">\(t=p\)</span> (unbiased raters) works as shown earlier. In practice, it’s better to just derive all three of the t-a-p parameters instead of making assumptions that have to be tested (by deriving all the parameters). This approach also avoids the potential problem of researchers choosing a kappa based on which method gives the best answer (see <span class="citation" data-cites="vach2023gwet">Vach &amp; Gerke (<a href="#ref-vach2023gwet" role="doc-biblioref">2023</a>)</span>).</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-bennett1954communications" class="csl-entry" role="listitem">
Bennett, E. M., Alpert, R., &amp; Goldstein, A. (1954). Communications through limited-response questioning. <em>Public Opinion Quarterly</em>, <em>18</em>(3), 303–308.
</div>
<div id="ref-cohen1960coefficient" class="csl-entry" role="listitem">
Cohen, J. (1960). A coefficient of agreement for nominal scales. <em>Educational and Psychological Measurement</em>, <em>20</em>(1), 37–46.
</div>
<div id="ref-dawid1979maximum" class="csl-entry" role="listitem">
Dawid, A. P., &amp; Skene, A. M. (1979). Maximum likelihood estimation of observer error-rates using the EM algorithm. <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em>, <em>28</em>(1), 20–28.
</div>
<div id="ref-delgado2019cohen" class="csl-entry" role="listitem">
Delgado, R., &amp; Tibau, X.-A. (2019). Why cohen’s kappa should be avoided as performance measure in classification. <em>PloS One</em>, <em>14</em>(9), e0222916.
</div>
<div id="ref-fleiss1971measuring" class="csl-entry" role="listitem">
Fleiss, J. L. (1971). Measuring nominal scale agreement among many raters. <em>Psychological Bulletin</em>, <em>76</em>(5), 378.
</div>
<div id="ref-fleiss2013statistical" class="csl-entry" role="listitem">
Fleiss, J. L., Levin, B., &amp; Paik, M. C. (2013). <em>Statistical methods for rates and proportions</em>. john wiley &amp; sons.
</div>
<div id="ref-gwet2008computing" class="csl-entry" role="listitem">
Gwet, K. L. (2008). Computing inter-rater reliability and its variance in the presence of high agreement. <em>British Journal of Mathematical and Statistical Psychology</em>, <em>61</em>(1), 29–48.
</div>
<div id="ref-holley1964note" class="csl-entry" role="listitem">
Holley, J. W., &amp; Guilford, J. P. (1964). A note on the g index of agreement. <em>Educational and Psychological Measurement</em>, <em>24</em>(4), 749–753.
</div>
<div id="ref-krippendorff2018content" class="csl-entry" role="listitem">
Krippendorff, K. (2018). <em>Content analysis: An introduction to its methodology</em>. Sage publications.
</div>
<div id="ref-landis1977measurement" class="csl-entry" role="listitem">
Landis, J. R., &amp; Koch, G. G. (1977). The measurement of observer agreement for categorical data. <em>Biometrics</em>, 159–174.
</div>
<div id="ref-shrout1979intraclass" class="csl-entry" role="listitem">
Shrout, P. E., &amp; Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing rater reliability. <em>Psychological Bulletin</em>, <em>86</em>(2), 420.
</div>
<div id="ref-vach2023gwet" class="csl-entry" role="listitem">
Vach, W., &amp; Gerke, O. (2023). Gwet’s AC1 is not a substitute for cohen’s kappa–a comparison of basic properties. <em>MethodsX</em>, 102212.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Not to be confused with the G-index used for ranking citation counts of scholars.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/kappazoo\.com\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>