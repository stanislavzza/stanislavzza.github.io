[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Kappa Zoo",
    "section": "",
    "text": "This site–a work in progress–explains rater reliability and how to measure it. Much of the content is new, using a general model to explain the features of existing methods from the measurement literature, in the context of more modern approaches from the machine learning (ML) literature. The sythesis of these two fields is the t-a-p model, which combines the intuitiveness of the measurement ideas with the statistical power of the ML algorithms.\nUse the navigation bar at the top of the page to explore the chapters, or jump directly to specific sections using the links below. If you are new to t-a-p models, I recommend starting with the Chapter 1. Then if you want to dive into theory, the subsequent two chapters lay out the statistical derivations and relationship to older methods, for example showing how the Fleiss kappa is a special case of a t-a-p model.\nIf you’d rather jump straight to applications, you can skip the theory and go to the The t-a-p App page, which is a user manual for the R Shiny app that accompanies this site. With it you can simulate data or load your own, then model the data for rater accuracy.\nThe Kappa Zoo includes a growing collection of real-world data sets and the model parameters estimated from them.\n\n\n\nChapter 1: Quantifying Agreement\nChapter 2: The t-a-p Model\nChapter 3: Kappa Statistics\nChapter 4: Hierarchical Models\nChapter 5: Machine Learning Methods\nChapter 6: The kappaZoo R Package\nChapter 7: The t-a-p App\nExamples\nReferences\nAppendix A: Statistical Details\n\n\n\nI recommend starting with Chapter 1, which summarizes the problem we want to solve, existing solutions, and how the t-a-p model contributes to that ongoing conversation. For the statistical development of the idea, including the binomial mixture model and its properties, see Chapter 2. The t-a-p model’s relationship to several existing rater agreement methods (the kappas) is addressed in Chapter 3. There, for example, we see how the Fleiss kappa is a special case of the t-a-p model with that assumes \\(t = p\\)."
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "Welcome to the Kappa Zoo",
    "section": "",
    "text": "Chapter 1: Quantifying Agreement\nChapter 2: The t-a-p Model\nChapter 3: Kappa Statistics\nChapter 4: Hierarchical Models\nChapter 5: Machine Learning Methods\nChapter 6: The kappaZoo R Package\nChapter 7: The t-a-p App\nExamples\nReferences\nAppendix A: Statistical Details\n\n\n\nI recommend starting with Chapter 1, which summarizes the problem we want to solve, existing solutions, and how the t-a-p model contributes to that ongoing conversation. For the statistical development of the idea, including the binomial mixture model and its properties, see Chapter 2. The t-a-p model’s relationship to several existing rater agreement methods (the kappas) is addressed in Chapter 3. There, for example, we see how the Fleiss kappa is a special case of the t-a-p model with that assumes \\(t = p\\)."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This chapter gives a brief background on rater agreement with a focus on the goals of the project and how legacy methods fall short. It contrasts psychological measurement methods to machine learning algorithms. A general model is introduced that combines the best features of both of those cultures, and an example is given.\n\n\nOne of the powers of human language is the facility to agree or disagree with a statement. This chapter introduces the concept of quantifying agreement, which is a way to measure the extent to which two or more people agree that a statement is true or fall in cases where they have independently made up their minds. A jury would not meet that qualification, because their purpose to discuss and mull over evidence to reach unanimous agreement. However, a straw poll taken before conversations begin could be considered a set of independent opinions.\nIn ordinary discourse, we use informal agreement to decide what to each for dinner, what movie to watch, or what to do on a Saturday afternoon. In science, we use formal agreement to decide whether a new drug is effective, whether a new theory is correct, or whether a new technology is safe. In politics, we use agreement to decide who should be elected, what laws should be passed, or what policies should be implemented. Science is generally the only domain where we work to ensure that the data (measurements instead of opinions) are taken independently, so they don’t influence one another. This assumption of independence makes the calculations easier, but it is not always realistic. Another limiting assumption is that the process of rating cases is repeatable using the same process. Many real-world situations are not repeatable on a personal level, for example deciding where to go on vacation. Sometimes we can hypothesize repeatability by aggregating over many people who go on vacation, for example.\nThe combination of independence and repeatability are abbreviated in probability theory as “iid” for “independent and identically distributed.” This is a powerful assumption that allows us to reach strong conclusions. However, it is not always realistic.\nThe focus of this guide is on the statistical methods used to quantify agreement between humans, who we’ll call “raters” who assign “ratings” to “subjects.” This describes a familiar case like a product review with a five-star rating system. However, the same methods can be used to quantify agreement between any two entities that can assign categories to cases. A fascinating example comes from an article in The Atlantic Magazine, here describing an analysis of a very old text called the Voynich manuscript.\n\nDavis magnified the handwriting, she noticed subtle variations. In certain places, the script was more cramped or more likely to slant as it crossed the page. She tested this observation by picking a letter that didn’t appear often and tracking it across the manuscript’s pages. Its style, she saw, varied among groups of pages but not within those groups. This suggested that the differences—larger or smaller loops, straighter or curvier crossbars, longer or shorter feet—were the product of different scribes rather than of one scribe writing the same letter in different ways.\n\nSee Davis’s paper for details1. For our purposes, the “raters” are scholars who categorize writing styles, the “subjects” are sections (folios) of the manuscript they are reviewing, and the presumed different scribes are the categories we generically refer to as “ratings.”\n\nSelected rows from a table at the website for the manuscript: https://voynich.nu/index.html, comparing an early attempt to classify writing “hands” (Currier) to the recent classification (Davis). Hypothesized identity is unique to each column with the numbers denoting individual scribes.\n\n\nFolio(s)\nPrescotte Currier\nLisa Fagin Davis\n\n\n\n\nf41, f48, f57r\n2\n5\n\n\nfRos (obverse)\n3\n2\n\n\nfRos (main)\n3\n4\n\n\nf87, f88, f93, 96\n4\n1\n\n\nf94, f95\n5\n3\n\n\nf103, f104, f106\nX\n3\n\n\nf105\nY\n3\n\n\n\nIn the table, Currier thought that the obverse and main folio fRos were in a single hand, whereas Davis has two scribes. Conversely, Davis has the last three rows as the same author while Currier has three. This is an example of how science works toward consensus by placing observations into categories. Agreements are good for pointing to what might be real, and disagreements are for honing theory.\nThe statistical question for data like this is to quantify a degree of separation between categories., to compare variation within groups to variation between groups. One application is an idea called “signal-to-noise ratio,” which is a measure of how much of the variation in the data is due to the thing we’re interested in (signal) versus how much is due to random variation (noise). In the manuscript example, the signal is the commonality of handwriting within contiguous sections, and the noise is variation within each section. Reliability of measures is the same idea in different clothing, where we might formula reliability as between-subject variation divided by total variation. As an example, college grade averages (GPA) can have a reliability of about 0.8, meaning that 80% of the variation in grades is due to differences between students, and 20% is due to variation in how individual students earn grades. This makes GPA useful as a statistical measure, for example in predicting career outcomes [cite eubanks, et al].\nHow do we summarize all this to reach a formal conclusion? This formality is in service of the question “what is the truth?”\n\n\n\nIt turns out that the questions “what is true?” and “what is knowledge, anyway?” are difficult to answer. There is a great body of work in epistemology, the philosophical study of knowledge, from which we can take a clever idea to get started.\nTo start, we have to imagine that “truth” exists. Since we are talking about classifications of subjects into one of two categories, we must commit to the idea that these categories (words that humans made up) might correspond to something in objective reality (whatever that is). We can easily make up descriptions that are fantasies, like unicorns or flying whales, and the goal is to have a way to distinguish these phantasms from real things like penguins and taxes. To do so, we have to imagine that by some mysterious means, our intellects and discourse can access Nature and name things.\nWords are slippery things, however. Although there are definitions, the real meaning of “pizza” is (ironically) ineffable; one could spend a whole career trying to understand it [cite the guy who wrote four vol on it, msft cto]. Let’s take Aristotle’s definition of a man as a featherless biped.\nOur goal is a set of classifications “man” or “not man” generated from independent observers (we call them raters) who each inspect some objects in the world (we call them subjects) and assign “man” or “not man” to each. Then we’ll take that data and apply the between- and within-variance idea.\nTherefore the definition isn’t enough. We also need a procedure for generating the data the way we want. To ensure independence of raters, we don’t want them to collaborate on decisions, for example. In the sciences, procedures can be quite elaborate, as in detecting and classifying Higgs bosons, which requires a huge particle accelerator.\nThe story is that Diogneses the Cynic tossed a plucked chicken at Aristotle’s feet and declared “here’s your man.” [cite the book]. The example drives a wedge between the definition “featherless biped” and what is actually meant by “man.” Outside of math and science, definitions and procedures are fuzzy in this way. Rater agreement statistics attempt to defuzz the ratings, to understand the reality behind them.\nHere’s where the idea from epistemology is key, because it gets to the nature of the fuzziness of classification. First we define knowledge as justified true belief (JTB), and then sort out where the fuzziness is.\nThere are three parts to JTB, and we already talked about truth; as a prerequisite we must believe that true statements can be made about the world, which means accessing something real and describing it. The third piece of JTB is belief, which we’ll take to be the classification assigned by a rater. If our rater looks at a horse and writes down “not unicorn,” we’ll assume that the classification represents what the rater believes. This runs into problems if we want to consider adversarial raters, like someone paid to rate consumer products, or an angry customer rating a service poorly for extraneous reasons. We won’t consider those possibilities here.\nThe first piece of JTB is where the fuzziness lives: justification. Outside of math, justification in words isn’t complete2. In the case of Diogenes versus Aristotle, an follower of the latter might reason that the plucked chicken meets the great philosopher’s definition, and therefore is a man. Others might use common sense to conclude that it’s not a man. There is more than one way to justify belief, but not all of them result in true classifications.\nThe JTB idea was dealt a blow by Gettier, who posed a number of examples where justification goes awry, but the idea is much older. Consider this example from Dharmottara in 779 AD3.\n\nA desert traveller is searching for water. He sees, in the valley ahead, a shimmering blue expanse. Unfortunately, it’s a mirage. But fortunately, when he reaches the spot where there appeared to be water, there actually is water, hidden under a rock. Did the traveller know, as he stood on the hilltop hallucinating, that there was water ahead?\n\nThe traveller had a belief that turned out to be true, but the justification was incorrect. The philosphers are concerned with whether or not this constitutes knowledge4, but we’re more interested in something simpler. That is to categorize the ways truth and belief can happen. If they don’t coincide (if we believe something false), then we’ll assume that the justification was incorrect. As the Gettier problems and the discussion they generated demonstrate, there are a lot of complicated ways in which justification can go awry.\nIn the literature of rater agreement, one can find similar language. In , we find “The \\(\\alpha\\) agreement parameter is defined as the proportion of a population of items that are classified identically ‘for cause’ by two classifiers, the remaining items being classified at random […].” The author suggests that some subjects are more difficult to rate than others, which hints at modeling each subject’s probability of being in a category, in truth. This idea was explored in @dawid1979maximum, who also allowed that raters might have different proficiencies.\nAnother instance comes from the derivation of a rater agreement statistic called AC1 @gwet2008computing. The author notes that “the assumption [is] that any rating that is not random will automatically lead to a correct classification, while a random rating leads to a correct classification with probability 1/2.” The idea is that we conceptually separate accurate ratings from random ones, which then means we must consider the probability distribution of the random ones. The random assignments will sometimes be correct, but for the wrong reason–just like the Gettier problems.\n\n\n\nDiagnosing illness is an important classification problem, and the simplest version is a blood test that returns a positive or negative result, perhaps scanning for an antibody. There are four possibilities, which can be organized into what is commonly called a confusion matrix. Suppose that we consider a statement S about the world, like “this patient has the flu.” Using a definition and process we reach a conclusion, so that we believe S to be true or false. This generates the classifications we’re interested in: this is what the raters do, generating lots of S or not-S data on cases.\nThe truth is often inaccessible, but recall that we assume that it exists, so that the statement really is true or false: each patient really does have the flu or does not. Here are the four possibilities:\n\nSample confusion matrix\n\n\n\n\n\n\n\n\nS is true\nS is false\n\n\n\n\nBelieve S is true\nTrue positive cases\nFalse positive cases\n\n\nBelieve S is false.\nFalse negative cases\nTrue negative cases\n\n\n\nNotice that there are four spots in the table where data can go: if we had perfect knowledge we could count the number of true positive cases, etc. If we converted these numbers to proportions by dividing by the table, we have three parameters left. This foreshadows what comes later: the heart of the classification problem (in its simple form) is a three-parameter problem5.\nThe limitation of casting the confusion matrix into a single-parameter statistics was noted by @cicchetti1990high, who suggested “two separate indexes of proportionate agreement in the observers’ positive and negative decisions. […] Why do we want a single index?” Another example comes from the motivation for the AC1 rater statistic @gwet2008computing. It separately considers rater true positives and true negatives, but in to get to a single parameter, the model assumes that these are identical for each rater.\nFor a concrete illustration, consider the wine judging data used in @hodgson2008examination. The first five rows look like this:\n\nWine ratings showing rows 1-5 of 183, taken from the study by Hodgson (personal communication).\n\n\nWine\nJ1\nJ2\nJ3\nJ4\n\n\n\n\n1\n3\n3\n3\n3\n\n\n2\n3\n3\n3\n3\n\n\n3\n3\n3\n2\n4\n\n\n4\n3\n4\n3\n1\n\n\n5\n4\n2\n1\n1\n\n\n\nThe 1-4 scale is ordinal, meaning that we imagine quality to increase with the numerical value. For this discussion, we will reduce that complexity to a binary classification: acceptable wine (ratings 2-4) or wine to avoid (rating 1). This simplifies the table to.\n\nSimplified wine ratings, where 1 means “acceptable” and 0 means “avoid.”\n\n\nWine\nJ1\nJ2\nJ3\nJ4\n\n\n\n\n1\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n1\n\n\n3\n1\n1\n0\n1\n\n\n4\n1\n1\n1\n0\n\n\n5\n1\n0\n0\n0\n\n\n\nIntuitively there is more reason to believe wines 1 and 2 are acceptable than with 3 or 4, for which one judge found them not metal-worthy. It seems safe to avoid wine 5, since all but one of the judges found it not to be metal-worthy.\nWe can put this information into the confusion matrix, but we’re missing information.\n\nConfusion matrix for the first five wines, with missing information as question marks.\n\n\n\ntrue value of 2-4\ntrue value of 1\nTotal\n\n\n\n\nrating of 2-4\n?\n?\n15\n\n\nrating of 1\n?\n?\n5\n\n\n\nThere is some awkwardness in imagining that the physical universe encompasses a reality including true wine ratings. It seems absurd on the face of it that the physical laws of the universe imply “wine science.” It’s useful to think of it in reverse: this reification of the ratings is a hypothesis that may be rejected by the statistics we derive from the data. If we find that raters are assigning ratings at random, there’s no evidence here for the reality of wine asthetics. On the other hand, strong indications of non-randomness need to be explained somehow.\nTaking the leap that all non-randomness is causal, and physical reality has a monopoly on causality, then something concrete in the world is behind the rating consistency. We could imagine the chemical similarities and differences of wines and their effects on human anatomy, and so on. Being “real” in this case doesn’t mean that the universe appreciates wine, only that the universe includes physical blobs called humans, and they have mechanical consistencies that correlate observations in this case. The confusion matrix is also used for calculating the causal effect of an experiment. In a medical context, the rows might be treatment categories and the columns patient outcomes. See @eubankscause for more on that.\nBy using the rating counts for each subject instead of the total, we can–surprisingly–estimate the whole confusion matrix, including the true values for each subject.\n\n\n\nSignal and noise as variability between and variability within subjects translates into rater agreement within subjects versus between subjects. The usual way to turn this idea into a number is to count the number of actual agreements and divide by the total number possible.\n\nRater agreements count, showing the maximum agreements for N raters, the number matches that are evidence versus logical implications and the ratio of evidence to matches.\n\n\n\n\n\n\n\n\n\nNumber of raters\nMaximum agreements\nEvidentiary agreements\nImplied agreements\nEvidence / Maximum\n\n\n\n\n1\n0\n-\n-\n-\n\n\n2\n1\n1\n0\n1\n\n\n3\n3\n2\n1\n.67\n\n\n4\n6\n3\n3\n.5\n\n\n5\n10\n4\n6\n.4\n\n\nN\n\\(N(N-1)/2\\)\n\\(N-1\\)\n\\(N(N-1)/2 - N + 1\\)\n2/N\n\n\n\nThe maximum agreements in the table come from the combinatorics formula “N choose 2,” counting the number of possible pairs out of N. If a group of 5 people meet and all shake hands, it’s 10 handshakes. The number of pairs grow much more rapidly than the number of raters does6.\nThere’s a rule in thumb in statistics that it’s preferable to use standard deviation instead of variance when understanding the data. That’s because the units of standard deviation are the same as the original data, whereas the variance has squared units. The same effect is going on with counting agreement. In some sense, what we want looks more like the square root of agreement.\nIntuitively, three raters agreeing doesn’t seem like three times as much evidence as two agreeing. And if we look at where those agreements are coming from, most of them are purely logical. Agreement is transitive, meaning if A agrees with B and B agrees with C, then A agrees with C. This is a logical necessity, and it’s not evidence of anything. The Evidentiary and Implied columns of the table record the distinction between the two types of matches. As the number of raters \\(N\\) increases, the logically necessary agreements greatly outnumber the evidentiary ones, so the ratio in the last column of the table decreases toward zero.\nIf we only counted the evidentiary agreements, the maximum for \\(N\\) raters would be \\(N - 1\\), which is roughly proportional to the square root of the total number of agreements. We’ll come back to that idea later. However, the well-trodden path is to estimate the probability of agreement over all the subjects, which leads to the kappa statistics.\nFor the five wine ratings, we can calculate the agreement as follows.\n\nAgreement calculation for the first five wines, showing the maximum possible matches, the actual matches, and the agreement proportion.\n\n\nWine\nRaters\nPossible\nActual\nAgreement\n\n\n\n\n1\n4\n6\n6\n1\n\n\n2\n4\n6\n6\n1\n\n\n3\n4\n6\n3\n.5\n\n\n4\n4\n6\n3\n.5\n\n\n5\n4\n6\n3\n.5\n\n\n\nFor the fifth with, there are more matches with the zeros than there are with the ones; both are counted. It is at this point what seemed like a good starting point begins to turn into a muddle, because since both the zeros and the ones can create agreement, how do we know what’s good enough agreement? What’s the worst agreement possible?\nA group of raters who agree on the 1 ratings of size \\(n+1\\) produces a number of agreements about proportional to \\(n_1^2\\), and similarly the 0 agreements produce about \\(n_0^2\\) agreements. Together that’s around \\(n_1^2 + n_0^2\\) agreements, which is less than or equal to \\((n_1 + n_0)^2\\). So we get more agreement when everyone agrees than we do when the raters split into groups. That means that the least agreement happens when the raters are evenly split, giving us a floor for agreement.\n\nMinimum rater agreement rates\n\n\n\n\n\n\n\n\nNumber of raters\nMaximum agreements\nMinimum agreements\nMinimum rate\n\n\n\n\n4\n6\n2\n.33\n\n\n6\n15\n6\n.40\n\n\n8\n28\n12\n.43\n\n\n10\n45\n20\n.44\n\n\neven N\n\\(N(N-1)/2\\)\n\\(N(N/2-1)/2\\)\n\\((N/2-1)/(N - )\\)\n\n\n\nIt’s annoying that the minimum rate of agreement changes with the number of raters. For real cases when we want to use these ideas, we might have different numbers of raters for different subjects, complicating an average minimum rate, which defines the worst case agreement.\n\n\n\nWe saw in the previous section that the minimum frequency of agreement among raters is quite large, tending toward one half of the maximum possible. Whatever statistic we create as a measure of agreement needs to take that into account if we are to make sense of the results. We’ll get maximum agreement if all the raters choose the same category (of the two choices), and we’ll get minimum agreement if they split evenly between the categories, as if they each flipped a coin to decide. So there’s an association between worst-case random raters and the minimum agreement rate. This is the basis of the chance correction in the S statistic, which assumes coin-flipping raters as the worst-case benchmark from which to measure agreement.\nThe general formula for taking into account a worst-case (random) match rate \\(m_r\\) first computes the practical range of our scale, which is \\(1 - m_r\\), since the statistic can’t be less than \\(m_r\\). The amount of agreement observed is then pinned to this scale as a “chance-corrected” match rate with\n\\[\\kappa = \\frac{m_o - m_r}{1 - m_r}\\]\nThe S statistic is a special case of this formula, where \\(m_r = .5\\). The sample of wine ratings can be used to illustrate. Recall that in Table ?? we simplified the judges’ scale of “no medal” to “gold medal” to a binary classification between “no medal” (coded as zero) or “any medal” (coded as one). We saw in Table ?? that there were 30 maximum agreements among the four raters, and 21 actual agreements. The minimum is when the ratings are evenly split on each case, generating two agreements for each wine, or 10 total. An S-like statistic would be calculated like this.\n\\[\n\\begin{aligned}\nm_o &= 21/30 = .70 \\\\\nm_r &= 10/30 = .33 \\\\\n\\kappa_s &= \\frac{.70 - .33}{1 - .33} = .47\n\\end{aligned}\n\\] The kappa is now on a scale of zero (worse) to one (perfect agreement). Interpreting what the value means is difficult because it’s on the wrong scale as noted above. A value close to one half sounds mediocre, but not terrible, but we’re really looking for something like the square root of the agreement rate, which is .69. That value corresponds better to the intuition that the agreement in the table looks pretty good.\nThe coin-flipping assumption for randomness can be replaced with other assumptions. This has the effect of increasing the assumed worst-case agreement rate and hence decreasing kappa values. Perhaps the most popular choice is to reason that the raters are more likely to randomly pick the most common category so the coin should be weighted according to the rating distribution.\nFor example, the sample of wine ratings in Table ?? has 15 ratings of 1 and 5 of 0, or 75% 1-ratings. The worst case match rate is then the product of the proportions of 1 ratings, or \\(m_r = (.75)(.75) + (.25)(.25) = .625\\). This is the assumption of the Fleiss kappa. Note that the actual worst case match rate is still close to one half (because of the math), so it’s possible to get a negative Fleiss kappa if the assumption about the distribution of randomness fails.\nThe calculation in this case is analogous to the following.\n\\[\n\\begin{aligned}\nm_o &= 21/30 = .70 \\\\\nm_r &= .625 \\\\\n\\kappa_f &= \\frac{.70 - .625}{1 - .625} = .20\n\\end{aligned}\n\\] If the raters really were flipping coins, and \\(m_0 = .5\\), kappa would be negative.\nThe actual calculations of these statistics are complicated by the choice of using exact combinatorical counts or long-run estimates. This distinction need not concern us in here.\nThe deflation of the rater agreement statistic in the Fleiss calculation above makes it clear that assumptions about random agreement are critical. A researcher may justifiably wonder which value is correct: are the wine judges in pretty good agreement, or is the judging mostly random? For historical reasons, rather than treating rater disagreement as a parameter to be discovered, the field has evolved to propose many variations of the kappa formula, with different assumptions about the worst-case agreement rate.\nA limitation of most of the kappa statistics is that they make the simplifying assumption that a pair of raters reaches agreement randomly if both raters are random. This omits the possibility that one rater is accurate and the other isn’t.\n\n\n\nThere are by now several statistics advertised as rater agreement statistics, including Cohen’s kappa, Fleiss’s kappa, Scott’s pi, Krippendorff’s alpha, S, and AC1. For convenience, these statistics will be referred to generically as “kappas.” Researchers who want a simple answer are faced with a bewildering set of options and claims about them. In @vach2023gwet we get a sense of the situation.\n\nGwet’s AC1 has been proposed as an alternative to Cohen’s kappa in evaluating the agreement between two binary ratings. This approach is becoming increasingly popular, and researchers have been criticized for still using Cohen’s kappa. However, a rigorous discussion of properties of Gwet’s AC1 is still missing.\n\nThe authors illustrate both the need for clarification and the faddishness that research communities can adopt when lacking real criteria. The kappas are ill-suited to answer such questions, and the proliferation of agreement statistics makes it difficult to compare results across studies or to know which one to use. Given pressures to publish results, there may be a tendency to use the statistic that gives the highest value, or to use the one that is most familiar. See @button2020inter for a discussion of rater agreement statistics in the context of the “crisis in confidence in psychological research.”\nIt’s not just difficult to know what kappa to use, there are no meaningful guides to interpreting the results. “There is a wide distinction in the elucidation of Kappa values, and several efforts have been made to assign practical meaning to calculated Kappa values,” wrote the authors of , who mention a widely-used heuristic found in that proposed a translation between numerical values of kappa and qualitative descriptions of agreement, such as \\(kappa \\ge .81\\) is “almost perfect.” The categories are arbitrary, do not translate well between different agreement statistics (which can give different values for the same data) and their assumptions, and do not provide insight into how to improve ratings.\nThe agreement statistics for ratings have problems that should now be evident. The goal is to understand a three-dimensional relationship between ratings and true values, but the statistics are single parameters. A second parameter, the worst case baseline, is buried in an assumption, which varies by kappa, and is not tested for fit to the data. The result is a bewildering array of choices for rater agreement measures. As a result of this confusion, different cultures have emerged. If getting published is the goal, then higher agreement rates are more desirable, so a researcher can shop around for the “best” one.\nA research agenda was suggested in that can be paraphrased as understanding (1) the true category of each subject, (2) the accuracy of raters, (3) truth and accuracy within subpopulations of subjects, (4) conditions that cause disagreement, and (5) what distinguishes “for cause” agreement and random agreement.\n\n\n\nThe geyser of data produced in the information age has led to new methods of analyzing it that fall generally in the description of “machine learning (ML).” Assigning categories to subjects is a common task in machine learning, and the field has developed a number of methods to do so, including neural networks, support vector machines, and random forests. The goal is to assign categories to subjects in a way that generalizes to new subjects. The methods are often evaluated by comparing the predicted categories to the true categories, and the results are summarized in a confusion matrix.\nIt may come as a surprise to both behavioral scientists and machine learning researchers that they seem to have independently worked on the rater agreement problem, since the two fields’ literatures don’t seem to overlap. Articles on rater agreement don’t cite machine learning papers, and vice versa. However, the two fields are working on the same problem: how to measure the accuracy of a categorization when we don’t have the true categories to reference.\nThere are differences in the methods and philosophies of the two cultures. The kappa approach is implicitly backward-looking, asking “how accurate were these ratings,” with the assumption that the measured accuracy (a kappa) will carry forward to future instances. The ML approach is more forward-looking, with a suite of tools like cross-validation and a vocabulary (bias-variance trade-off) to measure generalizability. As we have seen, the kappa approach is to reason out a plausible chance-correction calculation and use it to produce the single-parameter kappa. The ML approach is to estimate three parameters with regression models. Finally, the kappas are grounded in the psychology of human classification and come with philosophical links to epistemology, which I described in Section 1.2. The ML algorithms are just statistics and code, barren of philosophical considerations.\nThe advantage of the ML approaches over the kappas is that the three parameter models avoid the confusions of the kappa zoo. But because they lack philosophical grounding, they run into an embarrassment of riches: three parameters is sometimes too many parameters for a model, so there can be multiple solutions. There are work-arounds, but it’s the same species of ad hoc reasoning that causes the problems of the kappa zoo. Fortunately, there’s a way to combine the best of both cultures to avoid most of these problems.\n\n\n\nWe can now describe a regression model that allows us to estimate the four proportions that appear in the confusion matrix (see Section 1.3). Since the four cells sum to one, there are three free parameters to estimate.\n\n: Confusion matrix with entries to be filled in by the model estimates. The four numbers are proportions and sum to one, leaving three free parameters to estimate. C0 = class zero, and C1 = class one, standing in for any binary categories we might choose.\n\n\n\nTrue C1\nTrue C0\n\n\n\n\nClassified C1\n\n\n\n\nClassified C0\n\n\n\n\n\nTwo philosophical assumptions are needed to get started:\n\nThe classifications are real in the sense that true values exist.\nThe true values have a binary causal effect on the classification process.\n\nThese assumptions are provisional like hypothesis in statistics; the evidence will provide some support for the assumptions, ranging from no support at all to quite good. The reality of the truth values posed in assumption one is not like Plato’s cave shadows. In the case of wine judging, we can’t say there is a universal ideal for good wine, but if rater agreement is high we can say that some physical causal process exists for translating the observable subject (tasting the wine) into a category. So in that sense the category exists as part of the world. The translation is imperfect, because the conditions are not always perfect for the cause to happen, as with the Gettier problems7.\nThis may seem fiddly, but it gives us a place to start. For any given subject to be rated (e.g. object to be classified) we provisionally assume that there’s a latent truth value that we can never know, but might find evidence for. The second assumption then allows us to provisionally assume that the causal effect of the true value in the context of the classification process has the following nature:\n\nThe rater either assigns the correct class due to the causal pathway operating to connect the observation to the class (justified true knowledge), or\nSomething goes wrong with the causal pathway (the conditions weren’t quite right, etc.) and the classification is assigned non-causally, which is to say randomly.\n\nThe key to this is that the cause either works to connect the true value to the rater’s assigned value, or it fails completely. There’s no “partial cause.” When it fails, the rating is generated from a random process called a Bernoulli trial. It’s the simplest possible type of randomness, taking only two values with some fixed probability, like flipping a weighted coin.\nLet’s take inventory of the three parameters that come from the reasoning above.\n\nAmong the subjects being rated there is a fraction \\(t\\) that are in reality Class 1, with the remaining \\(1-t\\) being Class 0.\nAmong the ratings there is a fraction \\(a\\) that are accurate ratings (justified true knowledge) where the causal connection worked.\nFor the remaining \\(1-a\\) ratings, the causal connection failed, and there is some probability \\(p\\) that describes the frequency that they are assigned to Class 1. The remaining \\(1-p\\) are assigned Class 0.\n\nAll three of these parameters are proportions ranging from zero to one, and can be treated as probabilities that we estimate from appropriate regression models. Setting it up this way, instead of the usual machine learning parameterization avoids the most significant problem with non-identifiability (multiple solutions), which is class-switching. That happens when the model can fit the data, but is agnostic about which class is which, and so attempts to fit the model both ways at once.\nThe classification model with \\(t\\), \\(a\\), and \\(p\\) can be most easily understood in a tree diagram. For convenience, we’ll put a “bar” over a parameter name to indicate that it’s subtracted from one, e.g. \\(\\bar{t} = 1-t\\). This makes the notation more compact, and we always have \\(a + \\bar{a} = 1\\), etc.\n\n\n\ntap diagram\n\n\nThe diagram shows probabilistic links between states of the world and rater classifications. The lowercase letters are probabilities (recall that the bar means subtract from one to make the complementary probability, so \\(\\bar{.2} = .8\\)). We read the tree top to bottom, hence the name for the model: t-a-p, an abbreviation of the tree structure.\nTo understand the model, consider a judge (the rater) tasting one of the wines (the subject at the top of the tree). We’re at the top of the tree, where there exists a true classification for the wine as either acceptable (C1) or unacceptable (C0). In the universe of wines the fraction of acceptable ones is \\(t\\), and for a given wine we can imagine a hidden variable \\(T\\) that records its true class. Suppose this one is acceptable, so that \\(T = C_1\\). That’s the set-up for the wine taster, who doesn’t know the value of \\(T\\), but is trying to ascertain it. If this assessment is accurate (a case of justified true belief), then we follow the left branch of the diagram again, and the judge’s classification \\(C\\) is assigned the value \\(\\hat{C_1}\\). The hat indicates that this is an empirical rating, so we can keep it distinct from \\(C_1\\), which is the true value. The accurate judgments happen with probability \\(a\\), and if it’s not accurate, the judge’s classification will be randomly determined. The rating will be assigned \\(\\hat{C_1}\\) with probability \\(p\\), and otherwise will get the \\(\\hat{C_0}\\) rating.\nIt’s important to note that the value of \\(T\\) is the same for the same wine. So when four judges all rate the same wine, they are all either on the left side of the diagram (if the wine is acceptable in reality) or on the right side (if not). The ratings are determined only by \\(a\\) and \\(p\\) at that point.\nFor a wine chosen at random, we can compute the probabilities of rater classifications. An acceptable wine will be classified accurately with a proportion of \\(ta\\), multiplying the probabilities along the leftmost edge from top to bottom. An inaccurate rating of \\(\\hat{C_1}\\) (acceptability) can come from either the left or right side of the diagram, and if we add those together we get \\(t\\bar{a}p + \\bar{t}\\bar{a}p\\), and since \\(t + \\bar{t} = 1\\) that expression reduces to \\(\\bar{a}p\\).\nAssuming we can estimate the three parameters from the data, we can then populate the confusion matrix by tracing the diagram down to each of the four outcomes, and multiplying probabilities as we go.\n\nThe t-a-p model’s correspondence to the confusion matrix. Terms in parentheses are inaccurate ratings.\n\n\n\n\n\n\n\n\nTrue C1\nTrue C0\n\n\n\n\nClassified C1\n\\(ta + (t\\bar{a}p)\\)\n\\((\\bar{t}\\bar{a}p)\\)\n\n\nClassified C0\n\\((t\\bar{a}\\bar{p})\\)\n\\(\\bar{t}a + (\\bar{t}\\bar{a}\\bar{p})\\)\n\n\n\nThe table demonstrates that if the t-a-p model fits the data and we are able to estimate the three parameters, it is a general answer to the rater agreement question. The later sections show how S, Fleiss kappa, and other statistics are special cases of t-a-p models.\n\n\n\nThe sections above used five rows of the wine judging data for illustration. We now use the whole data set of 183 wines each rated by four judges to estimate the t-a-p parameters. The included app was used (see Chapter 4) for the analysis.\nThe binary classification illustrated above was to convert the original scale (1 = no medal, 2 = bronze, 3 = silver, 4 = gold medal) to a binary outcome where 1 = any medal (bronze, silver, gold), and 0 = no medal. We called the Class 1 category “acceptable.” Using the app to analyze that binary comparison we get the following output.\n\n\n\nWine rating t-a-p analysis comparing C1 = ratings 1,2,3 (in-class) to C0 = rating 0.\n\n\nThe acceptable wines are the primary (in-) class, and the plot shows the distribution of ratings for acceptability. Since there are four judges, the possibilities are:\n\nNone of them could find the wine acceptable. This is the 0 at the left end of the plot. It happens for about 6% of the wines.\nAll four of them could find the wine acceptable. This is the 4 (four judges) at the right end of the plot. That happened for about 49% of the wines.\nSomething in between, where the judges disagree. In that case we can have from one to three ratings of “acceptable,” corresponding to those points on the plot.\n\nThe lollipops (black lines with dots on top) show the empirical data straight out of the ratings. The dashed blue line is the model fit, showing what the distribution would be if the model were exactly correct. Here, “model” means applying the estimated parameters to generate the distribution.\nThe estimated parameters are found at the top of the plot:\n\n\\(t\\) = .73 estimates that 73% of wines are acceptable in reality. This is more than the rate of unanimous agreement, which we saw above was only 49%.\n\\(a\\) = .54 estimates that the judges make accurate ratings (justified true belief) more than half the time. The rest of the ratings are assumed to be random.\n\\(p\\) = .78 estimates that when random ratings are made, judges choose “acceptable” 78% of the time. Notice that this is close to the estimated actual rate of 73% (the estimated value of \\(t\\) above). This turns out to be a desirable quality in judges. See Chapter 3 for details, especially the section on the Fleiss kappa.\n\nNote that the randomness of inaccurate ratings is not a conscious choice of raters. They aren’t giving up and flipping a coin. They still have a reasonable basis for making a rating, and may be quite confident about it. The assumption is that something Gettier-like has gone wrong with the causal process that links the input (wine) to output (rating), which flips the process from deterministic to stochastic. As with any regression model, the world doesn’t have to actually be that way; it’s just an assumption to allow us to create the model. Then we check to see if the data matches the model. In this case, the data matches the model pretty well, as we can visually assess by comparing the blue dashed line to the lollipops. More formal tests of agreement will have to wait until later.\nThe wine ratings comprise four categories that are naturally ordered from 1 = “no medal” to 4 = “gold medal.” That kind of data is common: from surveys with Strongly Disagree to Strongly Agree scales, from consumer ratings, and many other sources. We humans seem to have a natural facility to think of descriptions along an imaginary intensity scale. This data type is called “ordinal,” meaning we can put the categories in order.\nThe t-a-p model is designed to work with binary data. It’s possible to switch to a multinomial model (see the ML version in ???), but it’s common to analyze ordinal scales using binary distinctions. I’ll show how that works with the wine data. Along the 1,2,3,4 scale, there are three sequential cut-points where the commas are. The first one we can denote “1|2”, which splits the scores into two sets: an in-class {1} and and out-class {2,3,4}. This is the same thing as the Class 1 and Class 0 distinction from earlier. The second cut-point is at 2|3, splitting the data into ratings {1,2} and {3,4}. The plot shows the fraction of ratings in the in- and out-class for each cut-point.\n\n\n\nWine ratings divided into binary groups by cutpoint.\n\n\nThe 3|4 bar of the plot shows us that 88% of ratings are in {1,2,3}, so the remaining 12% are 4s (gold medal). As a validity check, there are 183 wines, each rated by four judges, so that should be 732 ratings. That looks like the height on the bars, so it checks out.\nFor a given cut-point, a wine rating will either be in the in-class or out-class This converts the scale to binary, and then we can estimate the model parameters. We repeat the t-a-p model for each cut-point to create a pattern like the one in the plot below."
  },
  {
    "objectID": "introduction.html#sec-intro-overview",
    "href": "introduction.html#sec-intro-overview",
    "title": "Introduction",
    "section": "",
    "text": "One of the powers of human language is the facility to agree or disagree with a statement. This chapter introduces the concept of quantifying agreement, which is a way to measure the extent to which two or more people agree that a statement is true or fall in cases where they have independently made up their minds. A jury would not meet that qualification, because their purpose to discuss and mull over evidence to reach unanimous agreement. However, a straw poll taken before conversations begin could be considered a set of independent opinions.\nIn ordinary discourse, we use informal agreement to decide what to each for dinner, what movie to watch, or what to do on a Saturday afternoon. In science, we use formal agreement to decide whether a new drug is effective, whether a new theory is correct, or whether a new technology is safe. In politics, we use agreement to decide who should be elected, what laws should be passed, or what policies should be implemented. Science is generally the only domain where we work to ensure that the data (measurements instead of opinions) are taken independently, so they don’t influence one another. This assumption of independence makes the calculations easier, but it is not always realistic. Another limiting assumption is that the process of rating cases is repeatable using the same process. Many real-world situations are not repeatable on a personal level, for example deciding where to go on vacation. Sometimes we can hypothesize repeatability by aggregating over many people who go on vacation, for example.\nThe combination of independence and repeatability are abbreviated in probability theory as “iid” for “independent and identically distributed.” This is a powerful assumption that allows us to reach strong conclusions. However, it is not always realistic.\nThe focus of this guide is on the statistical methods used to quantify agreement between humans, who we’ll call “raters” who assign “ratings” to “subjects.” This describes a familiar case like a product review with a five-star rating system. However, the same methods can be used to quantify agreement between any two entities that can assign categories to cases. A fascinating example comes from an article in The Atlantic Magazine, here describing an analysis of a very old text called the Voynich manuscript.\n\nDavis magnified the handwriting, she noticed subtle variations. In certain places, the script was more cramped or more likely to slant as it crossed the page. She tested this observation by picking a letter that didn’t appear often and tracking it across the manuscript’s pages. Its style, she saw, varied among groups of pages but not within those groups. This suggested that the differences—larger or smaller loops, straighter or curvier crossbars, longer or shorter feet—were the product of different scribes rather than of one scribe writing the same letter in different ways.\n\nSee Davis’s paper for details1. For our purposes, the “raters” are scholars who categorize writing styles, the “subjects” are sections (folios) of the manuscript they are reviewing, and the presumed different scribes are the categories we generically refer to as “ratings.”\n\nSelected rows from a table at the website for the manuscript: https://voynich.nu/index.html, comparing an early attempt to classify writing “hands” (Currier) to the recent classification (Davis). Hypothesized identity is unique to each column with the numbers denoting individual scribes.\n\n\nFolio(s)\nPrescotte Currier\nLisa Fagin Davis\n\n\n\n\nf41, f48, f57r\n2\n5\n\n\nfRos (obverse)\n3\n2\n\n\nfRos (main)\n3\n4\n\n\nf87, f88, f93, 96\n4\n1\n\n\nf94, f95\n5\n3\n\n\nf103, f104, f106\nX\n3\n\n\nf105\nY\n3\n\n\n\nIn the table, Currier thought that the obverse and main folio fRos were in a single hand, whereas Davis has two scribes. Conversely, Davis has the last three rows as the same author while Currier has three. This is an example of how science works toward consensus by placing observations into categories. Agreements are good for pointing to what might be real, and disagreements are for honing theory.\nThe statistical question for data like this is to quantify a degree of separation between categories., to compare variation within groups to variation between groups. One application is an idea called “signal-to-noise ratio,” which is a measure of how much of the variation in the data is due to the thing we’re interested in (signal) versus how much is due to random variation (noise). In the manuscript example, the signal is the commonality of handwriting within contiguous sections, and the noise is variation within each section. Reliability of measures is the same idea in different clothing, where we might formula reliability as between-subject variation divided by total variation. As an example, college grade averages (GPA) can have a reliability of about 0.8, meaning that 80% of the variation in grades is due to differences between students, and 20% is due to variation in how individual students earn grades. This makes GPA useful as a statistical measure, for example in predicting career outcomes [cite eubanks, et al].\nHow do we summarize all this to reach a formal conclusion? This formality is in service of the question “what is the truth?”"
  },
  {
    "objectID": "introduction.html#sec-intro-knowledge",
    "href": "introduction.html#sec-intro-knowledge",
    "title": "Introduction",
    "section": "",
    "text": "It turns out that the questions “what is true?” and “what is knowledge, anyway?” are difficult to answer. There is a great body of work in epistemology, the philosophical study of knowledge, from which we can take a clever idea to get started.\nTo start, we have to imagine that “truth” exists. Since we are talking about classifications of subjects into one of two categories, we must commit to the idea that these categories (words that humans made up) might correspond to something in objective reality (whatever that is). We can easily make up descriptions that are fantasies, like unicorns or flying whales, and the goal is to have a way to distinguish these phantasms from real things like penguins and taxes. To do so, we have to imagine that by some mysterious means, our intellects and discourse can access Nature and name things.\nWords are slippery things, however. Although there are definitions, the real meaning of “pizza” is (ironically) ineffable; one could spend a whole career trying to understand it [cite the guy who wrote four vol on it, msft cto]. Let’s take Aristotle’s definition of a man as a featherless biped.\nOur goal is a set of classifications “man” or “not man” generated from independent observers (we call them raters) who each inspect some objects in the world (we call them subjects) and assign “man” or “not man” to each. Then we’ll take that data and apply the between- and within-variance idea.\nTherefore the definition isn’t enough. We also need a procedure for generating the data the way we want. To ensure independence of raters, we don’t want them to collaborate on decisions, for example. In the sciences, procedures can be quite elaborate, as in detecting and classifying Higgs bosons, which requires a huge particle accelerator.\nThe story is that Diogneses the Cynic tossed a plucked chicken at Aristotle’s feet and declared “here’s your man.” [cite the book]. The example drives a wedge between the definition “featherless biped” and what is actually meant by “man.” Outside of math and science, definitions and procedures are fuzzy in this way. Rater agreement statistics attempt to defuzz the ratings, to understand the reality behind them.\nHere’s where the idea from epistemology is key, because it gets to the nature of the fuzziness of classification. First we define knowledge as justified true belief (JTB), and then sort out where the fuzziness is.\nThere are three parts to JTB, and we already talked about truth; as a prerequisite we must believe that true statements can be made about the world, which means accessing something real and describing it. The third piece of JTB is belief, which we’ll take to be the classification assigned by a rater. If our rater looks at a horse and writes down “not unicorn,” we’ll assume that the classification represents what the rater believes. This runs into problems if we want to consider adversarial raters, like someone paid to rate consumer products, or an angry customer rating a service poorly for extraneous reasons. We won’t consider those possibilities here.\nThe first piece of JTB is where the fuzziness lives: justification. Outside of math, justification in words isn’t complete2. In the case of Diogenes versus Aristotle, an follower of the latter might reason that the plucked chicken meets the great philosopher’s definition, and therefore is a man. Others might use common sense to conclude that it’s not a man. There is more than one way to justify belief, but not all of them result in true classifications.\nThe JTB idea was dealt a blow by Gettier, who posed a number of examples where justification goes awry, but the idea is much older. Consider this example from Dharmottara in 779 AD3.\n\nA desert traveller is searching for water. He sees, in the valley ahead, a shimmering blue expanse. Unfortunately, it’s a mirage. But fortunately, when he reaches the spot where there appeared to be water, there actually is water, hidden under a rock. Did the traveller know, as he stood on the hilltop hallucinating, that there was water ahead?\n\nThe traveller had a belief that turned out to be true, but the justification was incorrect. The philosphers are concerned with whether or not this constitutes knowledge4, but we’re more interested in something simpler. That is to categorize the ways truth and belief can happen. If they don’t coincide (if we believe something false), then we’ll assume that the justification was incorrect. As the Gettier problems and the discussion they generated demonstrate, there are a lot of complicated ways in which justification can go awry.\nIn the literature of rater agreement, one can find similar language. In , we find “The \\(\\alpha\\) agreement parameter is defined as the proportion of a population of items that are classified identically ‘for cause’ by two classifiers, the remaining items being classified at random […].” The author suggests that some subjects are more difficult to rate than others, which hints at modeling each subject’s probability of being in a category, in truth. This idea was explored in @dawid1979maximum, who also allowed that raters might have different proficiencies.\nAnother instance comes from the derivation of a rater agreement statistic called AC1 @gwet2008computing. The author notes that “the assumption [is] that any rating that is not random will automatically lead to a correct classification, while a random rating leads to a correct classification with probability 1/2.” The idea is that we conceptually separate accurate ratings from random ones, which then means we must consider the probability distribution of the random ones. The random assignments will sometimes be correct, but for the wrong reason–just like the Gettier problems."
  },
  {
    "objectID": "introduction.html#sec-intro-confusion",
    "href": "introduction.html#sec-intro-confusion",
    "title": "Introduction",
    "section": "",
    "text": "Diagnosing illness is an important classification problem, and the simplest version is a blood test that returns a positive or negative result, perhaps scanning for an antibody. There are four possibilities, which can be organized into what is commonly called a confusion matrix. Suppose that we consider a statement S about the world, like “this patient has the flu.” Using a definition and process we reach a conclusion, so that we believe S to be true or false. This generates the classifications we’re interested in: this is what the raters do, generating lots of S or not-S data on cases.\nThe truth is often inaccessible, but recall that we assume that it exists, so that the statement really is true or false: each patient really does have the flu or does not. Here are the four possibilities:\n\nSample confusion matrix\n\n\n\n\n\n\n\n\nS is true\nS is false\n\n\n\n\nBelieve S is true\nTrue positive cases\nFalse positive cases\n\n\nBelieve S is false.\nFalse negative cases\nTrue negative cases\n\n\n\nNotice that there are four spots in the table where data can go: if we had perfect knowledge we could count the number of true positive cases, etc. If we converted these numbers to proportions by dividing by the table, we have three parameters left. This foreshadows what comes later: the heart of the classification problem (in its simple form) is a three-parameter problem5.\nThe limitation of casting the confusion matrix into a single-parameter statistics was noted by @cicchetti1990high, who suggested “two separate indexes of proportionate agreement in the observers’ positive and negative decisions. […] Why do we want a single index?” Another example comes from the motivation for the AC1 rater statistic @gwet2008computing. It separately considers rater true positives and true negatives, but in to get to a single parameter, the model assumes that these are identical for each rater.\nFor a concrete illustration, consider the wine judging data used in @hodgson2008examination. The first five rows look like this:\n\nWine ratings showing rows 1-5 of 183, taken from the study by Hodgson (personal communication).\n\n\nWine\nJ1\nJ2\nJ3\nJ4\n\n\n\n\n1\n3\n3\n3\n3\n\n\n2\n3\n3\n3\n3\n\n\n3\n3\n3\n2\n4\n\n\n4\n3\n4\n3\n1\n\n\n5\n4\n2\n1\n1\n\n\n\nThe 1-4 scale is ordinal, meaning that we imagine quality to increase with the numerical value. For this discussion, we will reduce that complexity to a binary classification: acceptable wine (ratings 2-4) or wine to avoid (rating 1). This simplifies the table to.\n\nSimplified wine ratings, where 1 means “acceptable” and 0 means “avoid.”\n\n\nWine\nJ1\nJ2\nJ3\nJ4\n\n\n\n\n1\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n1\n\n\n3\n1\n1\n0\n1\n\n\n4\n1\n1\n1\n0\n\n\n5\n1\n0\n0\n0\n\n\n\nIntuitively there is more reason to believe wines 1 and 2 are acceptable than with 3 or 4, for which one judge found them not metal-worthy. It seems safe to avoid wine 5, since all but one of the judges found it not to be metal-worthy.\nWe can put this information into the confusion matrix, but we’re missing information.\n\nConfusion matrix for the first five wines, with missing information as question marks.\n\n\n\ntrue value of 2-4\ntrue value of 1\nTotal\n\n\n\n\nrating of 2-4\n?\n?\n15\n\n\nrating of 1\n?\n?\n5\n\n\n\nThere is some awkwardness in imagining that the physical universe encompasses a reality including true wine ratings. It seems absurd on the face of it that the physical laws of the universe imply “wine science.” It’s useful to think of it in reverse: this reification of the ratings is a hypothesis that may be rejected by the statistics we derive from the data. If we find that raters are assigning ratings at random, there’s no evidence here for the reality of wine asthetics. On the other hand, strong indications of non-randomness need to be explained somehow.\nTaking the leap that all non-randomness is causal, and physical reality has a monopoly on causality, then something concrete in the world is behind the rating consistency. We could imagine the chemical similarities and differences of wines and their effects on human anatomy, and so on. Being “real” in this case doesn’t mean that the universe appreciates wine, only that the universe includes physical blobs called humans, and they have mechanical consistencies that correlate observations in this case. The confusion matrix is also used for calculating the causal effect of an experiment. In a medical context, the rows might be treatment categories and the columns patient outcomes. See @eubankscause for more on that.\nBy using the rating counts for each subject instead of the total, we can–surprisingly–estimate the whole confusion matrix, including the true values for each subject."
  },
  {
    "objectID": "introduction.html#sec-intro-agreement",
    "href": "introduction.html#sec-intro-agreement",
    "title": "Introduction",
    "section": "",
    "text": "Signal and noise as variability between and variability within subjects translates into rater agreement within subjects versus between subjects. The usual way to turn this idea into a number is to count the number of actual agreements and divide by the total number possible.\n\nRater agreements count, showing the maximum agreements for N raters, the number matches that are evidence versus logical implications and the ratio of evidence to matches.\n\n\n\n\n\n\n\n\n\nNumber of raters\nMaximum agreements\nEvidentiary agreements\nImplied agreements\nEvidence / Maximum\n\n\n\n\n1\n0\n-\n-\n-\n\n\n2\n1\n1\n0\n1\n\n\n3\n3\n2\n1\n.67\n\n\n4\n6\n3\n3\n.5\n\n\n5\n10\n4\n6\n.4\n\n\nN\n\\(N(N-1)/2\\)\n\\(N-1\\)\n\\(N(N-1)/2 - N + 1\\)\n2/N\n\n\n\nThe maximum agreements in the table come from the combinatorics formula “N choose 2,” counting the number of possible pairs out of N. If a group of 5 people meet and all shake hands, it’s 10 handshakes. The number of pairs grow much more rapidly than the number of raters does6.\nThere’s a rule in thumb in statistics that it’s preferable to use standard deviation instead of variance when understanding the data. That’s because the units of standard deviation are the same as the original data, whereas the variance has squared units. The same effect is going on with counting agreement. In some sense, what we want looks more like the square root of agreement.\nIntuitively, three raters agreeing doesn’t seem like three times as much evidence as two agreeing. And if we look at where those agreements are coming from, most of them are purely logical. Agreement is transitive, meaning if A agrees with B and B agrees with C, then A agrees with C. This is a logical necessity, and it’s not evidence of anything. The Evidentiary and Implied columns of the table record the distinction between the two types of matches. As the number of raters \\(N\\) increases, the logically necessary agreements greatly outnumber the evidentiary ones, so the ratio in the last column of the table decreases toward zero.\nIf we only counted the evidentiary agreements, the maximum for \\(N\\) raters would be \\(N - 1\\), which is roughly proportional to the square root of the total number of agreements. We’ll come back to that idea later. However, the well-trodden path is to estimate the probability of agreement over all the subjects, which leads to the kappa statistics.\nFor the five wine ratings, we can calculate the agreement as follows.\n\nAgreement calculation for the first five wines, showing the maximum possible matches, the actual matches, and the agreement proportion.\n\n\nWine\nRaters\nPossible\nActual\nAgreement\n\n\n\n\n1\n4\n6\n6\n1\n\n\n2\n4\n6\n6\n1\n\n\n3\n4\n6\n3\n.5\n\n\n4\n4\n6\n3\n.5\n\n\n5\n4\n6\n3\n.5\n\n\n\nFor the fifth with, there are more matches with the zeros than there are with the ones; both are counted. It is at this point what seemed like a good starting point begins to turn into a muddle, because since both the zeros and the ones can create agreement, how do we know what’s good enough agreement? What’s the worst agreement possible?\nA group of raters who agree on the 1 ratings of size \\(n+1\\) produces a number of agreements about proportional to \\(n_1^2\\), and similarly the 0 agreements produce about \\(n_0^2\\) agreements. Together that’s around \\(n_1^2 + n_0^2\\) agreements, which is less than or equal to \\((n_1 + n_0)^2\\). So we get more agreement when everyone agrees than we do when the raters split into groups. That means that the least agreement happens when the raters are evenly split, giving us a floor for agreement.\n\nMinimum rater agreement rates\n\n\n\n\n\n\n\n\nNumber of raters\nMaximum agreements\nMinimum agreements\nMinimum rate\n\n\n\n\n4\n6\n2\n.33\n\n\n6\n15\n6\n.40\n\n\n8\n28\n12\n.43\n\n\n10\n45\n20\n.44\n\n\neven N\n\\(N(N-1)/2\\)\n\\(N(N/2-1)/2\\)\n\\((N/2-1)/(N - )\\)\n\n\n\nIt’s annoying that the minimum rate of agreement changes with the number of raters. For real cases when we want to use these ideas, we might have different numbers of raters for different subjects, complicating an average minimum rate, which defines the worst case agreement."
  },
  {
    "objectID": "introduction.html#chance-correction",
    "href": "introduction.html#chance-correction",
    "title": "Introduction",
    "section": "",
    "text": "We saw in the previous section that the minimum frequency of agreement among raters is quite large, tending toward one half of the maximum possible. Whatever statistic we create as a measure of agreement needs to take that into account if we are to make sense of the results. We’ll get maximum agreement if all the raters choose the same category (of the two choices), and we’ll get minimum agreement if they split evenly between the categories, as if they each flipped a coin to decide. So there’s an association between worst-case random raters and the minimum agreement rate. This is the basis of the chance correction in the S statistic, which assumes coin-flipping raters as the worst-case benchmark from which to measure agreement.\nThe general formula for taking into account a worst-case (random) match rate \\(m_r\\) first computes the practical range of our scale, which is \\(1 - m_r\\), since the statistic can’t be less than \\(m_r\\). The amount of agreement observed is then pinned to this scale as a “chance-corrected” match rate with\n\\[\\kappa = \\frac{m_o - m_r}{1 - m_r}\\]\nThe S statistic is a special case of this formula, where \\(m_r = .5\\). The sample of wine ratings can be used to illustrate. Recall that in Table ?? we simplified the judges’ scale of “no medal” to “gold medal” to a binary classification between “no medal” (coded as zero) or “any medal” (coded as one). We saw in Table ?? that there were 30 maximum agreements among the four raters, and 21 actual agreements. The minimum is when the ratings are evenly split on each case, generating two agreements for each wine, or 10 total. An S-like statistic would be calculated like this.\n\\[\n\\begin{aligned}\nm_o &= 21/30 = .70 \\\\\nm_r &= 10/30 = .33 \\\\\n\\kappa_s &= \\frac{.70 - .33}{1 - .33} = .47\n\\end{aligned}\n\\] The kappa is now on a scale of zero (worse) to one (perfect agreement). Interpreting what the value means is difficult because it’s on the wrong scale as noted above. A value close to one half sounds mediocre, but not terrible, but we’re really looking for something like the square root of the agreement rate, which is .69. That value corresponds better to the intuition that the agreement in the table looks pretty good.\nThe coin-flipping assumption for randomness can be replaced with other assumptions. This has the effect of increasing the assumed worst-case agreement rate and hence decreasing kappa values. Perhaps the most popular choice is to reason that the raters are more likely to randomly pick the most common category so the coin should be weighted according to the rating distribution.\nFor example, the sample of wine ratings in Table ?? has 15 ratings of 1 and 5 of 0, or 75% 1-ratings. The worst case match rate is then the product of the proportions of 1 ratings, or \\(m_r = (.75)(.75) + (.25)(.25) = .625\\). This is the assumption of the Fleiss kappa. Note that the actual worst case match rate is still close to one half (because of the math), so it’s possible to get a negative Fleiss kappa if the assumption about the distribution of randomness fails.\nThe calculation in this case is analogous to the following.\n\\[\n\\begin{aligned}\nm_o &= 21/30 = .70 \\\\\nm_r &= .625 \\\\\n\\kappa_f &= \\frac{.70 - .625}{1 - .625} = .20\n\\end{aligned}\n\\] If the raters really were flipping coins, and \\(m_0 = .5\\), kappa would be negative.\nThe actual calculations of these statistics are complicated by the choice of using exact combinatorical counts or long-run estimates. This distinction need not concern us in here.\nThe deflation of the rater agreement statistic in the Fleiss calculation above makes it clear that assumptions about random agreement are critical. A researcher may justifiably wonder which value is correct: are the wine judges in pretty good agreement, or is the judging mostly random? For historical reasons, rather than treating rater disagreement as a parameter to be discovered, the field has evolved to propose many variations of the kappa formula, with different assumptions about the worst-case agreement rate.\nA limitation of most of the kappa statistics is that they make the simplifying assumption that a pair of raters reaches agreement randomly if both raters are random. This omits the possibility that one rater is accurate and the other isn’t."
  },
  {
    "objectID": "introduction.html#the-kappa-zoo",
    "href": "introduction.html#the-kappa-zoo",
    "title": "Introduction",
    "section": "",
    "text": "There are by now several statistics advertised as rater agreement statistics, including Cohen’s kappa, Fleiss’s kappa, Scott’s pi, Krippendorff’s alpha, S, and AC1. For convenience, these statistics will be referred to generically as “kappas.” Researchers who want a simple answer are faced with a bewildering set of options and claims about them. In @vach2023gwet we get a sense of the situation.\n\nGwet’s AC1 has been proposed as an alternative to Cohen’s kappa in evaluating the agreement between two binary ratings. This approach is becoming increasingly popular, and researchers have been criticized for still using Cohen’s kappa. However, a rigorous discussion of properties of Gwet’s AC1 is still missing.\n\nThe authors illustrate both the need for clarification and the faddishness that research communities can adopt when lacking real criteria. The kappas are ill-suited to answer such questions, and the proliferation of agreement statistics makes it difficult to compare results across studies or to know which one to use. Given pressures to publish results, there may be a tendency to use the statistic that gives the highest value, or to use the one that is most familiar. See @button2020inter for a discussion of rater agreement statistics in the context of the “crisis in confidence in psychological research.”\nIt’s not just difficult to know what kappa to use, there are no meaningful guides to interpreting the results. “There is a wide distinction in the elucidation of Kappa values, and several efforts have been made to assign practical meaning to calculated Kappa values,” wrote the authors of , who mention a widely-used heuristic found in that proposed a translation between numerical values of kappa and qualitative descriptions of agreement, such as \\(kappa \\ge .81\\) is “almost perfect.” The categories are arbitrary, do not translate well between different agreement statistics (which can give different values for the same data) and their assumptions, and do not provide insight into how to improve ratings.\nThe agreement statistics for ratings have problems that should now be evident. The goal is to understand a three-dimensional relationship between ratings and true values, but the statistics are single parameters. A second parameter, the worst case baseline, is buried in an assumption, which varies by kappa, and is not tested for fit to the data. The result is a bewildering array of choices for rater agreement measures. As a result of this confusion, different cultures have emerged. If getting published is the goal, then higher agreement rates are more desirable, so a researcher can shop around for the “best” one.\nA research agenda was suggested in that can be paraphrased as understanding (1) the true category of each subject, (2) the accuracy of raters, (3) truth and accuracy within subpopulations of subjects, (4) conditions that cause disagreement, and (5) what distinguishes “for cause” agreement and random agreement."
  },
  {
    "objectID": "introduction.html#machine-learning",
    "href": "introduction.html#machine-learning",
    "title": "Introduction",
    "section": "",
    "text": "The geyser of data produced in the information age has led to new methods of analyzing it that fall generally in the description of “machine learning (ML).” Assigning categories to subjects is a common task in machine learning, and the field has developed a number of methods to do so, including neural networks, support vector machines, and random forests. The goal is to assign categories to subjects in a way that generalizes to new subjects. The methods are often evaluated by comparing the predicted categories to the true categories, and the results are summarized in a confusion matrix.\nIt may come as a surprise to both behavioral scientists and machine learning researchers that they seem to have independently worked on the rater agreement problem, since the two fields’ literatures don’t seem to overlap. Articles on rater agreement don’t cite machine learning papers, and vice versa. However, the two fields are working on the same problem: how to measure the accuracy of a categorization when we don’t have the true categories to reference.\nThere are differences in the methods and philosophies of the two cultures. The kappa approach is implicitly backward-looking, asking “how accurate were these ratings,” with the assumption that the measured accuracy (a kappa) will carry forward to future instances. The ML approach is more forward-looking, with a suite of tools like cross-validation and a vocabulary (bias-variance trade-off) to measure generalizability. As we have seen, the kappa approach is to reason out a plausible chance-correction calculation and use it to produce the single-parameter kappa. The ML approach is to estimate three parameters with regression models. Finally, the kappas are grounded in the psychology of human classification and come with philosophical links to epistemology, which I described in Section 1.2. The ML algorithms are just statistics and code, barren of philosophical considerations.\nThe advantage of the ML approaches over the kappas is that the three parameter models avoid the confusions of the kappa zoo. But because they lack philosophical grounding, they run into an embarrassment of riches: three parameters is sometimes too many parameters for a model, so there can be multiple solutions. There are work-arounds, but it’s the same species of ad hoc reasoning that causes the problems of the kappa zoo. Fortunately, there’s a way to combine the best of both cultures to avoid most of these problems."
  },
  {
    "objectID": "introduction.html#sec-intro-tap",
    "href": "introduction.html#sec-intro-tap",
    "title": "Introduction",
    "section": "",
    "text": "We can now describe a regression model that allows us to estimate the four proportions that appear in the confusion matrix (see Section 1.3). Since the four cells sum to one, there are three free parameters to estimate.\n\n: Confusion matrix with entries to be filled in by the model estimates. The four numbers are proportions and sum to one, leaving three free parameters to estimate. C0 = class zero, and C1 = class one, standing in for any binary categories we might choose.\n\n\n\nTrue C1\nTrue C0\n\n\n\n\nClassified C1\n\n\n\n\nClassified C0\n\n\n\n\n\nTwo philosophical assumptions are needed to get started:\n\nThe classifications are real in the sense that true values exist.\nThe true values have a binary causal effect on the classification process.\n\nThese assumptions are provisional like hypothesis in statistics; the evidence will provide some support for the assumptions, ranging from no support at all to quite good. The reality of the truth values posed in assumption one is not like Plato’s cave shadows. In the case of wine judging, we can’t say there is a universal ideal for good wine, but if rater agreement is high we can say that some physical causal process exists for translating the observable subject (tasting the wine) into a category. So in that sense the category exists as part of the world. The translation is imperfect, because the conditions are not always perfect for the cause to happen, as with the Gettier problems7.\nThis may seem fiddly, but it gives us a place to start. For any given subject to be rated (e.g. object to be classified) we provisionally assume that there’s a latent truth value that we can never know, but might find evidence for. The second assumption then allows us to provisionally assume that the causal effect of the true value in the context of the classification process has the following nature:\n\nThe rater either assigns the correct class due to the causal pathway operating to connect the observation to the class (justified true knowledge), or\nSomething goes wrong with the causal pathway (the conditions weren’t quite right, etc.) and the classification is assigned non-causally, which is to say randomly.\n\nThe key to this is that the cause either works to connect the true value to the rater’s assigned value, or it fails completely. There’s no “partial cause.” When it fails, the rating is generated from a random process called a Bernoulli trial. It’s the simplest possible type of randomness, taking only two values with some fixed probability, like flipping a weighted coin.\nLet’s take inventory of the three parameters that come from the reasoning above.\n\nAmong the subjects being rated there is a fraction \\(t\\) that are in reality Class 1, with the remaining \\(1-t\\) being Class 0.\nAmong the ratings there is a fraction \\(a\\) that are accurate ratings (justified true knowledge) where the causal connection worked.\nFor the remaining \\(1-a\\) ratings, the causal connection failed, and there is some probability \\(p\\) that describes the frequency that they are assigned to Class 1. The remaining \\(1-p\\) are assigned Class 0.\n\nAll three of these parameters are proportions ranging from zero to one, and can be treated as probabilities that we estimate from appropriate regression models. Setting it up this way, instead of the usual machine learning parameterization avoids the most significant problem with non-identifiability (multiple solutions), which is class-switching. That happens when the model can fit the data, but is agnostic about which class is which, and so attempts to fit the model both ways at once.\nThe classification model with \\(t\\), \\(a\\), and \\(p\\) can be most easily understood in a tree diagram. For convenience, we’ll put a “bar” over a parameter name to indicate that it’s subtracted from one, e.g. \\(\\bar{t} = 1-t\\). This makes the notation more compact, and we always have \\(a + \\bar{a} = 1\\), etc.\n\n\n\ntap diagram\n\n\nThe diagram shows probabilistic links between states of the world and rater classifications. The lowercase letters are probabilities (recall that the bar means subtract from one to make the complementary probability, so \\(\\bar{.2} = .8\\)). We read the tree top to bottom, hence the name for the model: t-a-p, an abbreviation of the tree structure.\nTo understand the model, consider a judge (the rater) tasting one of the wines (the subject at the top of the tree). We’re at the top of the tree, where there exists a true classification for the wine as either acceptable (C1) or unacceptable (C0). In the universe of wines the fraction of acceptable ones is \\(t\\), and for a given wine we can imagine a hidden variable \\(T\\) that records its true class. Suppose this one is acceptable, so that \\(T = C_1\\). That’s the set-up for the wine taster, who doesn’t know the value of \\(T\\), but is trying to ascertain it. If this assessment is accurate (a case of justified true belief), then we follow the left branch of the diagram again, and the judge’s classification \\(C\\) is assigned the value \\(\\hat{C_1}\\). The hat indicates that this is an empirical rating, so we can keep it distinct from \\(C_1\\), which is the true value. The accurate judgments happen with probability \\(a\\), and if it’s not accurate, the judge’s classification will be randomly determined. The rating will be assigned \\(\\hat{C_1}\\) with probability \\(p\\), and otherwise will get the \\(\\hat{C_0}\\) rating.\nIt’s important to note that the value of \\(T\\) is the same for the same wine. So when four judges all rate the same wine, they are all either on the left side of the diagram (if the wine is acceptable in reality) or on the right side (if not). The ratings are determined only by \\(a\\) and \\(p\\) at that point.\nFor a wine chosen at random, we can compute the probabilities of rater classifications. An acceptable wine will be classified accurately with a proportion of \\(ta\\), multiplying the probabilities along the leftmost edge from top to bottom. An inaccurate rating of \\(\\hat{C_1}\\) (acceptability) can come from either the left or right side of the diagram, and if we add those together we get \\(t\\bar{a}p + \\bar{t}\\bar{a}p\\), and since \\(t + \\bar{t} = 1\\) that expression reduces to \\(\\bar{a}p\\).\nAssuming we can estimate the three parameters from the data, we can then populate the confusion matrix by tracing the diagram down to each of the four outcomes, and multiplying probabilities as we go.\n\nThe t-a-p model’s correspondence to the confusion matrix. Terms in parentheses are inaccurate ratings.\n\n\n\n\n\n\n\n\nTrue C1\nTrue C0\n\n\n\n\nClassified C1\n\\(ta + (t\\bar{a}p)\\)\n\\((\\bar{t}\\bar{a}p)\\)\n\n\nClassified C0\n\\((t\\bar{a}\\bar{p})\\)\n\\(\\bar{t}a + (\\bar{t}\\bar{a}\\bar{p})\\)\n\n\n\nThe table demonstrates that if the t-a-p model fits the data and we are able to estimate the three parameters, it is a general answer to the rater agreement question. The later sections show how S, Fleiss kappa, and other statistics are special cases of t-a-p models."
  },
  {
    "objectID": "introduction.html#example-wine-judging",
    "href": "introduction.html#example-wine-judging",
    "title": "Introduction",
    "section": "",
    "text": "The sections above used five rows of the wine judging data for illustration. We now use the whole data set of 183 wines each rated by four judges to estimate the t-a-p parameters. The included app was used (see Chapter 4) for the analysis.\nThe binary classification illustrated above was to convert the original scale (1 = no medal, 2 = bronze, 3 = silver, 4 = gold medal) to a binary outcome where 1 = any medal (bronze, silver, gold), and 0 = no medal. We called the Class 1 category “acceptable.” Using the app to analyze that binary comparison we get the following output.\n\n\n\nWine rating t-a-p analysis comparing C1 = ratings 1,2,3 (in-class) to C0 = rating 0.\n\n\nThe acceptable wines are the primary (in-) class, and the plot shows the distribution of ratings for acceptability. Since there are four judges, the possibilities are:\n\nNone of them could find the wine acceptable. This is the 0 at the left end of the plot. It happens for about 6% of the wines.\nAll four of them could find the wine acceptable. This is the 4 (four judges) at the right end of the plot. That happened for about 49% of the wines.\nSomething in between, where the judges disagree. In that case we can have from one to three ratings of “acceptable,” corresponding to those points on the plot.\n\nThe lollipops (black lines with dots on top) show the empirical data straight out of the ratings. The dashed blue line is the model fit, showing what the distribution would be if the model were exactly correct. Here, “model” means applying the estimated parameters to generate the distribution.\nThe estimated parameters are found at the top of the plot:\n\n\\(t\\) = .73 estimates that 73% of wines are acceptable in reality. This is more than the rate of unanimous agreement, which we saw above was only 49%.\n\\(a\\) = .54 estimates that the judges make accurate ratings (justified true belief) more than half the time. The rest of the ratings are assumed to be random.\n\\(p\\) = .78 estimates that when random ratings are made, judges choose “acceptable” 78% of the time. Notice that this is close to the estimated actual rate of 73% (the estimated value of \\(t\\) above). This turns out to be a desirable quality in judges. See Chapter 3 for details, especially the section on the Fleiss kappa.\n\nNote that the randomness of inaccurate ratings is not a conscious choice of raters. They aren’t giving up and flipping a coin. They still have a reasonable basis for making a rating, and may be quite confident about it. The assumption is that something Gettier-like has gone wrong with the causal process that links the input (wine) to output (rating), which flips the process from deterministic to stochastic. As with any regression model, the world doesn’t have to actually be that way; it’s just an assumption to allow us to create the model. Then we check to see if the data matches the model. In this case, the data matches the model pretty well, as we can visually assess by comparing the blue dashed line to the lollipops. More formal tests of agreement will have to wait until later.\nThe wine ratings comprise four categories that are naturally ordered from 1 = “no medal” to 4 = “gold medal.” That kind of data is common: from surveys with Strongly Disagree to Strongly Agree scales, from consumer ratings, and many other sources. We humans seem to have a natural facility to think of descriptions along an imaginary intensity scale. This data type is called “ordinal,” meaning we can put the categories in order.\nThe t-a-p model is designed to work with binary data. It’s possible to switch to a multinomial model (see the ML version in ???), but it’s common to analyze ordinal scales using binary distinctions. I’ll show how that works with the wine data. Along the 1,2,3,4 scale, there are three sequential cut-points where the commas are. The first one we can denote “1|2”, which splits the scores into two sets: an in-class {1} and and out-class {2,3,4}. This is the same thing as the Class 1 and Class 0 distinction from earlier. The second cut-point is at 2|3, splitting the data into ratings {1,2} and {3,4}. The plot shows the fraction of ratings in the in- and out-class for each cut-point.\n\n\n\nWine ratings divided into binary groups by cutpoint.\n\n\nThe 3|4 bar of the plot shows us that 88% of ratings are in {1,2,3}, so the remaining 12% are 4s (gold medal). As a validity check, there are 183 wines, each rated by four judges, so that should be 732 ratings. That looks like the height on the bars, so it checks out.\nFor a given cut-point, a wine rating will either be in the in-class or out-class This converts the scale to binary, and then we can estimate the model parameters. We repeat the t-a-p model for each cut-point to create a pattern like the one in the plot below."
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn earlier paper on this was published under the auspices of the NSA, and has a stamp declaring it unclassified. See https://www.nsa.gov/portals/75/documents/news-features/declassified-documents/tech-journals/application-of-cluster-analysis.pdf↩︎\nIt’s a mystery why humans can agree on mathematical justifications at such a high rate. If agreement indicates reality, then math must be real in some way.↩︎\nI found this on wikipedia here https://en.wikipedia.org/wiki/Gettier_problem↩︎\nwhich seems like an infinite regress of JTB inquiry.↩︎\nConfusion matrices are at the heart of measures of causality too, for reasons that are not coincidental. See Causal Interfaces.↩︎\nThe process of a group of people reaching agreement does not seem to be transitive! It may well be three times as difficult for three people to agree on a movie to watch as two people. The rater agreement models get around this by assuming that raters don’t talk to (argue with) each other, but reach conclusions independently before comparing notes.↩︎\nFor much more on this idea see “Causal Interfaces.”↩︎"
  },
  {
    "objectID": "tapmodel.html",
    "href": "tapmodel.html",
    "title": "Chapter 2: The t-a-p Model",
    "section": "",
    "text": "This chapter examines the assumptions and implications of the t-a-p model in detail to build a theoretical foundation for estimation and inference. This work also paves the way to connect the three parameter model to the kappas commonly in use as well as the machine learning (ML) algorithms used for rater agreement."
  },
  {
    "objectID": "tapmodel.html#conditional-probabilities",
    "href": "tapmodel.html#conditional-probabilities",
    "title": "Chapter 2: The t-a-p Model",
    "section": "1.1 Conditional Probabilities",
    "text": "1.1 Conditional Probabilities\nThe tree diagram in figure \\(\\ref{fig:tree}\\) models the assignments of \\(c_{ij}\\) and can be read by multiplying the conditional probabilities on the edges from the top down to find the probability that a given classification is \\(c_{ij} = \\widehat{C_1}\\). We use the convention throughout that the complement of probability \\(p\\) is represented by a hat, i.e. \\(\\bar{p} := 1-p\\) .\nIf a subject is not classified accurately, the classification for that rater is assumed to be made at random, with probability \\(p\\) of choosing \\(\\widehat{C_1}\\) regardless of the true class. So the conditional probability of a \\(\\widehat{C_1}\\) classification when the subject really is \\(C_1\\) is \\(\\text{Pr}(\\widehat{C_1} | C_1) = a + a'p\\). Similarly, \\(\\text{Pr}(\\widehat{C_1} | C_0) = a'p\\). This model is simplified in that it assumes that guess rates for the two classes are the same independent of the true classification. More complex models are introduced later."
  },
  {
    "objectID": "tapmodel.html#binomial-mixtures",
    "href": "tapmodel.html#binomial-mixtures",
    "title": "Chapter 2: The t-a-p Model",
    "section": "1.2 Binomial Mixtures",
    "text": "1.2 Binomial Mixtures\nThe accuracy rate \\(a\\) will affect the subject distributions. If \\(a = 0\\) the ratings will be distributed as \\(\\text{Bernoulli}(p)\\), independently of the subjects being rated. If \\(a=1\\), then all raters agree on the true value for each subject. Therefore the way we can reconstruct \\(a\\) from data is through the distribution of the within-subject ratings. The method used here can be seen as a latent class model with binomial mixture distributions. For a nice discussion of these ideas in practice see , which helpfully notes that binomial mixtures are statistically identifiable if the number of cases exceeds a low threshold . More generally, see chapter 14 on mixture models for discrete data.\nWe would like to know the true proportion \\(t\\) of the subjects belonging to \\(C_1\\) regardless of how they were rated, rater accuracy \\(a\\), and the proportion \\(p\\) of inaccurate assignments that are \\(\\widehat{C_1}\\). That goal describes the general model illustrated in the following section. A hierarchical version is given subsequently.\nAs is shown in Appendix A, rater accuracy \\(a\\) is proportional to the correlation between the true and assigned classifications. If \\(C\\) is the true classification and \\(T\\) is the assigned classification, then\n\\[\n\\text{Cor}(T, C) = a\\frac{\\sigma_T}{\\sigma_C}\n\\]\nThe t-app model focuses on raters more than subjects. For example, some rater models include a difficulty parameter for each subject, assuming that some are harder to classify than others. The t-a-p model does not include this, per se, but a random-effects version of the model is introduced later that allows for subject-specific coefficients \\(p_i\\), which adjusts the guessing rate per subject when a classification is made inaccurately. See @paun2018comparing for a discussion of difficulty parameters in machine-learning models similar to tap.\nnote: see https://www.jarad.me/courses/stat544/slides/Ch05/Ch05a.pdf\nand https://cran.r-project.org/web/packages/mixtools/vignettes/mixtools.pdf\nhttps://joss.theoj.org/papers/10.21105/joss.01505\nI should address the label-switching problem. I think that’s taken care of by way the classifications are pre-specified, AND accuracy is generic."
  },
  {
    "objectID": "tapmodel.html#overdispersion",
    "href": "tapmodel.html#overdispersion",
    "title": "Chapter 2: The t-a-p Model",
    "section": "1.3 Overdispersion",
    "text": "1.3 Overdispersion\nThe tap model assumes fixed averages over raters and subjects for the three parameters. The most sensible of these assumptions is that there is a single value for \\(t\\) that represents the fraction of Class 1 cases, although this idea has been challenged by @???. That leaves two parameters that are certainly oversimplified in the tap model, so that counts of Class 1 ratings per subject are likely to have more variance than a binomial model would. This is due to anticipated variance in rater ability and the difficulty in rating subjects, resulting in overdispersion. A general approach to this problem is to allow each rater to have a different accuracy rate \\(a_j\\) and each subject to have a different guessing rate \\(p_i\\). This is the hierarchical model, which is described later.\nOriginal suggestion to use beta-binomial @williams1975394, modern look at it in @ascari2021new."
  },
  {
    "objectID": "tapmodel.html#fitting-the-model",
    "href": "tapmodel.html#fitting-the-model",
    "title": "Chapter 2: The t-a-p Model",
    "section": "1.4 Fitting the Model",
    "text": "1.4 Fitting the Model\nThe first question about the model illustrated in figure \\(\\ref{fig:tree}\\) is whether it is computationally useful. Using known parameter values for \\(p, a, t\\) to generate simulated ratings, can we then recover the parameters from the data? The answer is yes, with some provisos. Given a data set \\(c_{ij}\\), we can fit a general model to estimate the three parameters \\(t\\), \\(a\\), and \\(p\\) using maximum likelihood to fit a binomial mixture model. The log likelihood function for the binomial mixture described by figure \\(\\ref{fig:tree}\\) with \\(N\\) subjects and \\(R&gt;1\\) raters is\n\\[\n\\ell(t,a,p;k_1, \\dots,k_N) = \\sum_{i = 1}^N \\log \\left( t\\binom{R}{k_i}(a + a'p)^{k_i}(a'p')^{R-k_i} + t'\\binom{R}{k_i}(a'p)^{k_i}(1-a'p)^{R-k_i} \\right)  \n\\]\nwhere \\(k_i=\\sum_{j}C_{ij}\\) the number of Class 1 ratings for subject \\(i\\). The sum over the logs is justified by the assumption that ratings are independent. It is straightforward to implement the function in the Bayesian programming language Stan , using uniform \\((0,1)\\) priors for the three parameters (see the Discussion section to access the source code).\nTo test the computational feasibility of this method, ratings were simulated using a range of values of \\(t\\), \\(a\\), and \\(p\\). The 729 trials each simulated 300 subjects with five raters each, using all combinations of values ranging from .1 to .9 in increments of .1 for each of \\(t\\), \\(a\\), and \\(p\\). The Stan engine uses a Markov chain Monte Carlo (MCMC) algorithm to gather representative samples from the joint probability density of the three parameters. Each run used 1,000 iterations (800 after warm-up) with four chains each.\n[need to cite @gelman2020bayesian and use those methods]\n\n\nCode\nregenerate = FALSE\n\nif(regenerate == TRUE){\n  param_grid &lt;- expand.grid(t = seq(.1,.9,.1),a = seq(.1,.9,.1),p = seq(.1,.9,.1))\n  \n  n_sims &lt;- nrow(param_grid)\n  N &lt;- 300 # number of subjects\n  R &lt;- 5   # number of raters\n  \n  out &lt;- data.frame()  \n  fixed_model_spec &lt;- stan_model(\"code/fixed_effects.stan\")\n  \n  for(i in 1:n_sims){\n    ratings &lt;- generate_ratings(N, R, param_grid$p[i], param_grid$a[i], param_grid$t[i])  \n    \n    # get the number of Class 1 ratings per subject in a vector\n    counts &lt;- ratings %&gt;% \n      group_by(SubjectID) %&gt;% \n      summarize(k = sum(RatedClass)) %&gt;% \n      select(k) %&gt;% \n      pull()\n    \n    kappa &lt;- sqrt(fleiss(counts,R))\n      \n    fixed_model &lt;- sampling(object = fixed_model_spec, \n                              data = list(N = N, R = R, S = 1, count = counts), \n                              iter = 1000,\n                              warmup = 200,\n                              thin = 1)\n      \n      stats &lt;- rbind( broom::tidy(fixed_model),\n                      data.frame(term = \"root kappa\",estimate = kappa, std.error = NA,\n                                 stringsAsFactors = FALSE)) %&gt;% \n                      mutate(p = param_grid$p[i], a = param_grid$a[i], t = param_grid$t[i])\n    \n      out &lt;- rbind(out,stats)\n    }\n    write_csv(out,\"data/fit_test_sim.csv\")\n} else {\n  out &lt;- read_csv(\"data/fit_test_sim.csv\")\n}\n\n# format for plotting\npdf &lt;- out %&gt;% \n#  filter(a &gt; .2) %&gt;% \n  select(parameter = term, value = estimate, actual_p = p, actual_a = a, actual_t = t) %&gt;% \n  spread(parameter, value) %&gt;% \n  rename(estimated_a = accuracy, estimated_p = p, kappa_a = `root kappa`, estimated_t = t) %&gt;% \n  mutate(Sim = row_number()) %&gt;% \n  gather(parameter, value, -Sim) %&gt;% \n  separate(parameter, into = c(\"type\",\"parameter\"), sep = \"_\") %&gt;% \n  spread(type, value)\n\n  ggplot(pdf, aes(x = actual, y = estimated, group = actual)) +\n    geom_boxplot() +\n    geom_abline(slope = 1, intercept = 0) +\n    facet_grid(. ~ parameter) +\n    theme_bw()\n\n\n\n\n\nBox and whisker plots show parameter estimates from simulations of rater data \\(t\\)-\\(a\\)-\\(p\\) values ranging from .1 to .9. The diagonal line marks perfect estimates.\n\n\n\n\nThe accuracy measure \\(a\\) and the Class 1 “guess rate” \\(p\\) are stable across scenarios in figure \\(\\ref{fig:tap_sim}\\), but the estimated true fraction of Class 1 cases \\(t\\) is sensitive to values of \\(a\\) near zero. To see this, substitute \\(a = 0\\) into the likelihood function to get\n\\[\n\\begin{aligned}\n\\ell(t,p;a = 0, k_1, \\dots,k_N) &= \\sum_{i = 1}^N \\log \\left( t\\binom{R}{k_i}p^{k_i}(p')^{R-k_i} + t'\\binom{R}{k_i}p^{k_i}(p')^{R-k_i} \\right) \\\\\n&= \\sum_{i = 1}^N \\log \\left( \\binom{R}{k_i}p^{k_i}(1-p)^{R-k_i}  \\right)\n\\end{aligned}\n\\]\nshowing that \\(t\\) is under-determined when \\(a = 0\\), and we should expect poor behavior as \\(a\\) nears zero. This is intuitive: if the raters are only guessing, they should give us no information about the true \\(C_1\\) rate. If the data in figure \\(\\ref{fig:tap_sim}\\) are filtered to \\(a &gt; .2\\) the estimates of \\(t\\) greatly improve. Aside from extreme values of \\(a\\) affecting the estimation of \\(t\\), a visual inspection of the scatter plots of the parameter estimates shows no correlations."
  },
  {
    "objectID": "tap.html",
    "href": "tap.html",
    "title": "tap",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "tap.html#quarto",
    "href": "tap.html#quarto",
    "title": "tap",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\nAgresti, A. (2003). Categorical data analysis (Vol. 482). John Wiley & Sons.\n\n\nAickin, M. (1990). Maximum likelihood estimation of agreement in the constant predictive probability model, and its relation to cohen’s kappa. Biometrics, 293–302.\n\n\nAscari, R., & Migliorati, S. (2021). A new regression model for overdispersed binomial data accounting for outliers and an excess of zeros. Statistics in Medicine, 40(17), 3895–3914.\n\n\nBassett, R., & Deride, J. (2019). Maximum a posteriori estimators as a limit of bayes estimators. Mathematical Programming, 174, 129–144.\n\n\nBennett, E. M., Alpert, R., & Goldstein, A. (1954). Communications through limited-response questioning. Public Opinion Quarterly, 18(3), 303–308.\n\n\nBonett, D. G. (2022). Statistical inference for g-indices of agreement. Journal of Educational and Behavioral Statistics, 47(4), 438–458.\n\n\nBrennan, R. L., Measurement in Education, N. C. on, et al. (2006). Educational measurement. Praeger Publishers,.\n\n\nButton, C. M., Snook, B., & Grant, M. J. (2020). Inter-rater agreement, data reliability, and the crisis of confidence in psychological research. Quant Methods Psychol, 16(5), 467–471.\n\n\nByrt, T., Bishop, J., & Carlin, J. B. (1993). Bias, prevalence and kappa. Journal of Clinical Epidemiology, 46(5), 423–429.\n\n\nCarpenter, B. (2008). Multilevel bayesian models of categorical data annotation. Unpublished Manuscript, 17(122), 45–50.\n\n\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., & Riddell, A. (2017). Stan: A probabilistic programming language. Journal of Statistical Software, 76(1).\n\n\nChaturvedi, S., & Shweta, R. (2015). Evaluation of inter-rater agreement and inter-rater reliability for observational data: An overview of concepts and methods. Journal of the Indian Academy of Applied Psychology, 41(3), 20–27.\n\n\nCicchetti, D. V., & Feinstein, A. R. (1990). High agreement but low kappa: II. Resolving the paradoxes. Journal of Clinical Epidemiology, 43(6), 551–558.\n\n\nCohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37–46.\n\n\nCronbach, L. J., Rajaratnam, N., & Gleser, G. C. (1963). Theory of generalizability: A liberalization of reliability theory. British Journal of Statistical Psychology, 16(2), 137–163.\n\n\nDavani, A. M., Dı́az, M., & Prabhakaran, V. (2022). Dealing with disagreements: Looking beyond the majority vote in subjective annotations. Transactions of the Association for Computational Linguistics, 10, 92–110.\n\n\nDawid, A. P., & Skene, A. M. (1979). Maximum likelihood estimation of observer error-rates using the EM algorithm. Journal of the Royal Statistical Society: Series C (Applied Statistics), 28(1), 20–28.\n\n\nDelgado, R., & Tibau, X.-A. (2019). Why cohen’s kappa should be avoided as performance measure in classification. PloS One, 14(9), e0222916.\n\n\nEngelhard, G. (2012). Examining rating quality in writing assessment: Rater agreement, error, and accuracy. Journal of Applied Measurement, 13, 321–335.\n\n\nEngelhard Jr, G. (1996). Evaluating rater accuracy in performance assessments. Journal of Educational Measurement, 33(1), 56–70.\n\n\nEubanks, D. (2017). (Re)visualizing rater agreement:beyond single-parameter measures. Journal of Writing Analytics, 1.\n\n\nEubanks, D. A. (2014). Causal interfaces. Arxiv.org Preprint. http://arxiv.org/abs/1404.4884v1\n\n\nFleiss, J. L. (1971). Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5), 378.\n\n\nFleiss, J. L., Levin, B., & Paik, M. C. (2013). Statistical methods for rates and proportions. john wiley & sons.\n\n\nGelman, A., & Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge university press.\n\n\nGelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., Kennedy, L., Gabry, J., Bürkner, P.-C., & Modrák, M. (2020). Bayesian workflow. arXiv Preprint arXiv:2011.01808.\n\n\nGettier, E. L. (1963). Is justified true belief knowledge? Analysis, 23(6), 121–123.\n\n\nGrilli, L., Rampichini, C., & Varriale, R. (2015). Binomial mixture modeling of university credits. Communications in Statistics - Theory and Methods, 44(22), 4866–4879. https://doi.org/10.1080/03610926.2013.804565\n\n\nGwet, K. L. (2008). Computing inter-rater reliability and its variance in the presence of high agreement. British Journal of Mathematical and Statistical Psychology, 61(1), 29–48.\n\n\nHodgson, R. T. (2008). An examination of judge reliability at a major US wine competition. Journal of Wine Economics, 3(2), 105–113.\n\n\nHolley, J. W., & Guilford, J. P. (1964). A note on the g index of agreement. Educational and Psychological Measurement, 24(4), 749–753.\n\n\nHovy, D., Berg-Kirkpatrick, T., Vaswani, A., & Hovy, E. (2013). Learning whom to trust with MACE. Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1120–1130.\n\n\nKrippendorff, K. (2013). Commentary: A dissenting view on so-called paradoxes of reliability coefficients. Annals of the International Communication Association, 36(1), 481–499.\n\n\nKrippendorff, K. (2018). Content analysis: An introduction to its methodology. Sage publications.\n\n\nKrippendorff, K., & Fleiss, J. L. (1978). Reliability of binary attribute data. Biometrics, 34(1), 142–144.\n\n\nKumar, S., Hooi, B., Makhija, D., Kumar, M., Faloutsos, C., & Subrahmanian, V. (2018). Rev2: Fraudulent user prediction in rating platforms. Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, 333–341.\n\n\nLandis, J. R., & Koch, G. G. (1977). The measurement of observer agreement for categorical data. Biometrics, 159–174.\n\n\nMcLachlan, G., & Peel, D. (2000). Wiley series in probability and statistics. Finite Mixture Models, 420–427.\n\n\nPaun, S., Carpenter, B., Chamberlain, J., Hovy, D., Kruschwitz, U., & Poesio, M. (2018). Comparing bayesian models of annotation. Transactions of the Association for Computational Linguistics, 6, 571–585.\n\n\nRasch, G. (1977). On specific objectivity. An attempt at formalizing the request for generality and validity of scientific statements in symposium on scientific objectivity, vedbaek, mau 14-16, 1976. Danish Year-Book of Philosophy Kobenhavn, 14, 58–94.\n\n\nRasch, G. (1993). Probabilistic models for some intelligence and attainment tests. MESA Press.\n\n\nRoss, V., & LeGrand, R. (2017). Assessing writing constructs: Toward an expanded view of inter-reader reliability. Journal of Writing Analytics, 1.\n\n\nScott, W. A. (1955). Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly, 321–325.\n\n\nShabankhani, B., Charati, J. Y., Shabankhani, K., & Cherati, S. K. (2020). Survey of agreement between raters for nominal data using krippendorff’s alpha. Arch Pharma Pract, 10(S1), 160–164.\n\n\nShrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing rater reliability. Psychological Bulletin, 86(2), 420.\n\n\nTeam, S. D. (2022). Stan user’s guide 2.34. Stan Development Team.\n\n\nVach, W., & Gerke, O. (2023). Gwet’s AC1 is not a substitute for cohen’s kappa–a comparison of basic properties. MethodsX, 102212.\n\n\nWilliams, D. (1975). 394: The analysis of binary responses from toxicological experiments involving reproduction and teratogenicity. Biometrics, 949–952."
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Appendix A: Statistical Details",
    "section": "",
    "text": "Given two distinct raters \\(i\\) and \\(j\\) with common accuracy \\(a\\) and guess probability \\(p\\), what’s the correlation between their ratings? Let \\(c = E[C_i] = E[C_j] = ta + p\\bar{a}\\).\n$$ \\[\\begin{align}\n\\text{Cor}(C_i, C_j) &= \\frac{\\text{Cov}(C_i, C_j)}{\\sqrt{\\text{Var}(C_i)\\text{Var}(C_j)}} \\\\\n\n&= \\frac{E[(TA_i + \\bar{A_i}P_i)(TA_j + \\bar{A_j}P_j)] - c^2}{\\text{Var}(C)} \\\\\n&= \\frac{ta^2 + 2ta\\bar{a}p + \\bar{a}^2p^2  - (ta + p\\bar{a})^2}{(ta + p\\bar{a})\\overline{(ta + p\\bar{a})  }} & (\\text{since }T^2 = T)\\\\\n&= \\frac{ta^2 + 2ta\\bar{a}p - t^2a^2 - 2tap\\bar{a}}{(ta + p\\bar{a})\\overline{(ta + p\\bar{a})  }} \\\\\n&= \\frac{a^2t\\bar{t}}{c\\bar{c }} \\\\\n\\end{align}\\]\n$$\nRater accuracy can be obtained via\n\\[\na^2 =\\frac{c\\overline{c }}{t\\bar{t}} \\text{Cor}(C_a, C_b) = \\frac{c\\overline{c }}{t\\bar{t}}\\kappa_{fleiss}\n\\tag{1}\\] The correlation between two raters’ ratings of the same subject is the intraclass correlation coefficient (ICC) for a two-way random effects model , which has been shown to be equivalent to the Fleiss kappa as described in . Under the \\(t = p\\) proficient rater assumption, \\(c = ta + \\bar{a}p = p\\), so that the Fliess kappa is (again) shown to be \\(a^2\\) under that condition. The relation Equation 1 suggests that the Fliess kappa could be adjusted for cases when \\(t \\ne p\\). The overall rate of Class 1 ratings \\(c\\) can be estimated directly from the data, but estimating \\(t\\) requires either prior knowledge of the context or using the full tap estimation process, in which case there’s no need to compute the Fliess kappa.\n\n\nIt is of interest to find the correlation between \\(T_i\\) the truth value of subject \\(i\\) and the resulting classification \\(C_i\\). Note that both of the random variables \\(T_i\\) and \\(C_i\\) take only values of zero or one, so squaring them doesn’t change their values. This fact simplifies computations, for example \\(E[C_i^2] = E[C_i] = ta + p\\bar{a}\\). The variance of \\(C\\) is therefore\n$$ \\[\\begin{align}\nVar(C) &= E[C^2] - E^2[C] \\\\\n       &= c - c^2 \\\\\n       &= c\\bar{c} \\\\\n       &= (ta + p\\bar{a})\\overline{(ta + p\\bar{a})}. \\\\\n\n\\end{align}\\] $$\nSimilarly, \\(Var(T) = t\\bar{t}\\). The correlation between true values and ratings is then\n$$ \\[\\begin{align}\n\\text{Cor}(T, C) &= \\frac{\\text{Cov}(T, C)}{\\sqrt{\\text{Var}(T)\\text{Var}(C)}} \\\\\n\n&= \\frac{E[T(Ta + p\\bar{a}) ] - t(ta + p\\bar{a})}{\\sqrt{t\\bar{t} c\\bar{c}}  } \\\\\n&= \\frac{t(a + p\\bar{a})  - t(ta + p\\bar{a})}{\\sqrt{t\\bar{t} c\\bar{c}}  } \\\\\n&= a\\frac{\\sqrt{t\\bar{t}}}{\\sqrt{c\\bar{c}}} \\\\\n&= a\\frac{\\sigma_T}{\\sigma_C}.\n\\end{align}\\]\n$$ {#eq-cor-tc}\nWhere \\(\\sigma\\) is the standard deviation (square root of variance). The relationship in ?@eq-cor-tc can also be seen as \\(a = \\text{Cor}(T, C) \\frac{\\sigma_C}{\\sigma_T}\\), which means \\(a\\) can be interpreted as the slope of the regression line \\(C = \\beta_0 + \\beta_1T + \\varepsilon\\), i.e. \\(a = \\beta_1\\). In the proficient rater case \\(p = t\\), \\(\\sigma_C = \\sigma_T\\) and so \\(\\text{Cor}(T, C) = a\\). It can also be shown that for a \\(t\\)-\\(a_1,a_0\\)-\\(p\\) model, the \\(t=p\\) assumption leads to \\(a = \\sqrt{a_1a_0}.\\) See .\nThe two correlations derived here are related by \\(\\text{Cor}^2(T, C) = \\text{Cor}(C_i, C_j)\\)."
  },
  {
    "objectID": "correlation.html#correlation-between-ratings-and-true-values",
    "href": "correlation.html#correlation-between-ratings-and-true-values",
    "title": "Appendix A: Statistical Details",
    "section": "",
    "text": "It is of interest to find the correlation between \\(T_i\\) the truth value of subject \\(i\\) and the resulting classification \\(C_i\\). Note that both of the random variables \\(T_i\\) and \\(C_i\\) take only values of zero or one, so squaring them doesn’t change their values. This fact simplifies computations, for example \\(E[C_i^2] = E[C_i] = ta + p\\bar{a}\\). The variance of \\(C\\) is therefore\n$$ \\[\\begin{align}\nVar(C) &= E[C^2] - E^2[C] \\\\\n       &= c - c^2 \\\\\n       &= c\\bar{c} \\\\\n       &= (ta + p\\bar{a})\\overline{(ta + p\\bar{a})}. \\\\\n\n\\end{align}\\] $$\nSimilarly, \\(Var(T) = t\\bar{t}\\). The correlation between true values and ratings is then\n$$ \\[\\begin{align}\n\\text{Cor}(T, C) &= \\frac{\\text{Cov}(T, C)}{\\sqrt{\\text{Var}(T)\\text{Var}(C)}} \\\\\n\n&= \\frac{E[T(Ta + p\\bar{a}) ] - t(ta + p\\bar{a})}{\\sqrt{t\\bar{t} c\\bar{c}}  } \\\\\n&= \\frac{t(a + p\\bar{a})  - t(ta + p\\bar{a})}{\\sqrt{t\\bar{t} c\\bar{c}}  } \\\\\n&= a\\frac{\\sqrt{t\\bar{t}}}{\\sqrt{c\\bar{c}}} \\\\\n&= a\\frac{\\sigma_T}{\\sigma_C}.\n\\end{align}\\]\n$$ {#eq-cor-tc}\nWhere \\(\\sigma\\) is the standard deviation (square root of variance). The relationship in ?@eq-cor-tc can also be seen as \\(a = \\text{Cor}(T, C) \\frac{\\sigma_C}{\\sigma_T}\\), which means \\(a\\) can be interpreted as the slope of the regression line \\(C = \\beta_0 + \\beta_1T + \\varepsilon\\), i.e. \\(a = \\beta_1\\). In the proficient rater case \\(p = t\\), \\(\\sigma_C = \\sigma_T\\) and so \\(\\text{Cor}(T, C) = a\\). It can also be shown that for a \\(t\\)-\\(a_1,a_0\\)-\\(p\\) model, the \\(t=p\\) assumption leads to \\(a = \\sqrt{a_1a_0}.\\) See .\nThe two correlations derived here are related by \\(\\text{Cor}^2(T, C) = \\text{Cor}(C_i, C_j)\\)."
  },
  {
    "objectID": "causality.html",
    "href": "causality.html",
    "title": "Cauality",
    "section": "",
    "text": "As a bonus, the confusion matrix can be split into the sum of two matrices to separate the causal part of the classification process, where true classes are identified accurately, from the random classifications, which will sometimes reach the right class assignments, but for the wrong reasons (without justification).\n\\[ a\\begin{bmatrix} t & 0 \\\\ 0 & \\bar{t} \\end{bmatrix} +  \\bar{a} \\begin{bmatrix} tp & \\bar{t}p \\\\ t\\bar{p} & \\bar{t}\\bar{p} \\end{bmatrix} \\]\nin the usual confusion matrix, we know that the off-diagonal cells are inaccurate, but we don’t know what fraction of the diagonal elements are. If the \\(t-a-p\\) model fits the data, we can estimate\nExperiments assume that each experimental unit is identical, and we’re causing an effect rather than classifying a case. However, if we think of the treatment as a classification algorithm, then each subject is unique and we don’t have the opportunity to treat multiple times, so no within-subject comparisons. If a treatment can be repeated, then we can test this way, and get at t and a.\nSection on link to causality.\nsensivity = TPR, specificity = TNR"
  },
  {
    "objectID": "hierarchical.html",
    "href": "hierarchical.html",
    "title": "Chapter 4: Hierarchical Models",
    "section": "",
    "text": "We’ve seen how adding parameters leads to more explanatory models. This idea can be expanded, for example to simultaneously estimate the accuracy of each rater and classification of subject individually. Such models are sometimes called hierarchical or random effects models or fixed effects models, depending on the research tradition. As noted in , fixed versus random effects are not good descriptions for Bayesian models. This is elaborated on in . What all these have in common is that they allow for complex relationships of average and individual parameters, for example having a grand average accuracy parameter \\(a\\) in addition to individual accuracy parameters \\(a_j\\) for each rater \\(j\\). In a Bayesian context, we can do “partial pooling” by allowing the scale (standard deviation) of the \\(a_j\\) parameters to be estimated from the data, rather than fixed at a constant value. This allows the model to estimate the degree to which the raters are similar to each other, and to the grand average, which can increase likelihood.\nIndividual rater parameters and subject truth values are of interest in many contexts. In educational psychology, we might want to know how well a teacher is able to assess student learning, and how well the students are learning. In medical research, we might want to know how well a doctor is able to diagnose a disease, and how well the patients are responding to treatment. In machine learning, we are likely concerned with the quality of training data drawn from human classifications. The ratings of consumer products on sites like Amazon or Yelp can be polluted with bad reviews, so some means of cleaning that data is useful.\nThere is an epistemological complexity in indexed \\(t_*\\)-\\(a_*\\)-\\(p_*\\) models that has practical implications. The three-parameter \\(t\\)-\\(a\\)-\\(p\\) model assumes that each subject \\(i\\) being observed has a true classification of Class 1 or Class 0 as indicated by the random variable \\(T_i\\). This is different from a model of the probability that a case is in Class 1. In the analysis so far, we’ve only dealt with expectations over a set of observations, so the distinction wasn’t apparent. But if we create individual parameters \\(a_j\\) for each rater, estimating those depends on true classifications of subjects existing, even if we cannot know what they are. Not all computational methods for estimating the parameters are not well-suited to enforce the discrete constraint that \\(t_i \\epsilon \\{0,1\\}\\) for each subject \\(i\\). One work-around is to average out (marginalize) truth values over the ratings for a subject. That idea is outlined in @stanuser, section 1.10 and implemented in several algorithms found in @carpenter2008multilevel. Another approach is to use a threshold function to map a continuous latent variable to a discrete truth value. Recall that a “latent” variable is allowed to have real values, usually with a normal distribution centered at zero, but is not directly observed. We can convert from the latent scale to an approximation of a discerte one by applying a threshold function like the logistic function, \\(\\tau(s) = \\frac{1}{1+e^{-cs}}\\), which has the property that \\(\\tau(s) \\rightarrow 0\\) as \\(s \\rightarrow -\\infty\\) and \\(\\tau(s) \\rightarrow 1\\) as \\(s \\rightarrow \\infty\\). The steepness of the thresholding can be adjusted via the parameter \\(c\\). This allows the truth parameters to remain real values, and the transformaion differentiable, but approximates a discrete truth value. In simulations, this worked to improve the estimates of rater accuracy parameters.\nThere are other reasons to use latent variables, which is one motivation for using item response theory or Rasche models. It’s straightforward to include predictor variables in the model, for example to estimate the effect of a rater’s experience on their accuracy. This is a common approach in educational psychology, where the goal is to estimate the effect of a teacher’s experience on their ability to assess student learning. Another option is to include a “difficulty” parameter for each subject, to allow for some subjects to be inherently more difficult to classify than others.\nOne consideration for a \\(t_*\\)-\\(a_*\\)-\\(p_*\\) model is that we must decide what model relationship there is between the \\(a_*\\) and \\(p_*\\) parameters. The three main options are (1) leave \\(p\\) a global average and don’t estimate indexed parameters for it, (2) assign \\(p_j\\) for each rater, or (3) assign \\(p_i\\) for each subject. The second option is a Cohen kappa approach. Generally this option will have identifiability problems for unbalanced data. To see that, consider \\(c_{ij} = T_i a_j + \\bar{a_j}p_j\\) when \\(T_i = 0\\). If this is the large majority of cases, then the same rating will be obtained from either low accuracy or high guess rates, and the model will not be able to distinguish between them (the kappa paradox situation). This is not the case for the third option, where the \\(p_i\\) parameters are assigned to subjects, not raters. In simulations across a range of conditions, the third option performed best.\n\n\nOne way to retreat slightly from discrete truth values is to borrow an idea from the latent scale approach used in log-odds methods, and parameterize \\(t_i\\) as \\(t_i = \\tau(s_i)\\), where \\(s_i \\epsilon \\mathbb{R}\\) is a latent variable representing characteristics of the subject being observed and categorized, and \\(\\tau\\) is a thresholding function that very nearly maps \\(x\\) to \\(\\{0,1\\}\\) in a continous manner. A reasonable choice for \\(\\tau\\) is a sigmoid like the logistic function, \\(\\tau(s) = \\frac{1}{1+e^{-s}}\\), which has the property that \\(\\tau(s) \\rightarrow 0\\) as \\(s \\rightarrow -\\infty\\) and \\(\\tau(s) \\rightarrow 1\\) as \\(s \\rightarrow \\infty\\). One advantage of this approach is that we can build full regression models for the parameters, e.g. to incorporate subject-specific or rater-specific information as explanatory variables. In simulations, this approach can work, but the sigmoid operation significantly slows down convergence of the MCMC sampler.\nA second approach, which deserves more analysis, is to penalize the likelihood function with a term proportional to \\(\\sum{t_i(1-t_i)}\\). This is a crude way to encourage the sampler to find values of \\(t_i\\) that are close to 0 or 1, but not exactly 0 or 1. This also slows down computation significantly.\nThe third, most practical solution for simple indexing, where explanatory variables are not needed, is to first estimate the three-parameter \\(t\\)-\\(a\\)-\\(p\\) model and then use those estimates to recenter the indexed parameters of interest. In simulations, this worked very will in a fully-indexed \\(t_i\\)-\\(a_j\\)-\\(p_j\\) model, where \\(i\\) indexes subjects being classified and \\(j\\) indexes raters. But it is not clear that it will work in all cases.\nThe likelihood function for \\(t_i\\)-\\(a_j\\)-\\(p_j\\) can be expressed as the product of independent ratings, as\n\\[\n\\begin{aligned}\n\\text{L}(a_j,t_i,p_j;c_{ij}) & = \\prod_{ij} \\left[ c_{ij}(\\tau(s_i)a_j+\\bar{a}_jp_j) + \\bar{c_{ij}}(1 -\\tau(s_i) a_j -\\bar{a}_jp_j) \\right]\\\\\n\\end{aligned}\n\\]\nwhere \\(t_i \\epsilon(0,1)\\) is the estimated probability that the \\(i\\)th case is Class 1, \\(a_j\\) is the estimated accuracy of the \\(j\\)th rater, and constant \\(c\\) is the known fraction of \\(\\overline{C}\\) ratings in the whole data set. The prior distributions of the parameters \\((\\t_i = \\tau(s_i),a_j,p_j)\\) are assumed to be uniform on \\((0,1)\\).\nTo interpret induced parameters in the context of the model in figure \\(\\ref{fig:tree}\\), a slight modification to the likelihood function is useful. The generative model in figure \\(\\ref{fig:tree}\\) is conditional on the true class being binary, not a probability between zero and one. This departure from the model is necessary computationally, so that the truth probability of each subject can smoothly evolve to maximize likelihood, but in practice, we’d like the values of \\(t_i\\) to be close to zero or one when computing likelihoods. This is accomplished using a soft threshold on \\(t_i\\) with a sigmoid \\(\\text{sig}_d(t) := 1/(1 + e^{-d(t-.5)})\\), where \\(d\\) is a discrimination parameter that adjusts steepness. The final likelihood function is\n\\[\n\\text{L}(a_j,t_i;c_{ij},P) = (-1)^{c_{ij}'}|a_j(\\text{sig}_d(t_i) -P)| + c_{ij}P + c_{ij}'P'. \\tag{3} \\label{eq:re_model}\n\\]\nInference of the \\(N + R\\) parameters maximizes likelihood using \\(\\eqref{eq:re_model}\\) assuming that each rating is independent of the others so that total probability is the product of the individual rating probabilities.\nTo test the model \\(N=1000\\) subjects with \\(R=5\\) raters each were simulated with a Class 1 rate of \\(p=.20\\), and with rater accuracies of .1, .3, .5, .7, and .9. The sigmoid discrimination parameter was set to \\(d=30\\). The model was specified in the Stan programming language, with four chains of 1000 iterations each, the first 200 being discarded as warm-up samples to control auto-correlation.\n\n\nCode\n# see code/plain model detail sim.R for details\n\nsim_summary_a &lt;- read_rds(\"data/sim_summary.rds\")\n\nsim_summary_a |&gt; \n  ggplot(aes(x = as.factor(n_raters), y = mean_error,\n             ymin = mean_error - SE*2, \n             ymax = mean_error + SE*2,\n             group = `Number of subjects`,\n             linetype = `Number of subjects`)) +\n  geom_point(size = 2, position = position_dodge(.3)) +\n  geom_errorbar(position = position_dodge(.3),\n                width = .1) +\n  theme_bw() +\n  xlab(\"Number of Raters\") +\n  ylab(\"Error in Accuracy\")\n\n\n\n\n\n\n\n\nFigure 1: Estimates of individual rater accuracy with uniformly distributed t-a-p parameters indexed by rater and subject, with bars encompassing two standard deviations from the mean. Each data point represents 200 simulations, using the three parameter t-a-p model to adjust individual accuracy estimates.\n\n\n\n\n\n\n\nCode\n# see code/plain model detail sim.R for details\n\nsim_summary_t &lt;- read_rds(\"data/sim_summary_t.rds\")\n\nsim_summary_t |&gt; \n  ggplot(aes(x = as.factor(n_raters), y = mean_error,\n             ymin = mean_error - SE*2, \n             ymax = mean_error + SE*2,\n             group = `Number of subjects`,\n             linetype = `Number of subjects`)) +\n  geom_point(size = 2, position = position_dodge(.3)) +\n  geom_errorbar(position = position_dodge(.3),\n                width = .3) +\n  theme_bw() +\n  xlab(\"Number of Raters\") +\n  ylab(\"Error in Classification\")\n\n\n\n\n\n\n\n\nFigure 2: Estimates of individual subject classification accuracy with uniformly distributed t-a-p parameters indexed by rater and subject, with bars encompassing two standard deviations from the mean. Each data point represents 200 simulations, using the three parameter t-a-p model to adjust individual accuracy estimates.\n\n\n\n\n\nFor a simulated data set of 5000 ratings analyzed in figure \\(\\ref{fig:sim_truth}\\), the cases are separated quite well even with only five ratings each. The estimated class probabilities \\(t_i\\) on the horizontal axis take into account estimated rater accuracy, which is imputed for each rater simultaneously. The distributions of posterior samples from the Stan output shows that the parameter estimates recapture individual rater accuracy from the simulated ratings. This does not happen without the soft threshold function to nudge the \\(t_i\\)s toward zero or one. For smaller data sets, the estimates become noisier. The estimates in figure \\(\\ref{fig:sim_truth2}\\) have only 100 subjects with three raters each, with randomly chosen accuracies. It shows much less certainty about the estimates.\nIf the model fits the data, the examples show it is possible to recover latent true class probabilities and individual rater accuracies, but a fairly large number of ratings are required to obtain small error bounds.\nIn addition to general models like \\(t\\)-\\(a\\)-\\(p\\), and the hierarchical model illustrated in this section, mixed effects models are straightforward to construct. For example, a \\(t\\)-\\(a_i\\)-\\(p\\) model can take individual rater accuracies into account when estimating \\(t\\) and \\(p\\). This will shrink the accuracy parameters toward their mean, so they are no longer usable as estimates for individual raters."
  },
  {
    "objectID": "hierarchical.html#threshold-functions",
    "href": "hierarchical.html#threshold-functions",
    "title": "Chapter 4: Hierarchical Models",
    "section": "",
    "text": "One way to retreat slightly from discrete truth values is to borrow an idea from the latent scale approach used in log-odds methods, and parameterize \\(t_i\\) as \\(t_i = \\tau(s_i)\\), where \\(s_i \\epsilon \\mathbb{R}\\) is a latent variable representing characteristics of the subject being observed and categorized, and \\(\\tau\\) is a thresholding function that very nearly maps \\(x\\) to \\(\\{0,1\\}\\) in a continous manner. A reasonable choice for \\(\\tau\\) is a sigmoid like the logistic function, \\(\\tau(s) = \\frac{1}{1+e^{-s}}\\), which has the property that \\(\\tau(s) \\rightarrow 0\\) as \\(s \\rightarrow -\\infty\\) and \\(\\tau(s) \\rightarrow 1\\) as \\(s \\rightarrow \\infty\\). One advantage of this approach is that we can build full regression models for the parameters, e.g. to incorporate subject-specific or rater-specific information as explanatory variables. In simulations, this approach can work, but the sigmoid operation significantly slows down convergence of the MCMC sampler.\nA second approach, which deserves more analysis, is to penalize the likelihood function with a term proportional to \\(\\sum{t_i(1-t_i)}\\). This is a crude way to encourage the sampler to find values of \\(t_i\\) that are close to 0 or 1, but not exactly 0 or 1. This also slows down computation significantly.\nThe third, most practical solution for simple indexing, where explanatory variables are not needed, is to first estimate the three-parameter \\(t\\)-\\(a\\)-\\(p\\) model and then use those estimates to recenter the indexed parameters of interest. In simulations, this worked very will in a fully-indexed \\(t_i\\)-\\(a_j\\)-\\(p_j\\) model, where \\(i\\) indexes subjects being classified and \\(j\\) indexes raters. But it is not clear that it will work in all cases.\nThe likelihood function for \\(t_i\\)-\\(a_j\\)-\\(p_j\\) can be expressed as the product of independent ratings, as\n\\[\n\\begin{aligned}\n\\text{L}(a_j,t_i,p_j;c_{ij}) & = \\prod_{ij} \\left[ c_{ij}(\\tau(s_i)a_j+\\bar{a}_jp_j) + \\bar{c_{ij}}(1 -\\tau(s_i) a_j -\\bar{a}_jp_j) \\right]\\\\\n\\end{aligned}\n\\]\nwhere \\(t_i \\epsilon(0,1)\\) is the estimated probability that the \\(i\\)th case is Class 1, \\(a_j\\) is the estimated accuracy of the \\(j\\)th rater, and constant \\(c\\) is the known fraction of \\(\\overline{C}\\) ratings in the whole data set. The prior distributions of the parameters \\((\\t_i = \\tau(s_i),a_j,p_j)\\) are assumed to be uniform on \\((0,1)\\).\nTo interpret induced parameters in the context of the model in figure \\(\\ref{fig:tree}\\), a slight modification to the likelihood function is useful. The generative model in figure \\(\\ref{fig:tree}\\) is conditional on the true class being binary, not a probability between zero and one. This departure from the model is necessary computationally, so that the truth probability of each subject can smoothly evolve to maximize likelihood, but in practice, we’d like the values of \\(t_i\\) to be close to zero or one when computing likelihoods. This is accomplished using a soft threshold on \\(t_i\\) with a sigmoid \\(\\text{sig}_d(t) := 1/(1 + e^{-d(t-.5)})\\), where \\(d\\) is a discrimination parameter that adjusts steepness. The final likelihood function is\n\\[\n\\text{L}(a_j,t_i;c_{ij},P) = (-1)^{c_{ij}'}|a_j(\\text{sig}_d(t_i) -P)| + c_{ij}P + c_{ij}'P'. \\tag{3} \\label{eq:re_model}\n\\]\nInference of the \\(N + R\\) parameters maximizes likelihood using \\(\\eqref{eq:re_model}\\) assuming that each rating is independent of the others so that total probability is the product of the individual rating probabilities.\nTo test the model \\(N=1000\\) subjects with \\(R=5\\) raters each were simulated with a Class 1 rate of \\(p=.20\\), and with rater accuracies of .1, .3, .5, .7, and .9. The sigmoid discrimination parameter was set to \\(d=30\\). The model was specified in the Stan programming language, with four chains of 1000 iterations each, the first 200 being discarded as warm-up samples to control auto-correlation.\n\n\nCode\n# see code/plain model detail sim.R for details\n\nsim_summary_a &lt;- read_rds(\"data/sim_summary.rds\")\n\nsim_summary_a |&gt; \n  ggplot(aes(x = as.factor(n_raters), y = mean_error,\n             ymin = mean_error - SE*2, \n             ymax = mean_error + SE*2,\n             group = `Number of subjects`,\n             linetype = `Number of subjects`)) +\n  geom_point(size = 2, position = position_dodge(.3)) +\n  geom_errorbar(position = position_dodge(.3),\n                width = .1) +\n  theme_bw() +\n  xlab(\"Number of Raters\") +\n  ylab(\"Error in Accuracy\")\n\n\n\n\n\n\n\n\nFigure 1: Estimates of individual rater accuracy with uniformly distributed t-a-p parameters indexed by rater and subject, with bars encompassing two standard deviations from the mean. Each data point represents 200 simulations, using the three parameter t-a-p model to adjust individual accuracy estimates.\n\n\n\n\n\n\n\nCode\n# see code/plain model detail sim.R for details\n\nsim_summary_t &lt;- read_rds(\"data/sim_summary_t.rds\")\n\nsim_summary_t |&gt; \n  ggplot(aes(x = as.factor(n_raters), y = mean_error,\n             ymin = mean_error - SE*2, \n             ymax = mean_error + SE*2,\n             group = `Number of subjects`,\n             linetype = `Number of subjects`)) +\n  geom_point(size = 2, position = position_dodge(.3)) +\n  geom_errorbar(position = position_dodge(.3),\n                width = .3) +\n  theme_bw() +\n  xlab(\"Number of Raters\") +\n  ylab(\"Error in Classification\")\n\n\n\n\n\n\n\n\nFigure 2: Estimates of individual subject classification accuracy with uniformly distributed t-a-p parameters indexed by rater and subject, with bars encompassing two standard deviations from the mean. Each data point represents 200 simulations, using the three parameter t-a-p model to adjust individual accuracy estimates.\n\n\n\n\n\nFor a simulated data set of 5000 ratings analyzed in figure \\(\\ref{fig:sim_truth}\\), the cases are separated quite well even with only five ratings each. The estimated class probabilities \\(t_i\\) on the horizontal axis take into account estimated rater accuracy, which is imputed for each rater simultaneously. The distributions of posterior samples from the Stan output shows that the parameter estimates recapture individual rater accuracy from the simulated ratings. This does not happen without the soft threshold function to nudge the \\(t_i\\)s toward zero or one. For smaller data sets, the estimates become noisier. The estimates in figure \\(\\ref{fig:sim_truth2}\\) have only 100 subjects with three raters each, with randomly chosen accuracies. It shows much less certainty about the estimates.\nIf the model fits the data, the examples show it is possible to recover latent true class probabilities and individual rater accuracies, but a fairly large number of ratings are required to obtain small error bounds.\nIn addition to general models like \\(t\\)-\\(a\\)-\\(p\\), and the hierarchical model illustrated in this section, mixed effects models are straightforward to construct. For example, a \\(t\\)-\\(a_i\\)-\\(p\\) model can take individual rater accuracies into account when estimating \\(t\\) and \\(p\\). This will shrink the accuracy parameters toward their mean, so they are no longer usable as estimates for individual raters."
  }
]