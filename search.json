[
  {
    "objectID": "fit.html",
    "href": "fit.html",
    "title": "Chapter 6: Assessing Model Fit",
    "section": "",
    "text": "1 Assessing Model Fit\nTo assess the quality of a regression model, we often want to compare the original data to the model specification, which will tend to simplify the data in the direction of the model. The usefulness of the model depends on how well it represents the data in terms of our use cases. The best guide is experience in building models on data you understand. My main advice is not to think of goodness of fit tools as providing “truth,” but rather think of truth as a verb: make lots of models, until it becomes second nature to mentally compare the fit statistics to your use cases. In this way, the tools described in this chapter are like flashlights that let you see aspects of the data and its simplified form, and if you spend enough time looking, you’ll figure out how it’s useful. One way to gain experience is to simulate data sets with known properties, and then see if you can recover those properties.\nThe main measure of model fit is the likelihood, which we’re presenting as entropy measured in bits per rating. Within the same data set, we can compare models of different types, including different sorts of parameters, using this metric. This is one way to tell if the average t-a-p model with three parameters is “good enough” to describe a data set, or if we’d be better off with a hierarchical model.\nLikelihood alone is not enough to give us confidence in a model. It’s necessary to lay eyes on the results, usually through a calibration plot, which is the subject of the first section below. The underlying question is whether or not the assumption of a latent truth variable can be supported by the evidence.\nThe sections below address some topics related to model fit. To see these in use, refer to the examples that show these tools applied to real data sets.\n\n\n2 Units of Likelihood\nThe likelihood of a ratings set under the assumption of given parameters was derived in Chapter 5. In practice, we use the log (base 1/2) of the likelihood as a measure of model fit. This produces a number of bits, which are averaged over all ratings to get a comparable figure of the rating entropy. This is the figure reported out in the tapModel functions like tapModel::subject_calibration(). At the moment, the tapModel::fit_counts() estimation function uses binomial mixtures directly on the counts of Class 1 ratings, so its log likelihood cannot be interpreted in the same way the hierarchical log likelihood can. To directly compute the log likelihood for a hierarchical model, use tapModel::bits_per_rating(rating_params).\nA simple form of the likelihood function is when the truth values are known. In practice, the probabilities \\(t_i\\) often closely approximate this situation. In that case we have\n\\[\nPr(\\text{data ; parameters}) = \\prod_{T_i = 1}\\pi_{1k_j}^{(j)} \\prod_{T_i = 0}  \\pi_{0k_j}^{(j)} ,\n\\tag{1}\\]\nwhere the products are over all ratings assigned to subjects that are Class 1 or Class 0 (respectively) in truth. With many data sets, as an estimation algorithm converges toward values of \\(t_i\\) that are close to 0 or 1, the likelihood converges to this form. Using approximations of \\(a_j \\approx a\\), \\(p_j \\approx p\\), we can see that\n\\[\n\\begin{aligned}\n\\ell(\\text{data ; parameters}) &= \\sum_{T_i = 1} \\log \\pi_{1k_j}^{(j)} +  \\sum_{T_i = 0}  \\log \\pi_{0k_j}^{(j)} \\\\\n& \\approx \\sum_{k\\epsilon {0,1}} ( t \\pi_{1k}\\log \\pi_{1k} +  \\bar{t}\\pi_{0k}\\log \\pi_{0k}  ) \\\\\n&= t\\bar{a}\\bar{p}\\log \\bar{a}\\bar{p} + t(a +\\bar{a}p)\\log (a +\\bar{a}p)\\\\  \n&+ \\bar{t}(a+\\bar{a}\\bar{p})\\log (a+\\bar{a}\\bar{p})+ \\bar{t}(\\bar{a}p)\\log (\\bar{a}p)\n.\n\\end{aligned}\n\\tag{2}\\]\nFrom Equation 2, we can derive the cases when \\(a=1\\) and \\(a=0\\) that were discussed earlier.\nEach rating of Class 1 by rater \\(j\\) contributes log likelihood in the form \\(\\log \\pi_{1k_j}^{(j)}\\). Recall that \\(k_j\\) is the assigned class (0 or 1) by that rater for that subject. It’s reasonable to average over these to create an measure of fit that’s comparable across hierarchical models. It gets more complicated with Bayesian models because of prior distributions on the parameters, but put that aside for now.\nIn the usual treatment, the log base is 2 or \\(e\\), in which case the log of any probability is between negative infinity (for very small probabilities) and zero (for probability of one). The reason for log base 2 is that we can think of \\(-\\log_2 p = \\log_2 1/p\\) as bits of information, thanks to Shannon’s entropy formula. I’ll take this a step further and use log base 1/2, so we don’t need the pesky negatives, which are only there because of the mathematicians’ habit of only using log bases greater than one.\nInformation in log-base-one-half equates a probability to fair coin flips. A probability of 1/8 is \\(\\log 1/8 = 3\\) bits of information (since \\(2^3 = 8\\)), corresponding to eight possible outcomes starting with three heads (HHH) and proceeding through seven more variations like HTH to TTT. The information is that novelty is introduced into the world when we create the three classifications (heads or tails) based on random events. Non-random events don’t produce information. So if a coin has two heads, we’d always get HHH, so the probability of that single outcome is 1, and \\(\\log 1 = 0\\) bits.\nFrom the point of view of ratings, perfect accuracy doesn’t “give us information,” which sounds weird, so it’s more natural to substitute “entropy” (randomness) for “information.”\n\n\n\n\n\n\nNote\n\n\n\nI will use the convention that \\(\\log x\\) means \\(\\log_{\\frac{1}{2} } x\\), without specifying the base each time. This reads oddly for those used to the usual log bases, since the log of any probability with base greater than one is negative. Hence there are a lot of expressions that start with negative signs in the usual likelihood formulations. The advantage of log base 1/2 is that we don’t need the negative signs to get a result in bits. Another important difference is that when we maximize likelihood, we minimize the number of informational bits per rating (again, because of the lack of a minus sign).\n\n\nTo get a sense of how this works, suppose all the raters have \\(a_j = 1\\), so all the ratings are accurate. Then the probability \\(\\pi\\) of the assignment of any given rating is 1, since the rater is certain to get the right answer. In that case, \\(\\log \\pi = 0\\), so the log likelihood is zero bits per rating. On the other hand, if \\(a_j = 0\\) for all raters, then all the ratings are random assignment. Assume that \\(p_j = p\\) is the constant random assignment rate, so that \\(\\pi_{00} = \\pi_{10} = \\bar{p}\\) and \\(\\pi_{01} = \\pi_{11} = p\\). In that case, the average log likelihood per rating is\n\\[\n\\frac{\\ell(\\text{data ; parameters})}{N}  = p \\log p+ \\bar{p} \\log \\bar{p} ,\n\\tag{3}\\]\nwhere there are \\(N\\) ratings. If \\(p = 1/2\\) this is the entropy of flipping a coin, which is 1 bit per rating.\nUsing this convention, likelihood is maximized when accuracy is 100%, and the log likelihood is zero bits per rating. The worst case for any t-a-p model ratings set is when accuracy is 0%, and (with constant \\(p\\)) we get average bits per rating that depends on the value of \\(t\\) and \\(p\\). Because of convexity, this value attains a maximum when \\(t = p = .5\\), so the maximum number of bits per rating is \\(s = \\log .5 = 1\\), the entropy from a coin flip. This line of thought shows us that maximizing likelihood is minimizing model randomness, which we’ll call entropy. Minimizing entropy is minimizing randomness left over in the ratings model, rather like a least squares regression minimizes residuals (the part we can’t explain with the model).\nThe log likelihood in bits per rating can be estimated using tapModel::bits_per_rating(rater_params) as shown below.\n\n\nShow the code\nxlogx &lt;- function(x){\n  if_else(x == 0, 0, -x*log(x)/log(2))\n}\n\nset.seed(12345)\n\noutput &lt;- expand.grid(t = c(.1, .5), a = seq(0,1,.05), p = c(.1, .5), LL = NA)\n\nfor(i in 1:nrow(output)){\n  params = list(t = output$t[i], a = output$a[i], p =output$p[i])\n  \n  # simulate data\n  ratings &lt;- generate_sample_ratings(N_s = 1000, params = params, details = TRUE)\n  \n  # use the average t-a-p params used to generate the data as heirarchical \n  # values for raters, and estimate from that the t_i values for each \n  # subject.\n  rating_params &lt;- ratings |&gt; \n      as_rating_params(params) |&gt; \n      estimate_ti()\n  \n  # store results\n  output$LL[i] &lt;- bits_per_rating(rating_params)\n  output$LL_approx[i] &lt;- expected_bits_per_rating(params)\n  \n}\n\noutput %&gt;%\n  gather(type, value, LL, LL_approx) %&gt;%\n  mutate(\n    tp = str_c(\"t=\", t, \" p=\", p),\n    type = recode(type, \n                  LL = \"Empirical\",\n                  LL_approx = \"Expected\"),   # nice legend labels\n    type = factor(type, levels = c(\"Empirical\", \"Expected\"))\n  ) %&gt;%\n  ggplot(aes(x = a, y = value, color = tp, group = interaction(tp, type),\n             linetype = type)) +\n  geom_line() +\n  theme_bw() +\n  ylab(\"Bits per rating\")\n\n\n\n\n\n\n\n\nFigure 1: Log likelihood in bits per rating for a variety of parameters, with 1000 samples. The dotted lines are the expected values and the solid lines are the empirical ones, which have sampling noise.\n\n\n\n\n\nThe overplotted purple and green lines in Figure 1 illustrate that the entropy (bits per rating) doesn’t depend on \\(t\\) if \\(p = 1/2\\). We also see that as \\(a\\) approaches one, the entropy approaches zero. However, the case \\(t = .5\\) and \\(p=.1\\) shows that decreasing entropy (i.e. maximizing likelihood) is not always the same thing as maximizing accuracy \\(a\\). In that case, as ratings become inaccurate, 90% of them tend toward Class 0, which reduces entropy to an expected value of \\((.1)\\log(.1) + (.9)\\log(.9)=\\) 0.47 bits per rating, per Equation 3. Note that neither the formulas nor plots for entropy at \\(a=0\\) depend on \\(t\\); the true class only matters if accuracy is positive.\nI found it surprising how large entropy is when all the parameters are .5 (\\(s = .81\\) bits per rating). If we disaggregate the contributions for each of the four cases, the conditions where the rating matches the true value (from \\(\\pi_{00}\\) and \\(\\pi_{11}\\)) contribute .5 to the entropy. This is because half of the correct answers are due to coin flips (Gettier-like conditions). The wrong answers (from \\(\\pi_{10}\\) and \\(\\pi_{01}\\)) contribute another .31 bits per rating of entropy. It’s probably better, however, to think of the entropy as a mixture of two sources, corresponding to Class 0 and Class 1, with \\(t\\) as the mixing parameter. This idea leads to the need to incorporate the class-specific parameters, e.g. \\(a_0, a_1\\) into both the E-M solver and the log likelihood calculations. I have not done that yet, however. See the wine judging example for more development of the idea.\n\n\n\n\n\n\nNote\n\n\n\nIf we use average parameters as produced by fit_counts() to estimate empirical bits per rating, first apply the E-step of the E-M algorithm, which estimates the individual \\(t_i\\) probabilities. This can be done with ratings |&gt; as_rating_params(params) |&gt;  estimate_ti() |&gt; bits_per_rating(), and you can see an example of this in the code block that generates Figure 1. If you’re satisfied with the expected entropy, the function expected_bits_per_rating(params) will do it with less fuss. Usually this is good enough.\n\n\nWhat if we want to compare the entropy (hence likelihood) of ratings from the same source, but that have different estimated values of \\(t\\)? Since the proportion of true Class 1 cases could vary by sample, it would be convenient to factor that out when comparing the other parameters. We could do this by artificially setting \\(t=.5\\) in the parameters and leaving \\(a\\) and \\(p\\) alone. There’s a convenience function for this called rater_entropy(params) but I’d consider this experimental. Another approach might be to sub-sample ratings to balance the Class 1 cases, but this is awkward and subject to all kinds of problems.\n\n\n3 Model Calibration\nCalibration means that a model can reproduce the basic distributional properties of the data. The most basic comparison we can make between a ratings set and its model is to look at the distribution of Class 1 ratings by subject. The tapModel package has a function to do this, subject_calibration(), which takes a data set and a set of model parameters and produces a comparative plot of the distribution of Class 1 ratings for the original data set and the simulated data sets from the model. This is a good first step to see if the model is reasonable.\nThe set up here is that we have 300 subjects and 25 raters each, with randomly-generated rater parameters that average 1/2, with \\(t = .1\\) fixed.\n\n\nShow the code\nset.seed(123)\nn_subjects &lt;- 300\nn_raters   &lt;- 25\n\nratings &lt;- tapModel::generate_ti_aj_pj_ratings( rating_params = NULL, \n                param_list = list(\n                    t = rep(.1,n_subjects), #runif(n_subjects)/2 + .25,\n                    a = runif(n_raters)/2 + .25, \n                    p = runif(n_raters)/2 + .25)) \n# average tap model\nparams &lt;- ratings |&gt; \n          as_counts() |&gt; \n          fit_counts()\n\navg_params &lt;- ratings |&gt; \n              as_rating_params(params)\n\np1 &lt;- subject_calibration(avg_params, n_sim = 50)\n\n# direct-method hierarchical model\nrating_params &lt;- fit_ratings(ratings)\n\np2 &lt;- subject_calibration(rating_params, n_sim = 50)\n\ncowplot::plot_grid(p1, p2, labels = c('A', 'B'), label_size = 12)\n\n\n\n\n\n\n\n\nFigure 2: Distribution of Class 1 ratings over subjects, showing the density of Class 1 ratings for the original data set containing noise (blue) and the model with true parameters (orange). Plot A is the average t-a-p model and plot B is the hierarchical version.\n\n\n\n\n\nThe plots in Figure 2 show that both the average (A) and hierarchical models (B) do a good job of reproducing the distribution of Class 1 ratings across cases. There are two evident modes (bumps where the density piles up). The left one is larger, because \\(t= .1\\) means that there are few Class 1 cases to be detected, so there are more false positives than true positives. The mode on the right is the true positives. The dashed vertical lines denote where the model parameters say these modes should be on average.\nNotice that although the error measures MAE (mean absolute error) and RMSE (root mean squared error) are the same for both models, the log likelihood is smaller (better) for the hierarchical model. This is what we’d expect, since adding parameters to a model will generally improve it.\nIn addition to comparing the distributions of Class 1 ratings over subjects, we can use raters as the basis for comparisons, to assess whether the modeled rate of Class 1 ratings per rater matches the empirical assignments.\n\n\nShow the code\np1 &lt;- rater_calibration(avg_params)\np2 &lt;- rater_calibration(rating_params)\n\ncowplot::plot_grid(p1, p2, labels = c('A', 'B'), label_size = 12)\n\n\n\n\n\n\n\n\nFigure 3: Distribution of Class 1 ratings for a simulated data set, showing the cumulative distribution of Class 1 ratings for the noisy data (blue) and the model with estimated average t-a-p parameters (orange).\n\n\n\n\n\nThe rater calibration plot in Figure 3 shows the main difference in the models. The average model (A) doesn’t distinguish between raters, so the predicted rate of Class 1 assignments is the same for each. For the hierarchical (B) plot, the rates are perfect.\nA t-a-p model produces a predicted Class 1 probability for each rating, which we can compare to the empirical distribution of ratings. To do this, we calculate \\(Pr(\\text{Class 1 rating}; t_i, a_j, p_j) = c_{ij}\\) for each rating, put these into bins and calculate the average number of actual Class 1 ratings for each bin.\n\n\nShow the code\np1 &lt;- rating_calibration(avg_params)\np2 &lt;- rating_calibration(rating_params)\n\ncowplot::plot_grid(p1, p2, labels = c('A', 'B'), label_size = 12)\n\n\n\n\n\n\n\n\nFigure 4: Distribution of Class 1 ratings for a simulated data set, showing the cumulative distribution of Class 1 ratings for the noisy data (blue) and the model with estimated average t-a-p parameters (orange).\n\n\n\n\n\nWe can again see the hierarchical fitting in Figure 4, with (A) showing constant modeled values of \\(c_{ij}\\), since all the parameters are the same for each rating, versus the individually fitted parameters that result in (B).\nWe might conclude from the calibration plots that both models give useful information, but if we are seeking information about the individual raters or subjects, the hierarchical model is perfectly usable for that purpose.\n\n\n4 Estimating Parameter Error\nThe tapModel library includes a function to assess the estimation error of parameters. It does this by using the parameters to simulate ratings sets, solving those and comparing the results. The resulting variation in parameters will include sampling error, so the margins will be wider for smaller data sets. The code below illustrates.\n\n\nShow the code\nresults &lt;- estimate_rater_parameter_error(rating_params, n_sim = 10)\nresults$plot_a\n\n\n\n\n\n\n\n\nFigure 5: Approximate estimation error for the a_j parameters. The red markers are the original parameter estimates, and the boxplots show the range of parameters generated from simulated data sets.\n\n\n\n\n\nA similar plot to Figure 5 is also provided for the \\(p_j\\) parameters. Other functions for assessing model fit can be found in Chapter 6.\nWe can also use MCMC methods to generate densities of likely parameter values.\n\n\nShow the code\nset.seed(12345)\nratings &lt;- generate_sample_ratings(params = list(t = .3, a = .5, p = .5))\n\n# fit individual parameters  \nmcmc_results &lt;- ratings |&gt;\n  fit_ratings_mcmc(quiet = TRUE)  \n\nmcmc_draws &lt;- mcmc_results$fitted_model$draws() |&gt; \n  extract_param_draws_mcmc()\n\nmcmc_draws |&gt; \n  select(`a[1]`,`a[2]`,`a[3]`) |&gt; \n  mutate(row = row_number()) |&gt; \n  gather(param, value, -row) |&gt; \n  ggplot(aes(x = value)) +\n  geom_density(fill = \"steelblue\", alpha = .5) +\n  xlim(0,1) +\n  facet_grid(. ~ param) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 6: MCMC draw densities for the first three accuracy parameters.\n\n\n\n\n\nThe densities from the MCMC draws, like those in Figure 6 can be used to create credible intervals for inferance and to look for multi-modality. See Chapter 4’s disussion of the Kappa Paradox for a good example of the latter. The fitted model that’s returned from the fitting function can also be used with the interactive Shiny app shinystan and other tools, such as the stan playground from the Flatiron Institute.\n\n\n5 Model Exaggeration\nThere is a particular issue worth mentioning with smaller data sets, particularly those with few raters per subject. There are cases where we can increase the likelihood of the model while degrading its performance. Recall the likelihood function for the hierarchical t-a-p model, which assigns parameters to each subject and rater. For a single subject, its form is\n\\[\nPr[\\text{data|params}] = t_i\\pi_1+\\bar{t}_i \\pi_0,\n\\] where the \\(\\pi_1\\) factor is the product of all the rater probabilities assuming the true class is 1. Assume that the \\(\\pi\\)s are not equal, e.g. \\(pi_1 &gt; pi_0\\). Then we can maximize the probability (likelihood) by setting \\(t_i = 1\\) and therefore \\(\\bar{t}_i = 0\\). Given any proposed set of hierarchical parameters, we can always increase the likelihood by rounding the estimated \\(t_i\\) values to 0 or 1. This is a kind of model exaggeration, because it hides the actual uncertainty.\nThis exaggeration can cause a seeming paradox, where the likelihood increases as the modeled distribution diverges from the empirical one. If the data fits the model, this may not be noticeable, but the presence of noise will cause the “rounding” operation to make more mis-classifications, so the model is more confident about the wrong answer. A sign of this is when model registration gets worse with the hierarchical parameters. Registration here means the match between the distribution of Class 1 ratings per subject in the data set versus the model’s predictions.\nIf we allow the \\(t_i\\) parameters to tend too much toward their \\([0,1]\\) limits, the model is more sensitive to error. In this example, it’s sampling error. With only three raters, it’s not a good assumption that a probability less than one half should be rounded to zero, etc. Other kinds of model error could have a similar effect. In the extreme case where all the \\(t_i\\) parameters are 0 or 1, we’re truncating the likelihood function as shown below.\n\n\nShow the code\n#' Illustrate a t-a-p binomial mixture distribution of Class 1 Counts\na &lt;- .4\np &lt;- .4\npr_c0 &lt;- (1-a)*p \npr_c1 &lt;- a + pr_c0\n\ndistr &lt;- data.frame(N_c = 0:10) |&gt; \n           rowwise() |&gt; \n           mutate(C0 = dbinom(N_c, 10, pr_c0),\n                  C1 = dbinom(N_c, 10, pr_c1),\n                  # apply the cut point \n                  C0_cut = if_else(C0 &gt; C1, C0 + C1, NA),\n                  C1_cut = if_else(C1 &gt; C0, C0 + C1, NA)) |&gt;\n           gather(Class, Probability, -N_c) |&gt; \n           separate(Class, into = c(\"Class\",\"Type\"), sep = \"_\") |&gt; \n           replace_na(list(Type = \"original\"))\n\ndistr |&gt; \n  ggplot(aes(x = N_c, y = Probability, color = Class, linetype = Type)) +\n  geom_point() +\n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = 0:10) +\n  xlab(\"Number of Class 1 Ratings out of 10\")\n\n\n\n\n\n\n\n\nFigure 7: Sample distribution of Class 1 ratings counts, with \\(a = .4\\) and \\(p=.4\\) after a cut point is applied at 4.\n\n\n\n\n\nThe distributions in Figure 7 show the distorting effect of applying a cut point at four ratings of Class 1 with \\(a = p = .4\\). The original distributions are shown in the dashed lines, and the cut point distributions are shown in the solid lines. The cut point process biases the likelihood function due to the truncation where the two binomial distributions overlap.\nModel exaggeration won’t usually be a problem when there are many raters, because the \\(t_i\\) values will be close to zero or one with high confidence anyway.\n\n\n6 Adversarial Raters\nNot every data set will fit the t-a-p assumptions. In that case, we’d expect to see divergence between observed and modeled data even after fitting the data with average or hierachical parameters. One such case is when we have non-independent ratings. Another case is if some raters are adversarial, meaning that they report exactly inaccurately, inverting the rating for what would normally be accurate ratings.\n\n\nShow the code\nset.seed(123)\nn_subjects &lt;- 300\nn_raters   &lt;- 20\nN &lt;- n_raters*n_subjects\n\nratings &lt;- tapModel::generate_sample_ratings(N_s = 100, N_r = 10, \n                                             params = list(t = .5, a = .7, p = .5),\n                                             details = TRUE) \n\nbad_ratings &lt;- ratings |&gt; \n  # flip accurate ratings for the first ten raters\n  mutate(rating = if_else(rater_id &lt;= 3 & A_ij == 1, 1 - rating, rating)) \n\n# get counts and find the average params\n\navg_params &lt;- ratings |&gt; \n              as_counts() |&gt; \n              fit_counts()\n\nbad_params &lt;- bad_ratings |&gt; \n  as_counts() |&gt; \n  fit_counts()\n\n# plot the computed average model\np1 &lt;- ratings |&gt; \n            as_rating_params(avg_params) |&gt; \n            subject_calibration()\n\np2 &lt;- ratings |&gt; \n  as_rating_params(bad_params) |&gt; \n  subject_calibration()\n\n\ncowplot::plot_grid(p1, p2, labels = c(\"A\",\"B\"))\n\n\n\n\n\n\n\n\nFigure 8: Distribution of Class 1 ratings for a simulated data set, showing the subject calibration for the original data set (A) and one with three adversarial raters (B).\n\n\n\n\n\nThe B plot in Figure 8 shows that the model can’t capture the actual distribution of ratings when three out of ten raters invert their ratings. There are two signs that something is wrong. First the entropy is very high in the second plot, and second the modeled density doesn’t look anything like the observed one; the error measures are an order of magnitude worse.\nIn such cases, we might want to try to identify adversarial raters to “fix” the data by filtering out the bad actors.\nEven if we switch to a heirarchical model, it doesn’t completely solve the problem. Adversarial raters are not considered in the basic t-a-p model, so the assumption about the behavior of accurate raters fails. However, we can estimate the accuracy of each rater and use that to filter them. The resulting calibration plot is show in Figure 9.\n\n\nShow the code\nfixed_rating_params &lt;- fit_ratings(bad_ratings) |&gt; \n                 filter(a &gt; .01)\n\nsubject_calibration(fixed_rating_params)\n\n\n\n\n\n\n\n\nFigure 9: An attempt to eliminate bad raters by filtering on accuracy.\n\n\n\n\n\nThe average parameters in Figure 9 are pretty close to the values used to generate the original data set, and the calibration plot looks okay. Obviously, we’d want to dig into the details more deeply for an actual study. Which raters were eliminated, and what evidence is there that they are rating backwards?\n\n\n7 Independence\nImagine that you’ve been asked to assess the reliability of portfolio ratings done by an assessment committee. Three committee members independently review each student’s portfolio of work and rate is as pass (Class 1) or fail (Class 0). Unknown to you, the program director has added a forth column to the data, which appears as a fourth rater, but actually represents the majority vote. You’re not aware of this dependent artificial “rater” and proceed as if it’s another independent vote.\n\n\nShow the code\nset.seed(123)\n\nratings &lt;- tapModel::generate_sample_ratings(N_s = 100, N_r = 3, \n                                             params = list(t = .8, a = .3, p = .8)) \n\nbad_ratings &lt;- ratings |&gt; \n  # create a majority vote column\n  spread(rater_id, rating) |&gt; \n  mutate(`4` = as.integer(`1` + `2` + `3` &gt; 1)) |&gt; \n  gather(rater_id, rating, -subject_id)\n\n# get counts and find the average params\navg_params &lt;- ratings |&gt; \n  as_counts() |&gt; \n  fit_counts()\n\nbad_params &lt;- bad_ratings |&gt; \n  as_counts() |&gt; \n  fit_counts()\n\n# plot the computed average model\np1 &lt;- ratings |&gt; \n  as_rating_params(avg_params) |&gt; \n  subject_calibration()\n\np2 &lt;- ratings |&gt; \n  as_rating_params(bad_params) |&gt; \n  subject_calibration()\n\n\ncowplot::plot_grid(p1, p2, labels = c(\"A\",\"B\"))\n\n\n\n\n\n\n\n\nFigure 10: Distribution of Class 1 ratings for a simulated data set, showing the subject calibration for the original data set (A) and one with a non-independent rater (B).\n\n\n\n\n\nThe plots in Figure 10 contrast the model fit without the dependent data (A) and with it (B), showing a poor subject calibration for the latter. This is good! Dependent raters violate the model assumptions, and we’d hope that the model would not fit in such cases.\nWe can attempt to detect non-independence using a correlation matrix among raters.\n\n\nShow the code\nbad_ratings |&gt; \n  spread(rater_id, rating) |&gt; \n  select(-subject_id) |&gt; \n  cor() |&gt; \n  kable(digits = 2) |&gt; \n  kable_styling(\n    full_width = FALSE,           # &lt;- stops spanning the full page\n    position   = \"center\",\n    bootstrap_options = c(\"striped\",\"hover\",\"condensed\")\n  )\n\n\n\n\nTable 1: Correlation matrix among raters per subject.\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\n1.00\n0.06\n0.13\n0.55\n\n\n0.06\n1.00\n0.00\n0.34\n\n\n0.13\n0.00\n1.00\n0.50\n\n\n0.55\n0.34\n0.50\n1.00\n\n\n\n\n\n\n\n\nNotice that the correlations for rater 4 in Table 1 are all larger than the rest. This test isn’t definitive, but it gives us a reason to look into rater 4, at which point we would likely discover the problem.\nThe correlation is problematic when raters have no variation in their ratings. A more general approach is to use the covariance of ratings between raters. See the formula for correlation between raters, found in the Appendix for how to proceed. Given two raters, we’ll compute the centered dot product of their ratings as the empirical covariance, and use the formula is \\(a_1a_2 t \\bar{t}\\) as the expected value. This is done in the tapModel::rater_cov() function. Using that idea, we can find the difference between expected and actual values by rater pair.\nFor larger numbers of raters, we can benchmark the expected range of covariance differences by using simulated data with the rater_cov_quantile() function. This is illustrated with another problem we might encounter: duplicated raters, where the ratings are identical or nearly so.\n\n\nShow the code\nset.seed(123)\n\nratings &lt;- tapModel::generate_sample_ratings(N_s = 200, N_r = 90, \n                                             params = list(t = .5, a = .5, p = .5)) \n\ndupe_ratings &lt;- ratings |&gt; \n  filter(rater_id &lt;= 10) |&gt; \n  mutate(rater_id = rater_id + 90)\n\nbad_ratings &lt;- rbind(ratings, dupe_ratings)\n \n# find the model \nbad_params &lt;- bad_ratings |&gt; \n          as_counts() |&gt; \n          fit_counts()\n\n# create the expanded data \nrating_params &lt;- bad_ratings |&gt;  \n                 as_rating_params(bad_params)\n\n# range of expected correlations\nrange &lt;- rater_cov_quantile(rating_params)\n\n# expected and actual correlations\ncov_results &lt;- rating_params |&gt; \n  rater_cov(max_raters = 200)\n\nplot_rater_cov(cov_results, range)\n\n\n\n\n\n\n\n\nFigure 11: Distribution covariances between raters, with 96% confidence interval marked. Ten duplicate raters have been added to show non-independence.\n\n\n\n\n\nNotice the blip to the far right of Figure 11 that shows the ten duplicated raters. These are far outside the expected range, and deserving of our attention.\n\n\n8 The Dunning-Kruger Threshold\nAs a reference point for interpreting coefficients, it can be useful to compute the Dunning-Kruger threshold for the parameters. Recall that this horizon is the largest plausible result for accuracy that can be obtained when accuracy is actually zero. Suppose we have 25 subjects that are each rated by 2 raters, a pretty small data set. Assuming that \\(t = p = .5\\), we can estimate accuracy values from simulated data sets and compare to known values.\nThis computation is limited in that is assumes ratings are unbiased (i.e. \\(t = p\\)). However, it may still be useful as a sanity check. See the discussion of Dunning-Kruger in Chapter 2 for more. If you want more detail, there’s a function to simulate accuracy sampling error called tapModel::simulate_exact_fit()`. An example is found in Figure 12 showing results for 50 subjects with two raters each.\n\n\nShow the code\ntapModel::simulate_exact_fit(25,2,n_sim = 30, output = \"plot\")\n\n\n\n\n\n\n\n\nFigure 12: Ranges of accuracy estimates for simulated rating sets with t = p = .5 samples. The red markers indicate the mean for each estimated value of accuracy.\n\n\n\n\n\nAs accuracy approaches zero, more and more of the sampling error must be larger than the true value, so the average result is biased upwards. A more useful direct calculation with tapModel::dk_horizon() gives the bootstrapped probabilities for the “DK horizon”. These appear in Table 2.\n\n\nShow the code\ndk &lt;- dk_horizon(25, 2, n_sim = 100)\ndk |&gt; \n  as.tibble() |&gt;  \n  mutate(Threshold = names(dk)) |&gt; \n  select(Threshold , Accuracy = value) |&gt; \n  kable(digits = 2)|&gt; \n  kable_styling(\n    full_width = FALSE,           # &lt;- stops spanning the full page\n    position   = \"center\",\n    bootstrap_options = c(\"striped\",\"hover\",\"condensed\")\n  )\n\n\n\n\nTable 2: Estimated percentiles for accuracy, given 600 subjects with ten raters each, when accuracy is actually zero.\n\n\n\n\n\n\nThreshold\nAccuracy\n\n\n\n\n50%\n0.08\n\n\n75%\n0.34\n\n\n90%\n0.45\n\n\n95%\n0.55\n\n\n98%\n0.58\n\n\n\n\n\n\n\n\nFor a data set of this size, 98% of the randomly generated ratings resulted in an accuracy parameter estimate of .53 or less. It’s reasonable to be suspicious of parameter estimates that have \\(a\\) below that threshold. We might be being fooled by sampling error into thinking the raters are much more accurate than they actually are.\n\n\n9 A Validation Process for Binary Ratings\nThe following steps may help you think through the analysis of binary t-a-p models.\n\nCheck Assumptions. Validity testing begins with assumptions about the data-gathering process, so it’s essential to seek expertise with the data type to avoid personal errors. If you didn’t generate the data, talk to whoever did. Ask questions that will build confidence that the encodings are valid, the data format reflects your understanding, and the statistical assumptions. In the formal t-a-p model, the most important assumption to check is the independence of raters. If a group of faculty who are scoring student writing portfolios are sitting around a table discussing the essays, the ratings are not independent. Optionally, use prior experience or literature to write down your expectations for \\(t\\) and \\(a\\). This is so that your ‘surprise factor’ doesn’t get biased by seeing the results first.\nCompute the three-parameter t-a-p solution with params = fit_counts(as_counts(ratings)) and inspect for reasonableness (given your priors) and degeneracy (any coefficient of zero or one).\nCompute the Dunning-Kruger horizon for your data configuration using the number of subjects as \\(N_s\\) and average raters per subject as \\(N_r\\). This gives a value for accuracy, below which any estimate is suspect. Suppose the DK number comes back as .3, using a 98% threshold, and the accuracy estimate for your data is .25. We should double-check the validity of the data.\n\n2.1 If the estimated accuracy is below the DK horizon, fit the model using the MCMC method and inspect the distribution of posterior samples for accuracy. Look for a reasonable range of values that represents uncertainty. If this includes values that are unacceptable, then you may not be able to proceed further. It’s an indication that you need more samples or more reliable ratings or more raters, in some combination. You can simulate these combinations as a kind of power test.\n\nUse the t-a-p parameters to estimate the \\(t_i\\) truth probabilities and rater \\(a_j\\) and \\(p_j\\) parameters using rating_params = fit_ratings(ratings) and compare the model fit for this hierarchical model to the three-parameter version using the calibration plots. We’re looking for two things here. One is whether the hierarchical model has better fit (fewer bits/rating) than the average model. If not, we should probably stick with the three-parameter model to avoid overfitting. The second thing to inspect is overall match between modeled and observed ratings, using the error statistics provided on the calibration plots. See the example above on adversarial raters for an obvious misfit.\nIf the model is not well-calibrated, check the independence assumption using rater_cor() and rater_cor_quantile().\nUse the rating_params, which have all the hierarchical coefficients, to generate new sample ratings and then see if the solver can properly estimate those coefficients. You can generate boxplots for accuracy with estimate_rater_parameter_error(rating_params)$plot_a. This gives us an idea of how much the sampling error combined with estimation error affect the estimates. Alternatively, you can get more detailed reports using the MCMC solver fit_ratings_mcmc(). The shinystan package has interactive tools for detailed analysis.\n\nThis process isn’t intended to produce a single statistic that tells us if the model is “good” or not. Rather, it’s intended to be a work flow that allows an analyst to develop an intuition about the data and models to aid in judgments about applications of the ratings. At all points, it’s necessary to apply judgment to test the results for reasonableness with respect to the subjects, raters, and rating processes that produced the data."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Example: PTSD Assessment",
    "section": "",
    "text": "1 Intro\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\ndf &lt;- data.frame(x = runif(10), y = runif(10))\n\ndf |&gt; \n  ggplot(aes(x = x, y = y)) + geom_point()\n\n\n\n\n\n\n\n\nFigure 1: Test caption\n\n\n\n\n\nAnd a table\n\n\nShow the code\ndf |&gt; \n  kable(digits = 2)  |&gt; \n  kable_styling(\n    full_width = FALSE,           # &lt;- stops spanning the full page\n    position   = \"center\",\n    bootstrap_options = c(\"striped\",\"hover\",\"condensed\"))\n\n\n\n\nTable 1\n\n\n\n\n\n\nx\ny\n\n\n\n\n0.71\n0.15\n\n\n0.66\n0.83\n\n\n0.89\n0.83\n\n\n0.57\n0.69\n\n\n0.10\n0.91\n\n\n0.73\n0.74\n\n\n0.12\n0.39\n\n\n0.71\n0.62\n\n\n0.93\n0.60\n\n\n0.19\n0.02"
  },
  {
    "objectID": "t-a-p diagram.html",
    "href": "t-a-p diagram.html",
    "title": "t-a-p diagram",
    "section": "",
    "text": "1 simplified version with binary vars\nModified to separate branches\nModified to separate branches with probabilities\nand with rates (not used–wrong)\n\n\n2 invariant scale\nDawid &Skene"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\nAgresti, A. (2003). Categorical data analysis (Vol. 482). John Wiley & Sons.\n\n\nAickin, M. (1990). Maximum likelihood estimation of agreement in the constant predictive probability model, and its relation to cohen’s kappa. Biometrics, 293–302.\n\n\nAscari, R., & Migliorati, S. (2021). A new regression model for overdispersed binomial data accounting for outliers and an excess of zeros. Statistics in Medicine, 40(17), 3895–3914.\n\n\nBassett, R., & Deride, J. (2019). Maximum a posteriori estimators as a limit of bayes estimators. Mathematical Programming, 174, 129–144.\n\n\nBennett, E. M., Alpert, R., & Goldstein, A. (1954). Communications through limited-response questioning. Public Opinion Quarterly, 18(3), 303–308.\n\n\nBonett, D. G. (2022). Statistical inference for g-indices of agreement. Journal of Educational and Behavioral Statistics, 47(4), 438–458.\n\n\nBreiman, L. (2001). Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). Statistical Science, 16(3), 199–231. https://doi.org/10.1214/ss/1009213726\n\n\nBrennan, R. L., Measurement in Education, N. C. on, et al. (2006). Educational measurement. Praeger Publishers,.\n\n\nButton, C. M., Snook, B., & Grant, M. J. (2020). Inter-rater agreement, data reliability, and the crisis of confidence in psychological research. Quant Methods Psychol, 16(5), 467–471.\n\n\nByrt, T., Bishop, J., & Carlin, J. B. (1993). Bias, prevalence and kappa. Journal of Clinical Epidemiology, 46(5), 423–429.\n\n\nCarpenter, B. (2008). Multilevel bayesian models of categorical data annotation. Unpublished Manuscript, 17(122), 45–50.\n\n\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., & Riddell, A. (2017). Stan: A probabilistic programming language. Journal of Statistical Software, 76(1).\n\n\nChaturvedi, S., & Shweta, R. (2015). Evaluation of inter-rater agreement and inter-rater reliability for observational data: An overview of concepts and methods. Journal of the Indian Academy of Applied Psychology, 41(3), 20–27.\n\n\nCicchetti, D. V., & Feinstein, A. R. (1990). High agreement but low kappa: II. Resolving the paradoxes. Journal of Clinical Epidemiology, 43(6), 551–558.\n\n\nCohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37–46.\n\n\nCronbach, L. J., Rajaratnam, N., & Gleser, G. C. (1963). Theory of generalizability: A liberalization of reliability theory. British Journal of Statistical Psychology, 16(2), 137–163.\n\n\nDavani, A. M., Dı́az, M., & Prabhakaran, V. (2022). Dealing with disagreements: Looking beyond the majority vote in subjective annotations. Transactions of the Association for Computational Linguistics, 10, 92–110.\n\n\nDawid, A. P., & Skene, A. M. (1979). Maximum likelihood estimation of observer error-rates using the EM algorithm. Journal of the Royal Statistical Society: Series C (Applied Statistics), 28(1), 20–28.\n\n\nDelgado, R., & Tibau, X.-A. (2019). Why cohen’s kappa should be avoided as performance measure in classification. PloS One, 14(9), e0222916.\n\n\nEngelhard, G. (2012). Examining rating quality in writing assessment: Rater agreement, error, and accuracy. Journal of Applied Measurement, 13, 321–335.\n\n\nEngelhard Jr, G. (1996). Evaluating rater accuracy in performance assessments. Journal of Educational Measurement, 33(1), 56–70.\n\n\nEngelhard Jr, G., & Wind, S. (2017). Invariant measurement with raters and rating scales: Rasch models for rater-mediated assessments. Routledge.\n\n\nEubanks, D. (2017). (Re)visualizing rater agreement:beyond single-parameter measures. Journal of Writing Analytics, 1.\n\n\nEubanks, D. (2022). Grades and learning. Journal of Assessment and Institutional Effectiveness, 12(1-2), 1–24.\n\n\nEubanks, D. A. (2014). Causal interfaces. Arxiv.org Preprint. http://arxiv.org/abs/1404.4884v1\n\n\nFleiss, J. L. (1971). Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5), 378.\n\n\nFleiss, J. L., Levin, B., & Paik, M. C. (2013). Statistical methods for rates and proportions. john wiley & sons.\n\n\nGelman, A., & Carlin, J. (2014). Beyond power calculations: Assessing type s (sign) and type m (magnitude) errors. Perspectives on Psychological Science, 9(6), 641–651.\n\n\nGelman, A., & Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge university press.\n\n\nGelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., Kennedy, L., Gabry, J., Bürkner, P.-C., & Modrák, M. (2020). Bayesian workflow. arXiv Preprint arXiv:2011.01808.\n\n\nGettier, E. L. (1963). Is justified true belief knowledge? Analysis, 23(6), 121–123.\n\n\nGrilli, L., Rampichini, C., & Varriale, R. (2015). Binomial mixture modeling of university credits. Communications in Statistics - Theory and Methods, 44(22), 4866–4879. https://doi.org/10.1080/03610926.2013.804565\n\n\nGwet, K. L. (2008). Computing inter-rater reliability and its variance in the presence of high agreement. British Journal of Mathematical and Statistical Psychology, 61(1), 29–48.\n\n\nHinde, K. (2023). Inter-rater reliability in determining criterion a for PTSD and complex PTSD in parents of autistic children using the life events checklist for DSM-5.\n\n\nHodgson, R. T. (2008). An examination of judge reliability at a major US wine competition. Journal of Wine Economics, 3(2), 105–113.\n\n\nHolley, J. W., & Guilford, J. P. (1964). A note on the g index of agreement. Educational and Psychological Measurement, 24(4), 749–753.\n\n\nHovy, D., Berg-Kirkpatrick, T., Vaswani, A., & Hovy, E. (2013). Learning whom to trust with MACE. Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1120–1130.\n\n\nKrippendorff, K. (2013). Commentary: A dissenting view on so-called paradoxes of reliability coefficients. Annals of the International Communication Association, 36(1), 481–499.\n\n\nKrippendorff, K. (2018). Content analysis: An introduction to its methodology. Sage publications.\n\n\nKrippendorff, K., & Fleiss, J. L. (1978). Reliability of binary attribute data. Biometrics, 34(1), 142–144.\n\n\nKruger, J., & Dunning, D. (1999). Unskilled and unaware of it: How difficulties in recognizing one’s own incompetence lead to inflated self-assessments. Journal of Personality and Social Psychology, 77(6), 1121.\n\n\nKumar, S., Hooi, B., Makhija, D., Kumar, M., Faloutsos, C., & Subrahmanian, V. (2018). Rev2: Fraudulent user prediction in rating platforms. Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, 333–341.\n\n\nLandis, J. R., & Koch, G. G. (1977). The measurement of observer agreement for categorical data. Biometrics, 159–174.\n\n\nMcElreath, R. (2020b). Statistical rethinking: A bayesian course with examples in r and stan (2nd ed.). Chapman; Hall/CRC.\n\n\nMcElreath, R. (2020a). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.\n\n\nMcLachlan, G., & Peel, D. (2000). Wiley series in probability and statistics. Finite Mixture Models, 420–427.\n\n\nMillet, I. (2010). Improving grading consistency through grade lift reporting. Practical Assessment, Research, and Evaluation, 15(1).\n\n\nPassonneau, R. J., & Carpenter, B. (2014). The benefits of a model of annotation. Transactions of the Association for Computational Linguistics, 2, 311–326.\n\n\nPaun, S., Carpenter, B., Chamberlain, J., Hovy, D., Kruschwitz, U., & Poesio, M. (2018). Comparing bayesian models of annotation. Transactions of the Association for Computational Linguistics, 6, 571–585.\n\n\nRasch, G. (1977). On specific objectivity. An attempt at formalizing the request for generality and validity of scientific statements in symposium on scientific objectivity, vedbaek, mau 14-16, 1976. Danish Year-Book of Philosophy Kobenhavn, 14, 58–94.\n\n\nRasch, G. (1993). Probabilistic models for some intelligence and attainment tests. MESA Press.\n\n\nRoss, V., & LeGrand, R. (2017). Assessing writing constructs: Toward an expanded view of inter-reader reliability. Journal of Writing Analytics, 1.\n\n\nScott, W. A. (1955). Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly, 321–325.\n\n\nShabankhani, B., Charati, J. Y., Shabankhani, K., & Cherati, S. K. (2020). Survey of agreement between raters for nominal data using krippendorff’s alpha. Arch Pharma Pract, 10(S1), 160–164.\n\n\nShannon, C. E. (1948). A mathematical theory of communication. The Bell System Technical Journal, 27(3), 379–423.\n\n\nShrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing rater reliability. Psychological Bulletin, 86(2), 420.\n\n\nTeam, S. D. (2022). Stan user’s guide 2.34. Stan Development Team.\n\n\nVach, W., & Gerke, O. (2023). Gwet’s AC1 is not a substitute for cohen’s kappa–a comparison of basic properties. MethodsX, 102212.\n\n\nWilliams, D. (1975). 394: The analysis of binary responses from toxicological experiments involving reproduction and teratogenicity. Biometrics, 949–952."
  },
  {
    "objectID": "package.html",
    "href": "package.html",
    "title": "The tapModel Package",
    "section": "",
    "text": "This is a user guide to the tapModel package, which you can find on github. The guide will show you how to format a data set of ratings and fit t-a-p models to the data. The library has been designed to make it as easy as possible to generate parameter estimates and goodness of fit tests, so you can think about the meaning of the results instead of fiddling with data formats. The chart in Figure 1 shows the work flow for analyzing a binary rating set. There are two branches, one to estimate the average t-a-p model parameters and the other to estimate rater and subject parameters, which is only possible if rater identifiers are present.\n\n\nShow the code\nflowchart TB\n  A(raw_ratings) --&gt;|\"format_raw_ratings()\"|B(ratings)\n  B --&gt; |\"as_binary_ratings()\"|C(binary_ratings)\n  C --&gt; |\"as_counts()\"|D[counts]\n  D --&gt; |\"fit_counts()\"|F[params]\n  C --&gt; |\"fit_ratings()\"|E[rating_params]\n\n\n\n\n\n\nflowchart TB\n  A(raw_ratings) --&gt;|\"format_raw_ratings()\"|B(ratings)\n  B --&gt; |\"as_binary_ratings()\"|C(binary_ratings)\n  C --&gt; |\"as_counts()\"|D[counts]\n  D --&gt; |\"fit_counts()\"|F[params]\n  C --&gt; |\"fit_ratings()\"|E[rating_params]\n\n\n\n\nFigure 1: Data flow diagram for ratings\n\n\n\n\n\nRaw ratings can come in a variety of forms, so the first step is to standardize the data into binary ratings with integer subject identifiers and (optionally) rater identifiers. The format_raw_ratings() function is intended for this purpose. It assumes that ratings values are found in columns, meaning that there are one or more columns in the data that comprise only ratings and all the ratings. It can be that the ratings are in rows instead, in which case you’ll need to transpose the data before applying the formatting function.\nThe formatting function returns data frame in long form with the first column rater_id and the second rating. If there is a rater identifier, it is next as rater_id. There may be other columns of data used for explanatory purposes, which will follow. The data types for the columns are left unchanged.\nUsually we want to start analysis by estimating the three-parameter t-a-p model, which is computed from the rating counts of Class 1 ratings per subject. This is accomplished by applying the as_counts() function to the binary ratings and sending that to fit_counts(). If you have rater identifiers in the data you can fit the individual subject and rater parameters with fit_ratings() as shown in Figure 1."
  },
  {
    "objectID": "package.html#raw-ratings",
    "href": "package.html#raw-ratings",
    "title": "The tapModel Package",
    "section": "2.1 Raw ratings",
    "text": "2.1 Raw ratings\nThe information in a raw ratings data frame needs to be given structure to use the analytical functions. Each column of raw data will be one of:\n\nRating. This can be text or numerical, ordinal or categorical. For the basic t-a-p model, ratings are converted to binary before analysis. Note that ordinal ratings need to sort in the correct order. For example, a scale of “strongly agree” to “strongly disagree” won’t normally sort into the correct order, but can be fixed by appending a number, like “1 = strongly agree.” Or you can use a factor variable in R to arrange the values in order.\nSubject identifier. Each subject being rated needs a unique identifier or we can’t do a t-a-p analysis. Originally this identifier can be text or numerical, but it will be converted to an integer. Sometimes the subject is not explicitly identified, but each row of data corresponds to a unique subject. In this case, we can create an identifier by using the row number.\nRater identifier. It’s optional to have a rater identifier, but you should include it if you have one. Without identifying unique raters, we must assume that they have identical characteristics. This isn’t usually a good assumption if the raters are human. With rater identifiers, we can estimate accuracy and bias for each.\nCategories of ratings. A simple rating set may have only one column called “rating,” but a survey with five questions has five categories of response; a customer satisfaction survey might have “quality of service” and “timelines” included in the categories. If there’s more than one category, we usually want to filter to one at a time for analysis. For relationships between items (like a factor analysis or correlation) there are other tools besides t-a-p models to use. However, we could compare the reliability of the items by estimating the t-a-p parameters for each category.\nOther descriptive information. Information about the subject or rater that’s either used in analysis or there for annotation.\n\nA helper function format_data(raw_ratings) is provided to help with data conversion, but you may prefer to do that yourself, e.g. using dplyr data operations from the tidyverse."
  },
  {
    "objectID": "package.html#example-long-data-format",
    "href": "package.html#example-long-data-format",
    "title": "The tapModel Package",
    "section": "2.2 Example: Long data format",
    "text": "2.2 Example: Long data format\nThe simplest form of a rating data set has ratings in a single column, with other columns adding information.\n\n\nShow the code\nraw_ratings_long &lt;- data.frame(my_subject = c(\"subject10\",\n                                              \"subject21\",\n                                              \"subject14\", \n                                              \"subject3\"),\n                               my_rater = c(\"agent33\",\n                                            \"agent12\",\n                                            \"agent4\",\n                                            \"agent23\"),\n                               my_rating = c(\"pass\",\n                                             \"pass\",\n                                             \"incomplete\",\n                                             \"fail\"))\n\nraw_ratings_long |&gt; \n  kable()\n\n\n\n\n\nmy_subject\nmy_rater\nmy_rating\n\n\n\n\nsubject10\nagent33\npass\n\n\nsubject21\nagent12\npass\n\n\nsubject14\nagent4\nincomplete\n\n\nsubject3\nagent23\nfail"
  },
  {
    "objectID": "package.html#example-rows-as-subjects",
    "href": "package.html#example-rows-as-subjects",
    "title": "The tapModel Package",
    "section": "2.3 Example: Rows as subjects",
    "text": "2.3 Example: Rows as subjects\nThis is a simple data arrangement with identified raters, and one row of data per subject. Ratings are found in each column. This might be a portfolio review, with a pass/fail outcome.\n\n\nShow the code\nset.seed(123)\noutcomes &lt;- c(\"pass\",\"fail\")\n\nraw_ratings &lt;- data.frame(rater1 = sample(outcomes, 3\n                                          , prob = c(.8,.2), replace = TRUE), \n                          rater2 = sample(outcomes, 3, prob = c(.7, .3), replace = TRUE), \n                          rater3 = sample(outcomes, 3, prob = c(.75,.25), replace = TRUE))\n\nraw_ratings |&gt; kable()\n\n\n\n\n\nrater1\nrater2\nrater3\n\n\n\n\npass\nfail\npass\n\n\npass\nfail\nfail\n\n\npass\npass\npass\n\n\n\n\n\nThere is only one category of rating here, so we can call\n\n\nShow the code\nratings &lt;- format_raw_ratings(raw_ratings,\n                              rating_cols = c(\"rater1\",\"rater2\",\"rater3\"),\n                              rating_colnames_type = \"rater\",\n                              subject_id_col = NULL,\n                              rater_id_col = NULL,\n                              prune = FALSE)\n\nratings |&gt; \n  kable()\n\n\n\n\n\nsubject_id\nrating\nrater_id\n\n\n\n\n1\npass\nrater1\n\n\n2\npass\nrater1\n\n\n3\npass\nrater1\n\n\n1\nfail\nrater2\n\n\n2\nfail\nrater2\n\n\n3\npass\nrater2\n\n\n1\npass\nrater3\n\n\n2\nfail\nrater3\n\n\n3\npass\nrater3"
  },
  {
    "objectID": "package.html#binary-ratings",
    "href": "package.html#binary-ratings",
    "title": "The tapModel Package",
    "section": "2.4 Binary ratings",
    "text": "2.4 Binary ratings\nThe majority of functions in the tapModel package operate with binary data. If the rating scale is not originally binary, for example it’s a 1-4 scale, then before using binary functions, we have to make a choice of how to convert to binary.\n\n\nShow the code\nflowchart TB\n  A(raw_ratings) --&gt;|\"format_raw_ratings()\"|B(ratings)\n  B --&gt; |\"as_binary_ratings()\"|C(binary_ratings)\n\n\n\n\n\n\nflowchart TB\n  A(raw_ratings) --&gt;|\"format_raw_ratings()\"|B(ratings)\n  B --&gt; |\"as_binary_ratings()\"|C(binary_ratings)\n\n\n\n\nFigure 2: Data flow diagram for creating binary ratings\n\n\n\n\n\nThe conversion can be done by using the two functions in Figure 2, as illustrated using the wine data (Hodgson, 2008), with the choice that Class 1 will be ratings of 2,3, or 4, and Class 0 is the ratings of 1.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tapModel)\nlibrary(knitr)\n\n#' Load the included wine data\ndata(wine)\n\n#' It has rating in columns, one for each wine judge. Each row is a wine.\nratings &lt;- format_raw_ratings(wine,\n                              rating_cols = c(\"J1\",\"J2\",\"J3\",\"J4\"),\n                              rating_colnames_type = \"rater\",\n                              subject_id_col = NULL,\n                              rater_id_col = NULL,\n                              prune = FALSE)\n\n#' To compute the t-a-p model, we need the counts for each class. This is a \n#' dataframe with one row per subjec, with two columns:\n#'        N_r, the number of ratings for that subject\n#'        N_c, the number of ratings in class 1 for that subject\n#'        \n#' If we define Class 1 to be \"acceptable\" wine, with ratings {2,3,4} vs \n#' Class 0 = rating 1, we can find the counts with\nbinary_ratings &lt;- tapModel::as_binary_ratings(ratings, in_class = c(2,3,4)) \n\nbinary_ratings |&gt; \n  head(5) |&gt; \n  kable()\n\n\n\n\nTable 1\n\n\n\n\n\n\nsubject_id\nrating\nrater_id\n\n\n\n\n1\n1\nJ1\n\n\n2\n1\nJ1\n\n\n3\n1\nJ1\n\n\n4\n1\nJ1\n\n\n5\n1\nJ1\n\n\n\n\n\n\n\n\nNote that in this case, the column headers J1, J1, … correspond to judges, but that these are not unique identifiers for the judges. The rater_id column that’s automatically created by format_raw_ratings() should not be used as a rater ID. There’s no way to know this without knowing the source of the data.\nIn coding practice, I use ratings to mean binary_ratings most of the time. That’s what you’ll see in the other chapters of this site. To verify that a (binary) ratings data frame is correct, use verify_ratings(ratings), which will halt after flagging any problems, or return a zero."
  },
  {
    "objectID": "package.html#counts",
    "href": "package.html#counts",
    "title": "The tapModel Package",
    "section": "2.5 Counts",
    "text": "2.5 Counts\n\n\nShow the code\nflowchart TB\n  A(raw_ratings) --&gt;|\"format_raw_ratings()\"|B(ratings)\n  B --&gt; |\"as_binary_ratings()\"|C(binary_ratings)\n  C --&gt; |\"as_counts()\"|D[counts]\n  C --&gt; |\"as_count_index()\"|E[count_index]\n\n\n\n\n\n\nflowchart TB\n  A(raw_ratings) --&gt;|\"format_raw_ratings()\"|B(ratings)\n  B --&gt; |\"as_binary_ratings()\"|C(binary_ratings)\n  C --&gt; |\"as_counts()\"|D[counts]\n  C --&gt; |\"as_count_index()\"|E[count_index]\n\n\n\n\nFigure 3: Data flow diagram for creating binary ratings to create counts\n\n\n\n\n\nThere are two types of count data proceeding from binary ratings. If we want individual subject counts, we use as_count_index(). There’s a more compact way to represent the data if subject IDs are not needed, for which we use as_counts(). Continuing with the binary ratings we made from the wine data, the difference is illustrated below.\n\n\nShow the code\nbinary_ratings |&gt; \n  as_count_index() |&gt; \n  head(5) |&gt; \n  kable()\n\n\n\n\nTable 2: Count index showing number of raters and Class 1 ratings by subject.\n\n\n\n\n\n\nsubject_id\nN_r\nN_c\n\n\n\n\n1\n4\n4\n\n\n2\n4\n4\n\n\n3\n4\n4\n\n\n4\n4\n3\n\n\n5\n4\n2\n\n\n\n\n\n\n\n\nThe last shown row of Table 2 tells us that the fifth wine had four raters, two of whom rated it Class 1 (i.e. a rating above 1). If we don’t need to keep track of individual subjects, we can use as_counts() instead.\n\n\nShow the code\nbinary_ratings |&gt; \n  as_counts() |&gt; \n  kable()\n\n\n\n\nTable 3: Counts enumerating the number of instances of each combination of number of raters and Class 1 ratings\n\n\n\n\n\n\nN_r\nN_c\nn\n\n\n\n\n4\n0\n10\n\n\n4\n1\n15\n\n\n4\n2\n25\n\n\n4\n3\n43\n\n\n4\n4\n90\n\n\n\n\n\n\n\n\nThe first row of Table 3 tells us that there were \\(n=10\\) subjects where the four judges unanimously aggreed that the wine was Class 0 (a rating of 1 on the 1-4 scale). In 90 cases they unanimously agreed on Class 1 (rating of 2-4). It’s this more compact representation that gets used by the estimation algorithms fit_counts() or fit_counts_mcmc(), the latter of which uses a Bayesian estimation with Monte Carlo methods.\nTo verify that a counts data frame is correct, use verify_counts(counts), which will halt after flagging any problems, or return a zero."
  },
  {
    "objectID": "package.html#parameters",
    "href": "package.html#parameters",
    "title": "The tapModel Package",
    "section": "2.6 Parameters",
    "text": "2.6 Parameters\nThe purpose of the tapModel package is to estimate model parameters for a variety of models. The simplest type is a params named list or array of (usually) the three parameters \\(t\\), \\(a\\), and \\(p\\). Note that data frames are lists in R, so the following are all acceptable.\n\n\n\nTable 4: Types of admissible param objects, which contain parameters for the basic model.\n\n\n\nShow the code\nparams_1 &lt;- c(t = .5, a = .5, p = .5)\nverify_params(params_1)\n\nparams_2 &lt;- list(t = .5, a = .5, p = .5)\nverify_params(params_2)\n\nparams_3 &lt;- data.frame(t = .5, a = .5, p = .5)\nverify_params(params_3)\n\n\n\n\nThere is a variation of this that allows for a bifurcation of the accuracy and randomness parameters, so that \\(a\\) becomes \\((a_0, a_1)\\) and \\(p\\) becomes \\((p_0,p_1)\\). Currently these extended parameters are only used in a few other places, like simulating data. This will likely change over time as more features are added."
  },
  {
    "objectID": "package.html#rating-parameters",
    "href": "package.html#rating-parameters",
    "title": "The tapModel Package",
    "section": "2.7 Rating Parameters",
    "text": "2.7 Rating Parameters\nThe basic hierarchical model assigns each subject \\(i\\) a truth probability \\(t_i\\) and each rater \\(j\\) parameters \\(a_j\\) and \\(p_j\\). So each rating potentially corresponds to a unique combination of parameters, making it convenient to package all this information in one data frame. These objects are generically called rating_params, and are the main object of interest for much of the work we’ll do. Most functions are designed to work with a rating_params data frame.\nThere are three main ways to make one of these. First, we can fit a basic model and then “upscale” the results.\n\n\nShow the code\nratings &lt;- generate_sample_ratings()\n\nparams &lt;- ratings |&gt; \n          as_counts() |&gt; \n          fit_counts()\n\nrating_params &lt;- ratings |&gt; \n  as_rating_params(params)\n\nrating_params |&gt; \n  head(8) |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 5: Upscaling params to rating_params.\n\n\n\n\n\n\nsubject_id\nrating\nrater_id\nt\na\np\n\n\n\n\n1\n1\n1\n0.52\n0.67\n0.5\n\n\n1\n0\n2\n0.52\n0.67\n0.5\n\n\n1\n1\n3\n0.52\n0.67\n0.5\n\n\n1\n1\n4\n0.52\n0.67\n0.5\n\n\n1\n0\n5\n0.52\n0.67\n0.5\n\n\n2\n0\n1\n0.52\n0.67\n0.5\n\n\n2\n0\n2\n0.52\n0.67\n0.5\n\n\n2\n0\n3\n0.52\n0.67\n0.5\n\n\n\n\n\n\n\n\nNotice the uniformity of the parameters in Table 5, since we’re simply unpacking the average parameters into individual assignments. This can be useful as a starting point for estimating custom parameters, and can also be useful if we want to use functions that require rating_params like subject_calibration(rating_params).\nThe second way to get the rating_params() is to fit the ratings instead of counts.\n\n\nShow the code\nratings &lt;- generate_sample_ratings()\n\nrating_params &lt;- ratings |&gt; \n  fit_ratings()\n\nrating_params |&gt; \n  head(8) |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 6: Estimating individual parameters to get rating_params.\n\n\n\n\n\n\nsubject_id\nrating\nrater_id\nt\na\np\n\n\n\n\n1\n0\n1\n0.01\n0.67\n0.48\n\n\n1\n0\n2\n0.01\n0.64\n0.29\n\n\n1\n1\n3\n0.01\n0.74\n0.70\n\n\n1\n0\n4\n0.01\n0.69\n0.64\n\n\n1\n0\n5\n0.01\n0.71\n0.55\n\n\n2\n1\n1\n0.85\n0.67\n0.48\n\n\n2\n0\n2\n0.85\n0.64\n0.29\n\n\n2\n1\n3\n0.85\n0.74\n0.70\n\n\n\n\n\n\n\n\nNotice the variety of parameter estimates in Table 6, which is now representing a hierarchical model.\nThe third way to get rating_params is to generate random samples from a specification.\n\n\nShow the code\nratings &lt;- generate_ti_aj_pj_ratings(param_list = list(\n                                                    t = runif(100),\n                                                    a = runif(10),\n                                                    p = runif(10)))\n                                       \nrating_params |&gt; \n  head(8) |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 7: Generating a random rating_params from specified parameters.\n\n\n\n\n\n\nsubject_id\nrating\nrater_id\nt\na\np\n\n\n\n\n1\n0\n1\n0.01\n0.67\n0.48\n\n\n1\n0\n2\n0.01\n0.64\n0.29\n\n\n1\n1\n3\n0.01\n0.74\n0.70\n\n\n1\n0\n4\n0.01\n0.69\n0.64\n\n\n1\n0\n5\n0.01\n0.71\n0.55\n\n\n2\n1\n1\n0.85\n0.67\n0.48\n\n\n2\n0\n2\n0.85\n0.64\n0.29\n\n\n2\n1\n3\n0.85\n0.74\n0.70\n\n\n\n\n\n\n\n\nInstead of providing a parameter list, you can also just provide an existing rating_params data frame using generate_ti_aj_pj_ratings(rating_params = my_rating_params) , and samples will be drawn from the parameters for subjects and raters. This is used, for example, to see if we can recover parameters by fitting the simulated ratings and matching to the original parameters."
  },
  {
    "objectID": "package.html#sample-mcmc-code",
    "href": "package.html#sample-mcmc-code",
    "title": "The tapModel Package",
    "section": "4.1 Sample MCMC Code",
    "text": "4.1 Sample MCMC Code\nWe can use the counts data frame from the introduction to illustrate the code for generating the basic t-a-p model using Bayesian MCMC methods.\n\n\nShow the code\n# ratings with t = .5, a = .7, p = .5\ncounts &lt;- generate_sample_ratings() |&gt; as_counts()\n\n#' Use the convenience function to run the MCMC for the three-parameter-model\nmcmc_output &lt;- tapModel::fit_counts_mcmc(counts, quiet = TRUE) \n\n#' Get the summary \nmcmc_params &lt;- mcmc_output$params\n\nmcmc_params |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 8: Estimating the t-a-p model with Bayesian MCMC methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nposition\nvar\navg\np05\np25\nmedian\np75\np95\nmode1\nmode2\nsd\n\n\n\n\n1\nlp__\n-172.39\n-174.82\n-172.95\n-172.08\n-171.46\n-171.05\n-171.39\nNA\n1.25\n\n\n2\na\n0.72\n0.66\n0.70\n0.72\n0.74\n0.78\n0.73\nNA\n0.03\n\n\n3\np\n0.58\n0.45\n0.53\n0.58\n0.63\n0.70\n0.60\nNA\n0.07\n\n\n4\nt\n0.58\n0.49\n0.54\n0.58\n0.61\n0.66\n0.58\nNA\n0.05\n\n\n\n\n\n\n\n\nNotice that the parameter estimates are given in a richer format that the basic params data, e.g. from fit_counts(counts). It returns a data frame with the mean and interquartile range of the draws, as well as an attempt to identify bimodal densities, which can indicate a problem with model fit or non-unique solutions. We can convert to the basic format by picking which estimate (e.g. average, median, or mode) to use. This is demonstrated in the code block below.\n\n\nShow the code\nparams &lt;- mcmc_params |&gt; \n  filter(var %in% c(\"t\",\"a\",\"p\")) |&gt; \n  select(var, avg) |&gt; # or median, or mode1 etc\n  spread(var, avg)\n\nparams |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 9: Converting to a params format using the average.\n\n\n\n\n\n\na\np\nt\n\n\n\n\n0.72\n0.58\n0.58\n\n\n\n\n\n\n\n\nOne advantage of the MCMC approach is the ability to examine draw densities. These can be visualized as follows.\n\n\nShow the code\nmcmc_output$fitted_model$draws() |&gt; \n  plot_densities_mcmc()\n\n\n\n\n\n\n\n\nFigure 4: Parameter estimate densities from MCMC draws. Shading shows 5% to 95%, the inter-quartile range, and the mean estimate. The dotted lines show the scaled likelihood.\n\n\n\n\n\nNote that to get the draws, we need to call the draws() function on the fitted model. The parentheses are required, since it’s a function call, not a data object."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This chapter gives a brief background on rater agreement with a focus on the goals of the project and how legacy methods fall short. It contrasts psychological measurement methods to machine learning algorithms. A general model is introduced that combines the best features of both of those cultures, and an example is given.\n\n\nHumans often use consensus to assess knowledge. In the introduction to an article on this topic Dawid & Skene (1979) describe problems in using such consensus.\n\nWhen a patient’s historyis taken by different clinicians, different replies may be obtained to the same question. This may occur for a number of reasons; perhaps slightly different wording is used in each case, or perhaps the question is one the patient finds difficult to answer satisfactorily and so changes his reply from time to time. Similarly, in classifying a facet (sign or symptom) for type, severity, extent or duration, the patient and the clinicians may have different interpretations of the underlying scale of measurement. Such facets are said to be subject to observer error in that the response recorded may not be the “true” response as defined by some standard description of the facet or as implied by a consensus of medical opinion.\n\nWe might seek a second opinion for a diagnosis, ask around for restaurant recommendations, or look for online reviews of a product. Intiuitively, we put more weight on opinions that have more agreement. This chapter describes how agreement or disagreement can be quantified. As in the quote above, we may have an idea of metaphysical truth that is obscured by error in the reported observations.\nIn behavioral science, the focus of these statistics is on human observers whom we’ll call “raters.” Raters independently assign “ratings” to “subjects.” This describes a familiar case like a product review with a five-star rating system (although those ratings may not be independent). However, the same idea applies to any categorization task, like classifying images of cats and dogs, or diagnosing diseases from medical images. A fascinating example comes from an article in The Atlantic Magazine, here describing an analysis of a very old text called the Voynich manuscript.\n\nDavis magnified the handwriting, she noticed subtle variations. In certain places, the script was more cramped or more likely to slant as it crossed the page. She tested this observation by picking a letter that didn’t appear often and tracking it across the manuscript’s pages. Its style, she saw, varied among groups of pages but not within those groups. This suggested that the differences—larger or smaller loops, straighter or curvier crossbars, longer or shorter feet—were the product of different scribes rather than of one scribe writing the same letter in different ways.\n\nSee Davis’s paper for details1. For our purposes, the “raters” are scholars who categorize writing styles, the “subjects” are sections (folios) of the manuscript they are reviewing, and the presumed different scribes are the categories we generically refer to as “ratings.”\n\n\n\nTable 1: Selected rows from a table at the website for the manuscript: https://voynich.nu/index.html, comparing an early attempt to classify writing “hands” (Currier) to the recent classification (Davis). Hypothesized identity is unique to each column with the numbers denoting individual scribes.\n\n\n\n\n\nFolio(s)\nPrescott Currier\nLisa Fagin Davis\n\n\n\n\nf41, f48, f57r\n2\n5\n\n\nfRos (obverse)\n3\n2\n\n\nfRos (main)\n3\n4\n\n\nf87, f88, f93, 96\n4\n1\n\n\nf94, f95\n5\n3\n\n\nf103, f104, f106\nX\n3\n\n\nf105\nY\n3\n\n\n\n\n\n\nIn Table 1 it’s recorded that Currier thought that the obverse and main folio fRos were in a single hand, whereas Davis has two scribes. Conversely, Davis has the last three rows as the same author while Currier has three. This is an example of how science works toward consensus by placing observations into categories. Agreements are good for pointing to what might be real, and disagreements are for honing or discarding theories.\nThe statistical question for data like this is to compare variation within groups to variation between groups. One application is an idea called “signal-to-noise ratio,” which is a measure of how much of the variation in the data is due to the thing we’re interested in (signal) versus how much is due to random variation (noise). In the manuscript example, the signal is the commonality of handwriting within contiguous sections, and the noise is variation within each section. Reliability of measures is the same idea in different clothing, where we might formulate reliability as between-subject variation divided by total variation (Shrout & Fleiss, 1979). As an example, college grade averages (GPA) can have a reliability of about 0.8, meaning that 80% of the variation in grades is due to differences between students, and 20% is due to variation in how individual students earn grades. This makes GPA useful as a statistical measure, for example in predicting career outcomes.\nThe signal-to-noise approach usually treats a rating as an ordinary number (scalar), so a 1-5 rating scale is placed on the number line as a measure. If we start from the idea of rater agreement, however, this number-line approach is not as useful as counting agreement on categories. In the development below, we’ll be concerned with ratings as categories, and usually binary choices between some Class 1 or Class 0, e.g. “Does this patient have Covid?”. It is useful to start with the idea of a “true” classification.\n\n\n\nIt turns out that the questions “what is true?” and “what is knowledge, anyway?” are difficult to answer. There is a great body of work in epistemology, the philosophical study of knowledge, from which we can take a clever idea to get started.\nTo start, we have to imagine that “truth” exists. Since we are talking about classifications of subjects into one of two categories, we must commit to the idea that these categories (words that humans made up) might correspond to something in objective reality (whatever that is). We can easily make up descriptions that are fantasies, like unicorns or flying whales, and the goal is to have a way to distinguish these phantasms from real things like penguins and taxes. To do so, we have to imagine that by some mysterious means, our intellects and discourse can access Nature and name things.\nWords are slippery things, however. Although there are definitions, the real meaning of “pizza” is (ironically) ineffable; one could write volumes on the subject. Let’s take Platos’s definition of a man as a featherless biped. Our goal is a set of classifications “man” or “not man” generated from independent observers (we call them raters) who each inspect some objects in the world (we call them subjects) and assign “man” or “not man” to each. Then we’ll take that data and apply the between- and within-variance idea.\nTherefore the definition isn’t enough. We also need a procedure for generating the data the way we want. To ensure independence of raters, we don’t want them to collaborate on decisions, for example. In the sciences, procedures can be quite elaborate, as in detecting and classifying Higgs bosons, which requires a huge particle accelerator.\nThe story is that Diogneses the Cynic tossed a plucked chicken at Plato’s feet and declared “here’s your man.” The example drives a wedge between the definition “featherless biped” and what is actually meant by “man.” Outside of math and science, definitions and procedures are fuzzy in this way. Rater agreement statistics attempt to defuzz the ratings, to understand the reality behind them.\nHere’s where the idea from epistemology is key, because it gets to the nature of the fuzziness of classification. First we define knowledge as justified true belief (JTB), and then sort out where the fuzziness is.\nThere are three parts to JTB, and we already talked about truth; as a prerequisite we must believe that true statements can be made about the world, which means accessing something real and describing it. The third piece of JTB is belief, which we’ll take to be the classification assigned by a rater. If our rater looks at a horse and writes down “not unicorn,” we’ll assume that the classification represents what the rater believes. This runs into problems if we want to consider adversarial raters, like someone paid to rate consumer products, or an angry customer rating a service poorly for extraneous reasons. We won’t consider those possibilities here.\nThe first piece of JTB is where the fuzziness lives: justification. Outside of math, justification in words isn’t complete2. In the case of Diogenes versus Plato, an follower of the latter might reason that the plucked chicken meets the great philosopher’s definition, and therefore is a man. Others might use common sense to conclude that it’s not a man. There is more than one way to justify belief, but not all of them result in true classifications.\nThe JTB idea was dealt a blow by Gettier, who posed a number of examples where justification goes awry, but the idea is much older. Consider this example from Dharmottara in 779 AD3.\n\nA desert traveller is searching for water. He sees, in the valley ahead, a shimmering blue expanse. Unfortunately, it’s a mirage. But fortunately, when he reaches the spot where there appeared to be water, there actually is water, hidden under a rock. Did the traveller know, as he stood on the hilltop hallucinating, that there was water ahead?\n\nThe traveller had a belief that turned out to be true, but the justification was incorrect. The philosphers are concerned with whether or not this constitutes knowledge4, but we’re more interested in something simpler. That is to categorize the ways truth and belief can happen. If they don’t coincide (if we believe something false), then we’ll assume that the justification was incorrect. As the Gettier problems and the discussion they generated demonstrate, there are a lot of complicated ways in which justification can go awry.\nIn the literature of rater agreement, one can find similar language. In Aickin (1990), we find “The \\(\\alpha\\) agreement parameter is defined as the proportion of a population of items that are classified identically ‘for cause’ by two classifiers, the remaining items being classified at random […].” The author suggests that some subjects are more difficult to rate than others, which hints at modeling each subject’s probability of being in a category, in truth. This idea was explored in Dawid & Skene (1979), who also allowed that raters might have different proficiencies.\nAnother instance comes from the derivation of a rater agreement statistic called AC1 Gwet (2008). The author notes that “the assumption [is] that any rating that is not random will automatically lead to a correct classification, while a random rating leads to a correct classification with probability 1/2.” The idea is that we conceptually separate accurate ratings from random ones, which then means we must consider the probability distribution of the random ones. The random assignments will sometimes be correct, but for the wrong reason–just like the Gettier problems.\n\n\n\nDiagnosing illness is an important classification problem, and the simplest version is a blood test that returns a positive or negative result, perhaps scanning for an antibody. There are four possibilities, which can be organized into what is commonly called a confusion matrix. Suppose that we consider a statement S about the world, like “this patient has the flu.” Using a definition and process we reach a conclusion, so that we believe S to be true or false. This generates the classifications we’re interested in: this is what the raters do, generating lots of S or not-S data on cases.\nThe truth is often inaccessible, but recall that we assume that it exists, so that the statement really is true or false: each patient really does have the flu or does not. Here are the four possibilities:\n\n\n\nTable 2: Sample confusion matrix\n\n\n\n\n\n\n\n\n\n\n\nS is true\nS is false\n\n\n\n\nBelieve S is true\nTrue positive cases\nFalse positive cases\n\n\nBelieve S is false.\nFalse negative cases\nTrue negative cases\n\n\n\n\n\n\nNotice that there are four spots in the table where data can go: if we had perfect knowledge we could count the number of true positive cases, etc. If we converted these numbers to proportions by dividing by the table, we have three parameters left. This foreshadows what comes later: the heart of the classification problem (in its simple form) is a three-parameter problem5.\nThe limitation of casting the confusion matrix into a single-parameter statistic was noted by Cicchetti & Feinstein (1990), who suggested “two separate indexes of proportionate agreement in the observers’ positive and negative decisions. […] Why do we want a single index?” Another example comes from the motivation for the AC1 rater statistic Gwet (2008). It separately considers rater true positives and true negatives, but then assumes that these are identical for each rater, so that the result is a single parameter.\nAs a concrete illustration, consider the wine judging data used in Hodgson (2008) (data from the author in personal communication). The first five rows look like this:\n\n\n\nTable 3: Wine ratings showing rows 1-5 of 183, taken from the study by Hodgson (personal communication).\n\n\n\n\n\nWine\nJ1\nJ2\nJ3\nJ4\n\n\n\n\n1\n3\n3\n3\n3\n\n\n2\n3\n3\n3\n3\n\n\n3\n3\n3\n2\n4\n\n\n4\n3\n4\n3\n1\n\n\n5\n4\n2\n1\n1\n\n\n\n\n\n\nThe 1-4 scale is ordinal, meaning that we imagine quality to increase with the numerical value. For this discussion, we will reduce the scale to a binary classification: acceptable wine (ratings 2-4) or wine to avoid (rating 1). This simplifies the table to.\n\n\n\nTable 4: Simplified wine ratings, where 1 means “acceptable” and 0 means “avoid.”\n\n\n\n\n\nWine\nJ1\nJ2\nJ3\nJ4\n\n\n\n\n1\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n1\n\n\n3\n1\n1\n0\n1\n\n\n4\n1\n1\n1\n0\n\n\n5\n1\n0\n0\n0\n\n\n\n\n\n\nIntuitively there is more reason to believe wines 1 and 2 are acceptable than with 3 or 4, for which one judge found them not metal-worthy. It seems safe to avoid wine 5, since all but one of the judges found it not to be metal-worthy.\nWe can put this information into the confusion matrix, but we’re missing information.\n\n\n\nTable 5: Confusion matrix for the first five wines, with missing information as question marks.\n\n\n\n\n\n\ntrue value of 2-4\ntrue value of 1\nTotal\n\n\n\n\nrating of 2-4\n?\n?\n15\n\n\nrating of 1\n?\n?\n5\n\n\n\n\n\n\nThere is some awkwardness in imagining that the physical universe encompasses a reality including true wine ratings. It seems absurd on the face of it that the physical laws of the universe imply “wine science.” It’s useful to think of it in reverse: this reification of the ratings is a hypothesis that may be rejected by the statistics we derive from the data. If we find that raters are assigning ratings at random, there’s no evidence here for the reality of wine asthetics. On the other hand, strong indications of non-randomness need to be explained somehow.\nTaking the leap that all non-randomness is causal, and physical reality has a monopoly on causality, then something concrete in the world is behind the rating consistency. We could imagine the chemical similarities and differences of wines and their effects on human anatomy, and so on. Being “real” in this case doesn’t mean that the universe appreciates wine, only that the universe includes physical blobs called humans, and they have mechanical consistencies that correlate observations in this case. The confusion matrix is also used for calculating the causal effect of an experiment. In a medical context, the rows might be treatment categories and the columns patient outcomes. See Eubanks (2014) for more on that.\nBy using the rating counts for each subject instead of the total, we can–surprisingly–estimate the whole confusion matrix. We can even estimate the true classification values for each subject. For that level of detail see Chapter 4: Hierarchical Models.\n\n\n\nSignal and noise as variability between and variability within subjects translates into rater agreement within subjects versus between subjects. The usual way to turn this idea into a number is to count the number of actual agreements and divide by the total number possible.\n\n\n\nTable 6: Rater agreements count, showing the maximum agreements for N raters, the number matches that are evidence versus logical implications and the ratio of evidence to matches.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of raters\nMaximum agreements\nEvidentiary agreements\nImplied agreements\nEvidence / Maximum\n\n\n\n\n1\n0\n-\n-\n-\n\n\n2\n1\n1\n0\n1\n\n\n3\n3\n2\n1\n.67\n\n\n4\n6\n3\n3\n.5\n\n\n5\n10\n4\n6\n.4\n\n\nN\n\\(N(N-1)/2\\)\n\\(N-1\\)\n\\(N(N-1)/2 - N + 1\\)\n\\(2/N\\)\n\n\n\n\n\n\nThe maximum agreements in the table come from the combinatorics formula “N choose 2,” counting the number of possible pairs out of N. If a group of 5 people meet and all shake hands, it’s 10 handshakes. The number of pairs grow much more rapidly than the number of raters does6.\nThere’s a rule in thumb in statistics that it’s preferable to use standard deviation instead of variance when understanding the data. That’s because the units of standard deviation are the same as the original data, whereas the variance has squared units. The same effect is going on with counting agreement. In some sense, what we want looks more like the square root of agreement.\nIntuitively, three raters agreeing doesn’t seem like three times as much evidence as two agreeing. And if we look at where those agreements are coming from, most of them are purely logical. Agreement is transitive, meaning if A agrees with B and B agrees with C, then A agrees with C. This is a logical necessity, and it’s not evidence of anything. The Evidentiary and Implied columns of the table record the distinction between the two types of matches. As the number of raters \\(N\\) increases, the logically necessary agreements greatly outnumber the evidentiary ones, so the ratio in the last column of the table decreases toward zero.\nIf we only counted the evidentiary agreements, the maximum for \\(N\\) raters would be \\(N - 1\\), which is roughly proportional to the square root of the total number of agreements. We’ll come back to that idea later. However, the well-trodden path is to estimate the probability of agreement over all the subjects, which leads to the kappa statistics.\nFor the five wine ratings that are reduced to the binary category “avoid” or “don’t avoid” in Table 4, we can calculate the agreement as follows.\n\n\n\nTable 7: Agreement calculation for the first five wines, showing the maximum possible agreements, the actual agreements, and the agreement proportion out of the maximum.\n\n\n\n\n\nWine\nRaters\nPossible\nActual\nAgreement\n\n\n\n\n1\n4\n6\n6\n1\n\n\n2\n4\n6\n6\n1\n\n\n3\n4\n6\n3\n.5\n\n\n4\n4\n6\n3\n.5\n\n\n5\n4\n6\n3\n.5\n\n\n\n\n\n\nFor the fifth vintage in Table 7, there are more matches with the zeros than there are with the ones; both are counted. It is at this point what seemed like a good starting point begins to turn into a muddle, because since both the zeros and the ones can create agreement, how do we know what’s good enough agreement? What’s the worst agreement possible?\nA group of \\(n_1\\) raters who agree on the 1 ratings produces a number of agreements about proportional to \\(n_1^2\\), and similarly the \\(n_0\\) raters of 0 agreements produce about \\(n_0^2\\) agreements. Together that’s around \\(n_1^2 + n_0^2\\) agreements, which is less than or equal to \\((n_1 + n_0)^2\\), which is about the number of agreements if everyone agreed on a single category. So we get more agreement when everyone agrees than we do when the raters split into groups. That means that the least agreement happens when the raters are evenly split, giving us a floor for agreement.\n\n\n\nTable 8: Minimum rater agreement rates\n\n\n\n\n\n\n\n\n\n\n\nNumber of raters\nMaximum agreements\nMinimum agreements\nMinimum rate\n\n\n\n\n4\n6\n2\n.33\n\n\n6\n15\n6\n.40\n\n\n8\n28\n12\n.43\n\n\n10\n45\n20\n.44\n\n\neven N\n\\(N(N-1)/2\\)\n\\(N(N/2-1)/2\\)\n\\((N/2-1)/(N - )\\)\n\n\n\n\n\n\nFor six or more raters, there’s at least a 40% agreement rate, even when there’s the least possible amount of agreement. It’s annoying that the minimum rate of agreement changes with the number of raters. For real cases when we want to use these ideas, we might have different numbers of raters for different subjects, complicating an average minimum rate, which defines the worst case agreement.\n\n\n\nWe saw in the previous section that the minimum frequency of agreement among raters is quite large, tending toward one half of the maximum possible. Whatever statistic we create as a measure of agreement needs to take that into account if we are to make sense of the results. We’ll get maximum agreement if all the raters choose the same category (of the two choices), and we’ll get minimum agreement if they split evenly between the categories, as if they each flipped a coin to decide. So there’s an association between worst-case random raters and the minimum agreement rate. This is the basis of the chance correction in the S statistic, which assumes coin-flipping raters as the worst-case benchmark from which to measure agreement.\nThe general formula for taking into account a worst-case (random) match rate \\(m_r\\) first computes the practical range of our scale, which is \\(1 - m_r\\), since the statistic can’t be less than \\(m_r\\). The amount of agreement observed \\(m_o\\) is then pinned to this scale as a “chance-corrected” match rate \\(\\kappa\\) (kappa) with\n\\[\\kappa = \\frac{m_o - m_r}{1 - m_r}\\]\nThe S statistic is a special case of this formula, where \\(m_r = .5\\). The sample of wine ratings can be used to illustrate. Recall that earlier we simplified the judges’ scale of “no medal” to “gold medal” to a binary classification between “no medal” (coded as zero) or “any medal” (coded as one). There were 30 maximum agreements among the four raters over the five wines (six per wine), and 21 actual agreements. The minimum is when the ratings are evenly split on each case, generating two agreements for each wine, or 10 total. An S-like statistic would be calculated like this:\n\\[\n\\begin{aligned}\nm_o &= 21/30 = .70 \\\\\nm_r &= 10/30 = .33 \\\\\n\\kappa_s &= \\frac{.70 - .33}{1 - .33} = .47\n\\end{aligned}\n\\] The kappa is now on a scale of zero (worse) to one (perfect agreement). Interpreting what the value means is difficult because it’s on the wrong scale as noted above. A value close to one half sounds mediocre, but not terrible, but we’re really looking for something like the square root of the agreement rate, which is .69. That value corresponds better to the intuition that the agreement in the table looks pretty good.\nThe coin-flipping assumption for randomness can be replaced with other assumptions. This has the effect of increasing the assumed worst-case agreement rate and hence decreasing kappa values. Perhaps the most popular choice is to reason that the raters are more likely to randomly pick the most common category so the coin should be weighted according to the rating distribution.\nFor example, the table of simplified wine ratings has 15 ratings of 1 and 5 of 0, or 75% 1-ratings. The worst case match rate is then the product of the proportions of 1 ratings, or \\(m_r = (.75)(.75) + (.25)(.25) = .625\\). This is the assumption of the Fleiss kappa. Note that the actual worst case match rate is still close to one half (because of the math), so it’s possible to get a negative Fleiss kappa if the assumption about the distribution of randomness fails.\nOne calculation under this proportionality assumption is:\n\\[\n\\begin{aligned}\nm_o &= 21/30 = .70 \\\\\nm_r &= .625 \\\\\n\\kappa_f &= \\frac{.70 - .625}{1 - .625} = .20\n\\end{aligned}\n\\]The actual calculations of these statistics are complicated by the choice of using exact combinatorical counts or long-run estimates. This distinction need not concern us in here.\nThe deflation of the rater agreement statistic in the Fleiss calculation above makes it clear that assumptions about random agreement are critical. A researcher may justifiably wonder which value is correct: are the wine judges in pretty good agreement, or is the judging mostly random? For historical reasons, rather than treating rater disagreement as a parameter to be discovered, the field has evolved to propose many variations of the kappa formula with different assumptions about the worst-case agreement rate.\nA limitation of most of the kappa statistics is that they make the simplifying assumption that a pair of raters reaches agreement randomly if both raters are random. This omits the possibility that one rater is accurate and the other isn’t."
  },
  {
    "objectID": "introduction.html#sec-intro-overview",
    "href": "introduction.html#sec-intro-overview",
    "title": "Introduction",
    "section": "",
    "text": "Humans often use consensus to assess knowledge. In the introduction to an article on this topic Dawid & Skene (1979) describe problems in using such consensus.\n\nWhen a patient’s historyis taken by different clinicians, different replies may be obtained to the same question. This may occur for a number of reasons; perhaps slightly different wording is used in each case, or perhaps the question is one the patient finds difficult to answer satisfactorily and so changes his reply from time to time. Similarly, in classifying a facet (sign or symptom) for type, severity, extent or duration, the patient and the clinicians may have different interpretations of the underlying scale of measurement. Such facets are said to be subject to observer error in that the response recorded may not be the “true” response as defined by some standard description of the facet or as implied by a consensus of medical opinion.\n\nWe might seek a second opinion for a diagnosis, ask around for restaurant recommendations, or look for online reviews of a product. Intiuitively, we put more weight on opinions that have more agreement. This chapter describes how agreement or disagreement can be quantified. As in the quote above, we may have an idea of metaphysical truth that is obscured by error in the reported observations.\nIn behavioral science, the focus of these statistics is on human observers whom we’ll call “raters.” Raters independently assign “ratings” to “subjects.” This describes a familiar case like a product review with a five-star rating system (although those ratings may not be independent). However, the same idea applies to any categorization task, like classifying images of cats and dogs, or diagnosing diseases from medical images. A fascinating example comes from an article in The Atlantic Magazine, here describing an analysis of a very old text called the Voynich manuscript.\n\nDavis magnified the handwriting, she noticed subtle variations. In certain places, the script was more cramped or more likely to slant as it crossed the page. She tested this observation by picking a letter that didn’t appear often and tracking it across the manuscript’s pages. Its style, she saw, varied among groups of pages but not within those groups. This suggested that the differences—larger or smaller loops, straighter or curvier crossbars, longer or shorter feet—were the product of different scribes rather than of one scribe writing the same letter in different ways.\n\nSee Davis’s paper for details1. For our purposes, the “raters” are scholars who categorize writing styles, the “subjects” are sections (folios) of the manuscript they are reviewing, and the presumed different scribes are the categories we generically refer to as “ratings.”\n\n\n\nTable 1: Selected rows from a table at the website for the manuscript: https://voynich.nu/index.html, comparing an early attempt to classify writing “hands” (Currier) to the recent classification (Davis). Hypothesized identity is unique to each column with the numbers denoting individual scribes.\n\n\n\n\n\nFolio(s)\nPrescott Currier\nLisa Fagin Davis\n\n\n\n\nf41, f48, f57r\n2\n5\n\n\nfRos (obverse)\n3\n2\n\n\nfRos (main)\n3\n4\n\n\nf87, f88, f93, 96\n4\n1\n\n\nf94, f95\n5\n3\n\n\nf103, f104, f106\nX\n3\n\n\nf105\nY\n3\n\n\n\n\n\n\nIn Table 1 it’s recorded that Currier thought that the obverse and main folio fRos were in a single hand, whereas Davis has two scribes. Conversely, Davis has the last three rows as the same author while Currier has three. This is an example of how science works toward consensus by placing observations into categories. Agreements are good for pointing to what might be real, and disagreements are for honing or discarding theories.\nThe statistical question for data like this is to compare variation within groups to variation between groups. One application is an idea called “signal-to-noise ratio,” which is a measure of how much of the variation in the data is due to the thing we’re interested in (signal) versus how much is due to random variation (noise). In the manuscript example, the signal is the commonality of handwriting within contiguous sections, and the noise is variation within each section. Reliability of measures is the same idea in different clothing, where we might formulate reliability as between-subject variation divided by total variation (Shrout & Fleiss, 1979). As an example, college grade averages (GPA) can have a reliability of about 0.8, meaning that 80% of the variation in grades is due to differences between students, and 20% is due to variation in how individual students earn grades. This makes GPA useful as a statistical measure, for example in predicting career outcomes.\nThe signal-to-noise approach usually treats a rating as an ordinary number (scalar), so a 1-5 rating scale is placed on the number line as a measure. If we start from the idea of rater agreement, however, this number-line approach is not as useful as counting agreement on categories. In the development below, we’ll be concerned with ratings as categories, and usually binary choices between some Class 1 or Class 0, e.g. “Does this patient have Covid?”. It is useful to start with the idea of a “true” classification."
  },
  {
    "objectID": "introduction.html#sec-intro-knowledge",
    "href": "introduction.html#sec-intro-knowledge",
    "title": "Introduction",
    "section": "",
    "text": "It turns out that the questions “what is true?” and “what is knowledge, anyway?” are difficult to answer. There is a great body of work in epistemology, the philosophical study of knowledge, from which we can take a clever idea to get started.\nTo start, we have to imagine that “truth” exists. Since we are talking about classifications of subjects into one of two categories, we must commit to the idea that these categories (words that humans made up) might correspond to something in objective reality (whatever that is). We can easily make up descriptions that are fantasies, like unicorns or flying whales, and the goal is to have a way to distinguish these phantasms from real things like penguins and taxes. To do so, we have to imagine that by some mysterious means, our intellects and discourse can access Nature and name things.\nWords are slippery things, however. Although there are definitions, the real meaning of “pizza” is (ironically) ineffable; one could write volumes on the subject. Let’s take Platos’s definition of a man as a featherless biped. Our goal is a set of classifications “man” or “not man” generated from independent observers (we call them raters) who each inspect some objects in the world (we call them subjects) and assign “man” or “not man” to each. Then we’ll take that data and apply the between- and within-variance idea.\nTherefore the definition isn’t enough. We also need a procedure for generating the data the way we want. To ensure independence of raters, we don’t want them to collaborate on decisions, for example. In the sciences, procedures can be quite elaborate, as in detecting and classifying Higgs bosons, which requires a huge particle accelerator.\nThe story is that Diogneses the Cynic tossed a plucked chicken at Plato’s feet and declared “here’s your man.” The example drives a wedge between the definition “featherless biped” and what is actually meant by “man.” Outside of math and science, definitions and procedures are fuzzy in this way. Rater agreement statistics attempt to defuzz the ratings, to understand the reality behind them.\nHere’s where the idea from epistemology is key, because it gets to the nature of the fuzziness of classification. First we define knowledge as justified true belief (JTB), and then sort out where the fuzziness is.\nThere are three parts to JTB, and we already talked about truth; as a prerequisite we must believe that true statements can be made about the world, which means accessing something real and describing it. The third piece of JTB is belief, which we’ll take to be the classification assigned by a rater. If our rater looks at a horse and writes down “not unicorn,” we’ll assume that the classification represents what the rater believes. This runs into problems if we want to consider adversarial raters, like someone paid to rate consumer products, or an angry customer rating a service poorly for extraneous reasons. We won’t consider those possibilities here.\nThe first piece of JTB is where the fuzziness lives: justification. Outside of math, justification in words isn’t complete2. In the case of Diogenes versus Plato, an follower of the latter might reason that the plucked chicken meets the great philosopher’s definition, and therefore is a man. Others might use common sense to conclude that it’s not a man. There is more than one way to justify belief, but not all of them result in true classifications.\nThe JTB idea was dealt a blow by Gettier, who posed a number of examples where justification goes awry, but the idea is much older. Consider this example from Dharmottara in 779 AD3.\n\nA desert traveller is searching for water. He sees, in the valley ahead, a shimmering blue expanse. Unfortunately, it’s a mirage. But fortunately, when he reaches the spot where there appeared to be water, there actually is water, hidden under a rock. Did the traveller know, as he stood on the hilltop hallucinating, that there was water ahead?\n\nThe traveller had a belief that turned out to be true, but the justification was incorrect. The philosphers are concerned with whether or not this constitutes knowledge4, but we’re more interested in something simpler. That is to categorize the ways truth and belief can happen. If they don’t coincide (if we believe something false), then we’ll assume that the justification was incorrect. As the Gettier problems and the discussion they generated demonstrate, there are a lot of complicated ways in which justification can go awry.\nIn the literature of rater agreement, one can find similar language. In Aickin (1990), we find “The \\(\\alpha\\) agreement parameter is defined as the proportion of a population of items that are classified identically ‘for cause’ by two classifiers, the remaining items being classified at random […].” The author suggests that some subjects are more difficult to rate than others, which hints at modeling each subject’s probability of being in a category, in truth. This idea was explored in Dawid & Skene (1979), who also allowed that raters might have different proficiencies.\nAnother instance comes from the derivation of a rater agreement statistic called AC1 Gwet (2008). The author notes that “the assumption [is] that any rating that is not random will automatically lead to a correct classification, while a random rating leads to a correct classification with probability 1/2.” The idea is that we conceptually separate accurate ratings from random ones, which then means we must consider the probability distribution of the random ones. The random assignments will sometimes be correct, but for the wrong reason–just like the Gettier problems."
  },
  {
    "objectID": "introduction.html#sec-intro-confusion",
    "href": "introduction.html#sec-intro-confusion",
    "title": "Introduction",
    "section": "",
    "text": "Diagnosing illness is an important classification problem, and the simplest version is a blood test that returns a positive or negative result, perhaps scanning for an antibody. There are four possibilities, which can be organized into what is commonly called a confusion matrix. Suppose that we consider a statement S about the world, like “this patient has the flu.” Using a definition and process we reach a conclusion, so that we believe S to be true or false. This generates the classifications we’re interested in: this is what the raters do, generating lots of S or not-S data on cases.\nThe truth is often inaccessible, but recall that we assume that it exists, so that the statement really is true or false: each patient really does have the flu or does not. Here are the four possibilities:\n\n\n\nTable 2: Sample confusion matrix\n\n\n\n\n\n\n\n\n\n\n\nS is true\nS is false\n\n\n\n\nBelieve S is true\nTrue positive cases\nFalse positive cases\n\n\nBelieve S is false.\nFalse negative cases\nTrue negative cases\n\n\n\n\n\n\nNotice that there are four spots in the table where data can go: if we had perfect knowledge we could count the number of true positive cases, etc. If we converted these numbers to proportions by dividing by the table, we have three parameters left. This foreshadows what comes later: the heart of the classification problem (in its simple form) is a three-parameter problem5.\nThe limitation of casting the confusion matrix into a single-parameter statistic was noted by Cicchetti & Feinstein (1990), who suggested “two separate indexes of proportionate agreement in the observers’ positive and negative decisions. […] Why do we want a single index?” Another example comes from the motivation for the AC1 rater statistic Gwet (2008). It separately considers rater true positives and true negatives, but then assumes that these are identical for each rater, so that the result is a single parameter.\nAs a concrete illustration, consider the wine judging data used in Hodgson (2008) (data from the author in personal communication). The first five rows look like this:\n\n\n\nTable 3: Wine ratings showing rows 1-5 of 183, taken from the study by Hodgson (personal communication).\n\n\n\n\n\nWine\nJ1\nJ2\nJ3\nJ4\n\n\n\n\n1\n3\n3\n3\n3\n\n\n2\n3\n3\n3\n3\n\n\n3\n3\n3\n2\n4\n\n\n4\n3\n4\n3\n1\n\n\n5\n4\n2\n1\n1\n\n\n\n\n\n\nThe 1-4 scale is ordinal, meaning that we imagine quality to increase with the numerical value. For this discussion, we will reduce the scale to a binary classification: acceptable wine (ratings 2-4) or wine to avoid (rating 1). This simplifies the table to.\n\n\n\nTable 4: Simplified wine ratings, where 1 means “acceptable” and 0 means “avoid.”\n\n\n\n\n\nWine\nJ1\nJ2\nJ3\nJ4\n\n\n\n\n1\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n1\n\n\n3\n1\n1\n0\n1\n\n\n4\n1\n1\n1\n0\n\n\n5\n1\n0\n0\n0\n\n\n\n\n\n\nIntuitively there is more reason to believe wines 1 and 2 are acceptable than with 3 or 4, for which one judge found them not metal-worthy. It seems safe to avoid wine 5, since all but one of the judges found it not to be metal-worthy.\nWe can put this information into the confusion matrix, but we’re missing information.\n\n\n\nTable 5: Confusion matrix for the first five wines, with missing information as question marks.\n\n\n\n\n\n\ntrue value of 2-4\ntrue value of 1\nTotal\n\n\n\n\nrating of 2-4\n?\n?\n15\n\n\nrating of 1\n?\n?\n5\n\n\n\n\n\n\nThere is some awkwardness in imagining that the physical universe encompasses a reality including true wine ratings. It seems absurd on the face of it that the physical laws of the universe imply “wine science.” It’s useful to think of it in reverse: this reification of the ratings is a hypothesis that may be rejected by the statistics we derive from the data. If we find that raters are assigning ratings at random, there’s no evidence here for the reality of wine asthetics. On the other hand, strong indications of non-randomness need to be explained somehow.\nTaking the leap that all non-randomness is causal, and physical reality has a monopoly on causality, then something concrete in the world is behind the rating consistency. We could imagine the chemical similarities and differences of wines and their effects on human anatomy, and so on. Being “real” in this case doesn’t mean that the universe appreciates wine, only that the universe includes physical blobs called humans, and they have mechanical consistencies that correlate observations in this case. The confusion matrix is also used for calculating the causal effect of an experiment. In a medical context, the rows might be treatment categories and the columns patient outcomes. See Eubanks (2014) for more on that.\nBy using the rating counts for each subject instead of the total, we can–surprisingly–estimate the whole confusion matrix. We can even estimate the true classification values for each subject. For that level of detail see Chapter 4: Hierarchical Models."
  },
  {
    "objectID": "introduction.html#sec-intro-agreement",
    "href": "introduction.html#sec-intro-agreement",
    "title": "Introduction",
    "section": "",
    "text": "Signal and noise as variability between and variability within subjects translates into rater agreement within subjects versus between subjects. The usual way to turn this idea into a number is to count the number of actual agreements and divide by the total number possible.\n\n\n\nTable 6: Rater agreements count, showing the maximum agreements for N raters, the number matches that are evidence versus logical implications and the ratio of evidence to matches.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of raters\nMaximum agreements\nEvidentiary agreements\nImplied agreements\nEvidence / Maximum\n\n\n\n\n1\n0\n-\n-\n-\n\n\n2\n1\n1\n0\n1\n\n\n3\n3\n2\n1\n.67\n\n\n4\n6\n3\n3\n.5\n\n\n5\n10\n4\n6\n.4\n\n\nN\n\\(N(N-1)/2\\)\n\\(N-1\\)\n\\(N(N-1)/2 - N + 1\\)\n\\(2/N\\)\n\n\n\n\n\n\nThe maximum agreements in the table come from the combinatorics formula “N choose 2,” counting the number of possible pairs out of N. If a group of 5 people meet and all shake hands, it’s 10 handshakes. The number of pairs grow much more rapidly than the number of raters does6.\nThere’s a rule in thumb in statistics that it’s preferable to use standard deviation instead of variance when understanding the data. That’s because the units of standard deviation are the same as the original data, whereas the variance has squared units. The same effect is going on with counting agreement. In some sense, what we want looks more like the square root of agreement.\nIntuitively, three raters agreeing doesn’t seem like three times as much evidence as two agreeing. And if we look at where those agreements are coming from, most of them are purely logical. Agreement is transitive, meaning if A agrees with B and B agrees with C, then A agrees with C. This is a logical necessity, and it’s not evidence of anything. The Evidentiary and Implied columns of the table record the distinction between the two types of matches. As the number of raters \\(N\\) increases, the logically necessary agreements greatly outnumber the evidentiary ones, so the ratio in the last column of the table decreases toward zero.\nIf we only counted the evidentiary agreements, the maximum for \\(N\\) raters would be \\(N - 1\\), which is roughly proportional to the square root of the total number of agreements. We’ll come back to that idea later. However, the well-trodden path is to estimate the probability of agreement over all the subjects, which leads to the kappa statistics.\nFor the five wine ratings that are reduced to the binary category “avoid” or “don’t avoid” in Table 4, we can calculate the agreement as follows.\n\n\n\nTable 7: Agreement calculation for the first five wines, showing the maximum possible agreements, the actual agreements, and the agreement proportion out of the maximum.\n\n\n\n\n\nWine\nRaters\nPossible\nActual\nAgreement\n\n\n\n\n1\n4\n6\n6\n1\n\n\n2\n4\n6\n6\n1\n\n\n3\n4\n6\n3\n.5\n\n\n4\n4\n6\n3\n.5\n\n\n5\n4\n6\n3\n.5\n\n\n\n\n\n\nFor the fifth vintage in Table 7, there are more matches with the zeros than there are with the ones; both are counted. It is at this point what seemed like a good starting point begins to turn into a muddle, because since both the zeros and the ones can create agreement, how do we know what’s good enough agreement? What’s the worst agreement possible?\nA group of \\(n_1\\) raters who agree on the 1 ratings produces a number of agreements about proportional to \\(n_1^2\\), and similarly the \\(n_0\\) raters of 0 agreements produce about \\(n_0^2\\) agreements. Together that’s around \\(n_1^2 + n_0^2\\) agreements, which is less than or equal to \\((n_1 + n_0)^2\\), which is about the number of agreements if everyone agreed on a single category. So we get more agreement when everyone agrees than we do when the raters split into groups. That means that the least agreement happens when the raters are evenly split, giving us a floor for agreement.\n\n\n\nTable 8: Minimum rater agreement rates\n\n\n\n\n\n\n\n\n\n\n\nNumber of raters\nMaximum agreements\nMinimum agreements\nMinimum rate\n\n\n\n\n4\n6\n2\n.33\n\n\n6\n15\n6\n.40\n\n\n8\n28\n12\n.43\n\n\n10\n45\n20\n.44\n\n\neven N\n\\(N(N-1)/2\\)\n\\(N(N/2-1)/2\\)\n\\((N/2-1)/(N - )\\)\n\n\n\n\n\n\nFor six or more raters, there’s at least a 40% agreement rate, even when there’s the least possible amount of agreement. It’s annoying that the minimum rate of agreement changes with the number of raters. For real cases when we want to use these ideas, we might have different numbers of raters for different subjects, complicating an average minimum rate, which defines the worst case agreement."
  },
  {
    "objectID": "introduction.html#chance-correction",
    "href": "introduction.html#chance-correction",
    "title": "Introduction",
    "section": "",
    "text": "We saw in the previous section that the minimum frequency of agreement among raters is quite large, tending toward one half of the maximum possible. Whatever statistic we create as a measure of agreement needs to take that into account if we are to make sense of the results. We’ll get maximum agreement if all the raters choose the same category (of the two choices), and we’ll get minimum agreement if they split evenly between the categories, as if they each flipped a coin to decide. So there’s an association between worst-case random raters and the minimum agreement rate. This is the basis of the chance correction in the S statistic, which assumes coin-flipping raters as the worst-case benchmark from which to measure agreement.\nThe general formula for taking into account a worst-case (random) match rate \\(m_r\\) first computes the practical range of our scale, which is \\(1 - m_r\\), since the statistic can’t be less than \\(m_r\\). The amount of agreement observed \\(m_o\\) is then pinned to this scale as a “chance-corrected” match rate \\(\\kappa\\) (kappa) with\n\\[\\kappa = \\frac{m_o - m_r}{1 - m_r}\\]\nThe S statistic is a special case of this formula, where \\(m_r = .5\\). The sample of wine ratings can be used to illustrate. Recall that earlier we simplified the judges’ scale of “no medal” to “gold medal” to a binary classification between “no medal” (coded as zero) or “any medal” (coded as one). There were 30 maximum agreements among the four raters over the five wines (six per wine), and 21 actual agreements. The minimum is when the ratings are evenly split on each case, generating two agreements for each wine, or 10 total. An S-like statistic would be calculated like this:\n\\[\n\\begin{aligned}\nm_o &= 21/30 = .70 \\\\\nm_r &= 10/30 = .33 \\\\\n\\kappa_s &= \\frac{.70 - .33}{1 - .33} = .47\n\\end{aligned}\n\\] The kappa is now on a scale of zero (worse) to one (perfect agreement). Interpreting what the value means is difficult because it’s on the wrong scale as noted above. A value close to one half sounds mediocre, but not terrible, but we’re really looking for something like the square root of the agreement rate, which is .69. That value corresponds better to the intuition that the agreement in the table looks pretty good.\nThe coin-flipping assumption for randomness can be replaced with other assumptions. This has the effect of increasing the assumed worst-case agreement rate and hence decreasing kappa values. Perhaps the most popular choice is to reason that the raters are more likely to randomly pick the most common category so the coin should be weighted according to the rating distribution.\nFor example, the table of simplified wine ratings has 15 ratings of 1 and 5 of 0, or 75% 1-ratings. The worst case match rate is then the product of the proportions of 1 ratings, or \\(m_r = (.75)(.75) + (.25)(.25) = .625\\). This is the assumption of the Fleiss kappa. Note that the actual worst case match rate is still close to one half (because of the math), so it’s possible to get a negative Fleiss kappa if the assumption about the distribution of randomness fails.\nOne calculation under this proportionality assumption is:\n\\[\n\\begin{aligned}\nm_o &= 21/30 = .70 \\\\\nm_r &= .625 \\\\\n\\kappa_f &= \\frac{.70 - .625}{1 - .625} = .20\n\\end{aligned}\n\\]The actual calculations of these statistics are complicated by the choice of using exact combinatorical counts or long-run estimates. This distinction need not concern us in here.\nThe deflation of the rater agreement statistic in the Fleiss calculation above makes it clear that assumptions about random agreement are critical. A researcher may justifiably wonder which value is correct: are the wine judges in pretty good agreement, or is the judging mostly random? For historical reasons, rather than treating rater disagreement as a parameter to be discovered, the field has evolved to propose many variations of the kappa formula with different assumptions about the worst-case agreement rate.\nA limitation of most of the kappa statistics is that they make the simplifying assumption that a pair of raters reaches agreement randomly if both raters are random. This omits the possibility that one rater is accurate and the other isn’t."
  },
  {
    "objectID": "introduction.html#example-wine-judging",
    "href": "introduction.html#example-wine-judging",
    "title": "Introduction",
    "section": "4.1 Example: Wine Judging",
    "text": "4.1 Example: Wine Judging\nThe sections above used five rows of the wine judging data for illustration. We now use the whole data set of 183 wines each rated by four judges to estimate the t-a-p parameters. The included app was used (see Chapter 7) for the analysis.\nThe binary classification illustrated above was to convert the original scale (1 = no medal, 2 = bronze, 3 = silver, 4 = gold medal) to a binary outcome where 1 = any medal (bronze, silver, gold), and 0 = no medal. We called the Class 1 category “acceptable.” Using the app to analyze that binary comparison we get the following output.\n\n\n\n\n\n\nFigure 3: Wine rating t-a-p analysis comparing C1 = ratings 1,2,3 (in-class) to C0 = rating 0.\n\n\n\nThe acceptable wines are the primary (in-) class, and the plot shows the distribution of ratings for acceptability. Since there are four judges, the possibilities are:\n\nNone of them could find the wine acceptable. This is the 0 at the left end of the plot. It happens for about 6% of the wines.\nAll four of them could find the wine acceptable. This is the 4 (four judges) at the right end of the plot. That happened for about 49% of the wines.\nSomething in between, where the judges disagree. In that case we can have from one to three ratings of “acceptable,” corresponding to those points on the plot.\n\nThe lollipops (black lines with dots on top) in Figure 3 show the empirical data straight out of the ratings. The dashed blue line is the model fit, showing what the distribution would be if the model were exactly correct. Here, “model” means applying the estimated parameters to generate the distribution.\nThe estimated parameters are found at the top of the plot:\n\n\\(t\\) = .73 estimates that 73% of wines are acceptable in reality. This is more than the rate of unanimous agreement, which we saw above was only 49%.\n\\(a\\) = .54 estimates that the judges make accurate ratings (justified true belief) more than half the time. The rest of the ratings are assumed to be random.\n\\(p\\) = .78 estimates that when random ratings are made, judges choose “acceptable” 78% of the time. Notice that this is close to the estimated actual rate of 73% (the estimated value of \\(t\\) above). This turns out to be a desirable quality in judges. See Chapter 3 for details, especially the section on the Fleiss kappa.\n\nNote that the randomness of inaccurate ratings is not a conscious choice of raters. They aren’t giving up and flipping a coin. They still have a reasonable basis for making a rating, and may be quite confident about it. The assumption is that something Gettier-like has gone wrong with the causal process that links the input (wine) to output (rating), which flips the process from deterministic to stochastic. As with any regression model, the world doesn’t have to actually be that way; it’s just an assumption to allow us to create the model. Then we check to see if the data matches the model. In this case, the data matches the model pretty well, as we can visually assess by comparing the blue dashed line to the lollipops. More formal tests of agreement will have to wait until later.\nThe wine ratings comprise four categories that are naturally ordered from 1 = “no medal” to 4 = “gold medal.” That kind of data is common: from surveys with Strongly Disagree to Strongly Agree scales, from consumer ratings, and many other sources. We humans seem to have a natural facility to think of descriptions along an imaginary intensity scale. This data type is called “ordinal,” meaning we can put the categories in order.\nThe t-a-p model is designed to work with binary data. It’s possible to switch to a multinomial model, but it’s common to analyze ordinal scales using binary distinctions. I’ll show how that works with the wine data. Along the 1,2,3,4 scale, there are three sequential cut-points where the commas are. The first one we can denote “1|2”, which splits the scores into two sets: an in-class {1} and and out-class {2,3,4}. This is the same thing as the Class 1 and Class 0 distinction from earlier. The second cut-point is at 2|3, splitting the data into ratings {1,2} and {3,4}. The plot shows the fraction of ratings in the in- and out-class for each cut-point.\n\n\n\n\n\n\nFigure 4: Wine ratings divided into binary groups by cutpoint.\n\n\n\nThe 3|4 bar of the plot shows us that 88% of ratings are in {1,2,3}, so the remaining 12% are 4s (gold medal). As a validity check, there are 183 wines, each rated by four judges, so that should be 732 ratings. That looks like the height on the bars, so it checks out.\nFor a given cut-point, a wine rating will either be in the in-class or out-class This converts the scale to binary, and then we can estimate the model parameters. We repeat the t-a-p model for each cut-point to create a pattern like the one in the plot below.\n\n\n\n\n\n\nFigure 5: t-a-p parameter estimates for ordinal scale based on cutpoints\n\n\n\nThe plot in Figure 5 shows each t-a-p parameter for each cut-point. As a reference, the Fleiss kappa estimates are also included as dotted lines. The significance of the Fleiss kappa is discussed later.\nThe accuracy parameter \\(a\\) at the top shows good accuracy for the lowest cut-point, meaning that the judges were good at distinguishing the least worthy wines from the rest. This is the case we analyzed earlier when we called the 1 rating unacceptable. As the quality rating increases, moving to the right on the top plot, accuracy decreases to less than half its value for the first cut-point (focus on the solid line). This would be the case if poor wines have more basis in physiology (sourness, etc.) and as the assessments become more aesthetic, they become more individualized and have less group agreement.\nThe second plot, showing estimates for \\(p\\) show the probability with which judges place a wine randomly into the in-class for inaccurate ratings. This will happen less often for the left-most cut-points, since accuracy is higher there. The dotted line is useful here: it shows what the \\(p\\) parameter would look like if the raters assigned ratings proportionally when making inaccurate classifications. For example, we noted earlier that 88% of the ratings are 1-3 (rightmost bar of the previous plot), so for the 3|4 cut-point, proportional random ratings would assign 88% of the inaccurate ratings into the {1,2,3} in-class. That’s where the dotted line is, at 88%. The actual parameter estimate (solid line) is at about 84%, meaning that judges are probably too conservative about assigning the gold medal category. Accuracy is low for the gold medals, and when the judges rate inaccurately, the ratings are slightly biased toward the lower ratings.\nThe bottom plot in Figure 5 estimates the true proportions for each cut-point, after taking into account the other two parameters. For the 3|4 cut-point on the right, it shows a proportion of {1,2,3} wines of about 65%. This is much lower than the 88% of ratings that are {1,2,3}. That’s the combined effect of inaccurate ratings at the top end of the scale combined with the bias toward lower ratings for inaccurate ratings. Looking at the two ends of the scale for the \\(t\\) plot, we can estimate that about 26% of wines are truly in the 1 = no medal category, and about 35% are in the 4 = gold medal category (1 - .65 = .35). That 35% figure comes from reading the estimated value of \\(t\\) at the 3|4 cut point, which is about 65%, and subtracting from one.\nIt’s possible to tell good wine from bad wine pretty reliably in this data set, but beyond that individual tastes may not be discerning enough to sort out four levels of wine quality. The scale could possibly be reduced to three ratings, or else keep the existing scale but collapse the 3 and 4 ratings into a single category before reporting the results. The effect of the existing rating system is leaving a lot of probably excellent wines with poorer ratings than they deserve. An alternative approach is to try to improve the accuracy of the higher ratings, which can be facilitated by reducing the rater bias against the highest award. One study showed that it’s possible to improve reliability in college grade assignment through feedback Millet (2010). This method might work more generally for reducing rater bias."
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn earlier paper on this was published under the auspices of the NSA, and has a stamp declaring it unclassified. See https://www.nsa.gov/portals/75/documents/news-features/declassified-documents/tech-journals/application-of-cluster-analysis.pdf↩︎\nIt’s a mystery why humans can agree on mathematical justifications at such a high rate. If agreement indicates reality, then math must be real in some way.↩︎\nI found this on wikipedia here https://en.wikipedia.org/wiki/Gettier_problem↩︎\nwhich seems like an infinite regress of JTB inquiry.↩︎\nConfusion matrices are at the heart of measures of causality too, for reasons that are not coincidental. See Eubanks (2014).↩︎\nThe process of a group of people reaching agreement does not seem to be transitive! It may well be three times as difficult for three people to agree on a movie to watch as two people. The rater agreement models get around this by assuming that raters don’t talk to (argue with) each other, but reach conclusions independently before comparing notes.↩︎\nFor much more on this idea see “Causal Interfaces.”↩︎"
  },
  {
    "objectID": "hierarchical.html",
    "href": "hierarchical.html",
    "title": "Chapter 5: Hierarchical Models",
    "section": "",
    "text": "In Chapter 4: The Kappa Paradox we saw how adding parameters can increase the explanatory power of t-a-p models. This is an old idea. The Cohen kappa (Cohen, 1960) dates from the 1960s, and for large data sets can be seen as a variation of the Fleiss kappa, but where each rater has a parameter for random assigments. In the original paper, the formula is derived for two rates, but it can be expanded to any number of them. Dawid & Skene (1979) proposed a more ambitious rater model that entailed multiple coefficients per rater and a truth parameter for each subject.\nA natural expansion of the t-a-p model assigns coefficients for each rater and subject. For example, we could assign a truth parameter \\(t_i\\) to each subject \\(i\\), and accuracy and random assignment parameters \\(a_j\\) and \\(p_j\\), respectively, to each rater \\(j\\). The latter requires that we have IDs for the raters. Generally, regression models like this are called hierarchical models or random effects models or fixed effects models, depending on the research tradition (As noted in Agresti (2003), p. 523 and Gelman & Hill (2006), p. 245, fixed versus random effects are not good descriptions for Bayesian models).\nThe three parameter t-a-p model is already hierarchical in the sense that the ratings for each subject are clustered for analysis. That’s how we create a distribution of agreement to compare with a binomial mixture. The hierarchical model we’re discussing in this chapter is a further extension of that idea, where we add parameters to the model, generally for one of two reasons. One reason is to increase model fit by accounting for variation in the data that is not explained by the three average parameters. The other reason is to obtain estimates for individual subjects or raters.\nIndividual rater parameters and subject truth values are of interest in many contexts. In educational psychology, we might want to know how well a teacher is able to assess student learning, and how well the students demonstrate knowledge or skill. In medical research, we might want to know how well a doctor is able to diagnose a disease, and how well the patients are responding to treatment. In machine learning, we may be concerned with the quality of training data drawn from human classifications. The ratings of consumer products on sites like Amazon or Yelp can be polluted with unfaithful reviews, so some means of detection is valuable. Likewise, individual subject truth parameters are valuable if we want to make decisions with the rating data, for example in a medical diagnosis, a pass/fail assessment of student learning, or using product ratings to make a selection."
  },
  {
    "objectID": "hierarchical.html#sec-dawid-skene",
    "href": "hierarchical.html#sec-dawid-skene",
    "title": "Chapter 5: Hierarchical Models",
    "section": "5.1 Dawid & Skene",
    "text": "5.1 Dawid & Skene\nDawid & Skene (1979) published what may be the first hierarchical (unpooled) moded for estimating subject and rater coefficients. In the abstract they describe the problem being addressed.\n\nIn compiling a patient record many facets are subject to errors of measurement. A model is presented which allows individual error-rates to be estimated for polytomous facets even when the patient’s “true” response is not available.\n\nHere, “polytomous facets” means more than two possible classifications. For this discussion, I’ll only consider binary classes. Even though “true” is in scare quotes above, the model assumes that true classes \\(T_i\\) exist. As we will see from the statistical approach here, the reference to error rates does not mean that Dawid & Skene estimates rater accuracy, on the rater error rates. Since some correct answers are inaccurate (at least in the t-a-p model), the estimated rater parameters contain both accurate and inaccurate matches between the assigned and true classes of subjects. In short, Dawid & Skene’s approach estimates rater distributions of assigned classes. Comparing to the classical rater agreement statistics, it’s perhaps closest to the Cohen kappa (Cohen, 1960), which was described in Chapter 3. Cohen’s statistic allows each rater to have a response pattern, which is used to estimate matches at random. One additional complication with Dawid & Skene’s method is that they consider multiple ratings from a rater being assigned to a single subject. This complicates the notation quite a bit, and I’ll simplify it to assume one rating per rater per subject.\nThe generation of ratings expresses the model in terms of an individual rater. For example, with \\(k=2\\) categories a rating looks like the following.\n\n\n\nDawid & Skene sampling model for a single rating and k = 2\n\n\nIn the paper describing the method, equation 2.7 gives the likelihood of the observations. For consistency with the rest of my treatment, I’ll change the notation to correspond to the t-a-p conventions, and simplify to one observation per rater. So as before \\(C_{ij}\\) means the annotated classication of rater \\(j\\) of subject \\(i\\). Now we’ll allow \\(C_{ij} \\, \\epsilon \\{1,\\dots,k\\}\\), corresponding to \\(k\\) possible classifications. The \\(K=2\\) case can be compared to what we’ve already done.\n\\[\nPr(\\text{data; parameters}) = \\prod_{i=1}^{N_s}  \\left( \\sum_{k=1}^K t_k \\prod_{j=1}^{N_r} \\prod_{l=1}^K (\\pi_{kl}^{(j)})^{I(C_{ij}=l)}\\right).\n\\]\nYou’ll recognize from earlier the rater distributions \\(\\pi_{kl}^{(j)} = Pr(\\text{Rater j assigns Class l}|\\text{true Class is k})\\). The indicator function \\(I(C_{ij}=l)\\) is binary, being one when the condition is true (the assigned class is \\(l\\)) and zero otherwise. So only one of the sub-products in the innermost loop will be something other than 1, and we can rewrite the likelihood as\n\\[\nPr(\\text{data; parameters}) \\sim \\prod_{i=1}^{N_s}  \\left( \\sum_{k=1}^K t_k  \\left[ \\prod_{C_{ij}=1} \\pi_{k1}^{(j)} \\dots \\prod_{C_{ij}=k}\\pi_{kk}^{(j)} \\right] \\right).\n\\tag{3}\\]\nThe innermost products are grouped by the assigned class, which is indexed by the subject \\(i\\) and rater \\(j\\). As the authors note, this describes the likelihood of \\(N_s\\) independent samples from a mixture of multinomial distribution with weights \\(t_1 \\dots t_k\\). The multinomial coefficients are all factored out and discarded here, since we don’t need them to maximize likelihood.\nThe formulation of Equation 3 is the same as what we derived in Section 2 before applying the t-a-p assumptions. See Equation 1; it’s the same thing except for the binary case. The application of the t-a-p model assumptions found in ?@eq-likelihood3 reparameterizes the \\(\\pi\\) coefficients and reduces from three per rater (since the four \\(\\pi\\)s sum to one, there are only three free variables per rater) plus the \\(t\\) parameter to two per rater. So the basic hierarchical t-a-p model has one fewer coefficient per rater. Dawid & Skene can be translated into a hierarchical t-a-p model along the lines of the average model in Chapter 4 where the accuracy and random assignment parameters split to be conditioned on the true class. It’s possible to use the Dawid & Skene approach while retaining a parameter for rater accuracy (two of them in this case), e.g. with a Stan script.\nBob Carpenter’s Stan code from Paun et al. (2018) is here:\n\n\nView Stan code\n\ndata {\n  int&lt;lower=1&gt; J; //number of annotators\n  int&lt;lower=2&gt; K; //number of classes\n  int&lt;lower=1&gt; N; //number of annotations\n  int&lt;lower=1&gt; I; //number of items\n  int&lt;lower=1,upper=I&gt; ii[N]; //the item the n-th annotation belongs to\n  int&lt;lower=1,upper=J&gt; jj[N]; //the annotator which produced the n-th annotation\n  int y[N]; //the class of the n-th annotation\n}\n\ntransformed data {\n  vector[K] alpha = rep_vector(1,K); //class prevalence prior\n  vector[K] eta = rep_vector(1,K); //annotator abilities prior\n}\n\nparameters {\n  simplex[K] pi;\n  simplex[K] beta[J,K];\n}\n\ntransformed parameters {\n  vector[K] log_q_c[I];\n  vector[K] log_pi;\n  vector[K] log_beta[J,K];\n  \n  log_pi = log(pi);\n  log_beta = log(beta);\n  \n  for (i in 1:I) \n    log_q_c[i] = log_pi;\n  \n  for (n in 1:N) \n    for (h in 1:K)\n      log_q_c[ii[n],h] = log_q_c[ii[n],h] + log_beta[jj[n],h,y[n]];\n}\n\nmodel {\n  for(j in 1:J)\n    for(h in 1:K)\n      beta[j,h] ~ dirichlet(eta);\n  \n  pi ~ dirichlet(alpha);\n  \n  for (i in 1:I)\n    target += log_sum_exp(log_q_c[i]);\n}\n\ngenerated quantities {\n  vector[K] q_z[I]; //the true class distribution of each item\n  \n  for(i in 1:I)\n    q_z[i] = softmax(log_q_c[i]);\n}\n\nSee p. 573 of Paun et al. (2018) for a “partially pooled” variation of the Dawid & Skene method.\n\nThis structure [uses] information about the population to improve estimates of individuals by regularizing toward the population mean. This is particularly helpful with low count data as found in many crowdsourcing tasks"
  },
  {
    "objectID": "hierarchical.html#sec-mace",
    "href": "hierarchical.html#sec-mace",
    "title": "Chapter 5: Hierarchical Models",
    "section": "5.2 MACE",
    "text": "5.2 MACE\nThe MACE algorithm is newer rater aggreement approach from the machine learning community. It was introduced in Hovy et al. (2013), where we find a description of the model’s annotation (rating) generation process.\n\nWe assume that an annotator always produces the correct label when he tries to. […] Our model generates the observed annotations as follows: First, for each instance \\(i\\), we sample the true label \\(T_i\\) from a uniform prior. Then for each annotator \\(j\\), we draw a binary variable \\(S_{ij}\\) from a Bernoulli distribution with parameter \\(1 - \\theta_j\\). \\(S_{ij}\\) represents whether or not an annotator \\(j\\) is spamming on instance \\(i\\). We assume that when an annotator is not spamming on an instance, i.e. \\(S_{ij}=0\\), he just copies the true value to produce annotation \\(A_{ij}\\). If \\(S_{ij}=1\\), we say that the annotator is spamming on the current instance, and \\(A_{ij}\\) is sampled from a multinomial with parameter vector \\(\\xi_j\\). Note that in this case the annotation \\(A_{ij}\\) does not depend on the true label \\(T_i\\).\n\nIt’s straightforward to translate this into the t-a-p notation., where we assume there are only two annotations (ratings), a 0 or 1.\n\nThe subjects are indexed by \\(i = 1 \\dots N\\) and raters/annotators indexed by \\(j = 1 \\dots M\\) in the MACE description. For t-a-p, we’re using \\(N_s\\) for number of subjects and \\(N_r\\) for raters, and a total of \\(N\\) ratings.\n\\(T_i\\) is the true class in both descriptions\n\\(C_{ij}\\), the set of \\(N\\) ratings/classifications/annotations in MACE becomes \\(A_{ij}\\).\n\\(A_{ij}\\) in the t-a-p model is the binary indication of an accurate rating, which is \\(\\bar{S}_{ij} = 1 - S_{ij}\\) in MACE. Spamming is the opposite (complement) of accuracy.\nAverage rater accuracy \\(a_j\\) is \\(\\theta_j\\) in the MACE paper.\nThe random assignment probability for a rater \\(p_j\\) is \\(\\xi_j\\) in MACE.\n\nThe MACE algorithm captures all the elements of Justified True Belief, with latent truth and rater accuracy, just slightly reformulated. So MACE is the same as the hierarchical t-a-p model. I didn’t know any of this when I started developing the t-a-p model from the kappa tradition, but obviously the credit for invention goes to the MACE authors, who published this in 2013.\nThe likelihood function given in Hovy et al. (2013) can be translated into the t-a-p model notation as\n\\[\n\\begin{aligned}\nPr(C;t,a,p) &= \\sum_{T,A} \\left[ \\prod_{i}  Pr(T_i) \\prod_{j}  Pr(A_{ij}; a_j) Pr(C_{ij} | A_{ij},T_i;p_j)\\right] \\\\\n\\end{aligned}\n\\tag{4}\\]\nwhere I’ve recast the product into the more compact \\(\\pi\\) coefficients from Dawid & Skene, and assume binary classifications. All I can figure is that the first line in the paper has a typo reversing the outer sum with the product over \\(i\\), and it should be written as\n\\[\n\\begin{aligned}\nPr(C;t,a,p) &=  \\prod_{i}  Pr(T_i) \\sum_{T,A}\\prod_{j}  Pr(A_{ij}; a_j) Pr(C_{ij} | A_{ij},T_i;p_j) \\\\\n&= \\prod_{i}\\left( t_i \\prod_{j} \\pi_{1C_{ij}}^{(j)} + \\bar{t}_i \\prod_{j} \\pi_{0C_{ij}}^{(j)} \\right) .\n\\end{aligned}\n\\tag{5}\\]\nThe second line is the earlier derivation, matching the Dawid & Skene likelihood. The next step is to apply the t-a-p reparameterization to the \\(\\pi\\) coefficients. Unlike the Dawid & Skene paper, there’s no derivation of the likelihood formula, and the meaning of the sum over \\(T\\) and \\(A\\) is not clear. I assume it’s over the values that these parameters can take, not the empirical vectors of values themselves. In any case, the likelihood is expressed as a sum of products in Equation 4, which is different from the t-a-p derivation above, which is a product of sums.\nBob Carpenter’s Stan code for implenting MACE (Paun et al., 2018) matches the Equation 5 version. I have changed the array declarations to conform with the latest version of Stan (which broke old code) and added comments to annotate how the variables match to the t-a-p nomenclature. This code, as MACE is, is designed for multiple classes (more than 2), but I’ve described in the binary case. In that case, the algorithm assumes initially that \\(t=.5\\) as a way to generate the parameters we call \\(a_j\\) and \\(p_j\\), which then allow refined estimation of \\(t_i\\).\n\n\nView MACE Stan code\n\ndata {\n  int&lt;lower=1&gt; J; //number of annotators\n  int&lt;lower=2&gt; K; //number of classes\n  int&lt;lower=1&gt; N; //number of annotations\n  int&lt;lower=1&gt; I; //number of items\n  int&lt;lower=1,upper=I&gt; ii[N]; //the item the n-th annotation belongs to\n  int&lt;lower=1,upper=J&gt; jj[N]; //the annotator which produced the n-th annotation\n  int y[N]; //the class of the n-th annotation\n}\n\ntransformed data {\n  vector[K] alpha = rep_vector(1.0/K,K);\n  vector[K] eta = rep_vector(10,K);\n}\n\nparameters {\n  simplex[K] epsilon[J];\n  real&lt;lower=0, upper=1&gt; theta[J];\n}\n\ntransformed parameters {\n  vector[K] log_q_c[I];\n  vector[K] log_alpha;\n  \n  log_alpha = log(alpha);\n  \n  for (i in 1:I) \n    log_q_c[i] = log_alpha;\n  \n  for (n in 1:N) \n  {\n    for (h in 1:K)\n    {\n      int indicator = (y[n] == h);\n      log_q_c[ii[n],h] = log_q_c[ii[n],h] + log( theta[jj[n]] * indicator + (1-theta[jj[n]]) * epsilon[jj[n],y[n]] );\n    }\n  }\n}\n\nmodel {\n  for(j in 1:J)\n  {\n    epsilon[j] ~ dirichlet(eta);\n    theta[j] ~ beta(0.5,0.5);\n  }\n  \n  for (i in 1:I)\n    target += log_sum_exp(log_q_c[i]);\n}\n\ngenerated quantities {\n  vector[K] q_z[I]; //the true class distribution of each item\n  \n  for(i in 1:I)\n    q_z[i] = softmax(log_q_c[i]);\n}"
  },
  {
    "objectID": "kappa.html",
    "href": "kappa.html",
    "title": "Chapter 3: Kappa Statistics",
    "section": "",
    "text": "The formula for chance-corrected measure of agreement (generically a “kappa”) compares observed match rates to the expectation of random match rates. The kappas vary in how they estimate the random match rates. For two ratings to match, two raters \\(j,k\\) of the same subject \\(i\\) must agree in their assignment of either Class 1 or Class 0 classifications. In other words, the binary random variables must agree: \\(C_{ij} = C_{ik}\\). A generic formula that includes the most common kappas is\n\\[\n\\kappa = \\frac{m_o - m_c}{1 - m_c},\n\\]where \\(m_o\\) is the observed proportion of agreements and \\(m_c\\) is the expected proportion of agreements under chance. The assumption about \\(m_c\\) is a defining feature of the various kappa statistics. The most general treatment of such statistics is the Krippendorff alpha (Krippendorff, 2018, pp. 221–250)\nThe various kappas differ in the assumption made about the chance correction probability \\(m_c\\). Commonly, the assumption is that \\(m_c = x^2 + \\bar{x}^2\\) for some probability \\(x\\). This simple formulation makes sense when both raters are guessing, but the actual case is more complicated because a match “by chance” could be a case where one rating was accurate and the other was a guess. This distinction isn’t generally made in the derivations of the kappas, although the AC1 paper discusses the issue, and hints at a full three-parameter model. It’s ironic that the confusion about kappas is disagreement about the probability of agreement by chance.\nThe Fleiss kappa (Fleiss, 1971) uses the fraction of Class 1 ratings \\(c\\) to create \\(m_c = c^2 + \\bar{c}^2\\). The S statistic (Bennett et al., 1954), also called the Guilfords’ G or G-index1 (Holley & Guilford, 1964), is a kappa that assumes \\(m_c = 1/2\\) when there are two categories. The AC1 kappa has a different form, assuming that \\(m_c = 2c\\bar{c}\\) (Gwet, 2008). The Cohen kappa is a variation where each rater gets a guessing distribution, so \\(m_c = x_1x_2 + \\bar{x_1}\\bar{x_2}\\) (Cohen, 1960). Because of the extra parameter, discussion of Cohen’s kappa is found in the chapter on hierarchical models rather than being included here.\nConsider two raters classifying an observation. In the t-a-p model we can express the expected value of observed matches \\(m_o\\) as the sum of three kinds of agreement: (1) \\(m_a\\) is when both raters are accurate (and hence agree), (2) \\(m_i\\) when both raters are inaccurate (guessing) and agree, and (3) \\(m_x\\) is the mixed case when one rater is accurate and the other is inaccurate but they agree. The second two of these have expressions that include the guessing rate \\(m_c\\). Following that thinking we have the following expectations for rates:\n\\[\n\\begin{aligned}\nm_a &= a^2 & \\text{(both accurate)}\\\\\nm_r &= p^2 + \\bar{p}^2 & \\text{(random ratings)}\\\\\nm_i &= \\bar{a}^2m_r = a^2m_r - 2am_r + m_r &\\text{(both inaccurate)}\\\\\nm_x &= 2a\\bar{a}(tp + \\bar{t}\\bar{p}) &\\text{(mixed accurate and inaccurate)}\\\\\nm_o &= m_a + m_i + m_x &\\text{(observed match rate)}\\\\\n    &= a^2+a^2m_r + m_r - 2am_r + 2a\\bar{a}(tp + \\bar{t}\\bar{p})\\\\\n\\end{aligned}\n\\tag{1}\\]\nFor \\(m_a\\), both ratings must be accurate, in which case they automatically agree. For \\(m_i\\), both must be inaccurate (probability \\(\\bar{a}^2\\)) and then match randomly (probability \\(m_r\\)). For \\(m_x\\), one rater must be accurate and the other inaccurate, in which case they agree if the accurate rater chooses the category that the inaccurate rater guesses. The various kappa derivations usually ignore these mixed matches in favor of using \\(m_r\\) as the chance match rate, which we called \\(m_c\\) in the kappa formula. This amounts to choosing \\(p\\) since \\(m_r = p^2 + \\bar{p}^2\\).\nThe various match rates in Equation 1 create a vocabulary for understanding some of the kappa statistics. The easiest one to analyze is the S-statistic (it is sometimes called the G-index)."
  },
  {
    "objectID": "kappa.html#footnotes",
    "href": "kappa.html#footnotes",
    "title": "Chapter 3: Kappa Statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot to be confused with the G-index used for ranking citation counts of scholars.↩︎"
  },
  {
    "objectID": "paradox.html",
    "href": "paradox.html",
    "title": "Chapter 4: More parameters",
    "section": "",
    "text": "1 Introduction\nProblems with chance-corrected agreement statistics have been studied for a long time, in particular when one class dominates the ratings (classifications). This would happen, for example, in a medical test for a rare condition, where most of the results would be negative. Non-intuitive results for kappa can result from such cases, a situation that has become known as the Kappa Paradox. See Bonett (2022) for one survey of such issues. Attempts have been made to modify the basic kappa formula to accommodate unbalanced classifications. In Byrt et al. (1993), observed agreement is adjusted for bias in rater tendencies (which would be the \\(p\\) parameter here) and for relative prevalence of classes. Krippendorff (2013),illustrates such a hypothetical case (page 484):\n\nSuppose an instrument manufacturer claims to have developed a test to diagnose a rare disease. Rare means that the probability of that disease in a population is small and to have enough cases in the sample, a large number of individuals need to be tested. […] Suppose two separate doctors administer the test to the same 1,000 individuals. Suppose each doctor finds one in 1,000 to have the disease and they agree in 998 cases on the outcome of the test.\n\nThe paradoxical result is that the Fliess kappa for this example is near zero, even though the raters agree on 99.8% of the cases. The reason is that the kappa is defined as the agreement rate minus the agreement rate expected by chance, and the expected agreement rate is very high due to the distribution of classes. Such unbalanced cases are common in practice, leading to the creation of new inter-rater measures like AC1 (Gwet, 2008) to attempt to accommodate this situation, but this solution has been criticized as well (Krippendorff, 2013).\nThe controversy can be summarized in the two points of view:\n\n(pro-kappa) the test has zero agreements on test-positive cases, and is therefore unreliable, therefore \\(\\kappa = 0\\) is correct, versus\n(anti-kappa) almost all the cases are in perfect agreement, therefore the test is reliable, and kappa should be near one.\n\nThe apparent problem can be usefully seen through the lens of the Fleiss kappa. If the baseline for comparison is randomly chosen pairs of ratings, any such set will be identical to the original data set in this case. Since the data is indistinguishable from randomness, the kappa, and hence accuracy should be zero. So in that sense the Fleiss kappa gives the correct result (kappa nearly zero). This conflicts with intuition in that there is a high level of raw agreement in the data.\nReliability statistics are generally concerned with two kinds of differences in ratings. Within subjects, if individual subjects are assigned ratings that don’t have much agreement, this is like measurement error. An ideal measure would return the same rating for the same subject every time. The other kind of difference is between subjects. There we want variation, because if all the subjects get the same rating, the scale doesn’t give us information. In the paradox example, there’s low variation within subjects (low measurement error), but very little variation between subjects, so the scale itself isn’t given us much information.\n\n\n2 Modeling the Paradox\nThe t-a-p model can illuinate this situation. If nearly all the ratings \\(C_{ij}\\) are one value, then the likelihood function over rating sums can be approximately reduced to the likelihood function over individual ratings given by\n\\[ \\text{L}(t,a,p;c_{ij}) \\approx a(t - p) + p \\tag{1}\\]\nso that \\(\\frac{\\partial L}{\\partial t} = a\\), \\(\\frac{\\partial L}{\\partial a} = t- p\\), and \\(\\frac{\\partial L}{\\partial p} = 1 - a\\). Setting these to zero in the usual way to find the maximum, we obtain contradictory solutions to the likelihood maximization problem, since the first and last conditions imply that \\(a=0\\) and \\(a=1\\), respectively. Additionally \\(t = p\\) in Equation 1 requires that raters are unbiased. The contradictory solutions for rater accuracy \\(a\\) reflect the two “paradoxical” positions described above, and these show up empirically if we solve for the unconstrained parameters. The data are nearly under-determined with respect to the full t-a-p model, and the posterior estimate samples show prevarication.\nTo illustrate the dual nature of the maximum likelihood solution, he three-parameter t-a-p model was fitted to N = 1,000 samples with R = 2 (two raters). As described above, 998 of the subjects are assigned Class 0 (negative test result) by both raters, and for the remaining two cases they split, with one Class 1 and one Class 0 each.\n\n\nShow the code\nrefresh = FALSE \n\nif (refresh) {\n  \n  counts &lt;- data.frame(\n                 N_r = 2,\n                 N_c = c(rep(0,980),rep(1,2))) |&gt; \n            count(N_r, N_c)\n  \n  fitted_model &lt;- tapModel::fit_counts_mcmc(counts, stan_model = \"t-a-p\", quiet = TRUE)\n  draws &lt;- fitted_model$fitted_model$draws()\n  write_rds(draws,\"data/Paradox_samples.RDS\")\n\n} else { # to save time\n  \n  draws &lt;- read_rds(\"data/Paradox_samples.RDS\")\n\n}\n\n# plot the draw densities\ndraws |&gt; \n  tapModel::plot_densities_mcmc()\n\n\n\n\n\n\n\n\nFigure 1: Results from a t-a-p model on the kappa paradox data, showing the density of estimates.\n\n\n\n\n\nTo see the Stan code used to fit the Bayesian t-a-p model, click the button below.\n\n\nShow the code\n\n//\n//   Stan model specification for fixed rater accuracy and no random effects\n//\n// Learn more about model development with Stan at:\n//\n//    http://mc-stan.org/users/interfaces/rstan.html\n//    https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\n//\n\n\n// functions to make the code simpler below\nfunctions {\n  real p_true(real a, real s) {  // convenience function for binomial probability for\n    return a + (1.0-a)*s;        // subjects that are class 1 in reality\n  }\n  real p_false(real a, real s) {  // convenience function for binomial probability for\n    return (1.0-a)*s;            // subjects that are class 2 in reality\n  }\n\n}\n\n// The ratings summary (number of 1-ratings per case) and descriptives\ndata {\n  int&lt;lower=0&gt; N;   // number of rows of data\n  array[N] int&lt;lower=0&gt; N_r;   // number of raters for a count pair\n  array[N] int N_c;  // count of ratings of category 1 for count pair\n  array[N] int&lt;lower=0&gt; n;  // multiplicity of this (N_r, N_c) pair\n}\n\n// The parameter to estimate\nparameters {\n  real&lt;lower=0, upper = 1&gt; a; // fixed for all raters\n  real&lt;lower=0, upper = 1&gt; p;        // guess rate for class 1 when inaccurate\n  real&lt;lower=0, upper = 1&gt; t;        // true class 1 rate\n}\n\n// The model to be estimated. We model the output\n// count (of 1s) by the binomial mixture described\n// in the paper. S is the fraction of 1-ratings in the whole data set\n// The log_sum_exp function is useful for this--we take the log of each binomial\n// likelihood using built-in functions, and the log_sum_exp function exponentiates,\n// adds, and then takes the log to get the actual likelihood we care about.\n// cf http://modernstatisticalworkflow.blogspot.com/2016/10/finite-mixture-models-in-stan.html\nmodel {\n  a ~ uniform(0,1);\n  t ~ uniform(0,1);\n  p ~ uniform(0,1);\n\n  for(i in 1:N) {  // for each subject rated\n    target += n[i] * log_sum_exp( log(t)   + binomial_lpmf(N_c[i] | N_r[i], p_true(a,p)),\n                            log(1-t) + binomial_lpmf(N_c[i] | N_r[i], p_false(a,p)));\n  }\n}\n\n\n\nThe results in Figure 1 come from a MCMC summary of samples for the parameters. The plot for \\(a\\) shows a bimodal distribution of rater accuracy that slightly favors \\(a \\approx 0\\) over \\(a \\approx 1\\). Also in agreement with the analysis, the mode for \\(p - t\\) is nearly zero since both of those statistics are near zero. The \\(t \\approx 0\\) tells us that the modeled result is that there are very view Class 1 (test positive) cases.\nThe parameter estimates in Figure 1 point us to divergent interpretations of the severely unbalanced data: (1) the accuracy is about zero and the results are due almost entirely to random ratings combined with a severe test bias, or (2) the accuracy is high, but the prevalence of the condition is very small. These two conclusions correspond to the usual philosophical positions in the paradox’s discussion. The graphs in Figure 1 show usefulness of MCMC sampling, and serve as a caution against taking mean estimate values as meaningful without inspecting the distribution first.\n\n\n3 Divergent Accuracy\nThe MCMC estimation for the accuracy parameter vacillates between two poles, which leads to the idea to split Class 1 accuracy from Class 0 accuracy using two parameters for accuracy. This approach was suggested in Cicchetti & Feinstein (1990). It is a straightforward modification to the t-a-p model to bifurcate accuracy by using \\(a_1\\) for the true Class 1 branch of the probability diagram, and \\(a_0\\) for the Class 0 side. The MCMC posterior distributions for those two parameters are shown in Figure 2.\n\n\nShow the code\nrefresh = FALSE \nrefresh = FALSE \n\nif (refresh) {\n  \n  counts &lt;- data.frame(\n                 N_r = 2,\n                 N_c = c(rep(0,980),rep(1,2))) |&gt; \n            count(N_r, N_c)\n  \n  fitted_model &lt;- tapModel::fit_counts_mcmc(counts, stan_model = \"t-a0a1-p\", quiet = TRUE)\n  draws &lt;- fitted_model$fitted_model$draws()\n  write_rds(draws,\"data/Paradox_samples2.RDS\")\n\n} else { # to save time\n  \n  draws &lt;- read_rds(\"data/Paradox_samples2.RDS\")\n\n}\n\n# plot the draw densities\ndraws |&gt; \n  tapModel::plot_densities_mcmc()\n\n\n\n\n\n\n\n\nFigure 2: Results from a t-a0,a1-p model on the kappa paradox data, showing the density of the parameter estimates. This one includes separate accuracy parameters for each class: a0 for Class 0 and a1 for Class1.\n\n\n\n\n\nTo see the Stan code used to fit the Bayesian t-a0,a1-p model, click the button below.\n\n\nShow the code\n//\n//   Stan model specification for fixed rater accuracy and no random effects\n//\n// Learn more about model development with Stan at:\n//\n//    http://mc-stan.org/users/interfaces/rstan.html\n//    https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\n//\n\n// functions to make the code simpler below\nfunctions {\n  real p_true(real a, real s) {  // convenience function for binomial probability for \n    return a + (1.0-a)*s;        // subjects that are class 1 in reality\n  }\n  real p_false(real a, real s) {  // convenience function for binomial probability for\n    return (1.0-a)*s;            // subjects that are class 2 in reality\n  }\n  \n}\n\n\n// The ratings summary (number of 1-ratings per case) and descriptives\ndata {\n  int&lt;lower=0&gt; N;   // number of subjects\n  array[N] int&lt;lower=0&gt; R;   // number of raters fo a given subject\n  array[N] int count;  // count of ratings of category 1 for subject i\n}\n\n// The parameter to estimate\nparameters {\n  real&lt;lower=0, upper = 1&gt; a1; // fixed for all raters\n  real&lt;lower=0, upper = 1&gt; a0; // fixed for all raters\n  real&lt;lower=0, upper = 1&gt; p;        // guess rate for class 1 when inaccurate\n  real&lt;lower=0, upper = 1&gt; t;        // true class 1 rate\n  \n}\n\n// The model to be estimated. We model the output\n// count (of 1s) by the binomial mixture described\n// in the paper. S is the fraction of 1-ratings in the whole data set\n// The log_sum_exp function is useful for this--we take the log of each binomial \n// likelihood using built-in functions, and the log_sum_exp function exponentiates,\n// adds, and then takes the log to get the actual likelihood we care about. \n// cf http://modernstatisticalworkflow.blogspot.com/2016/10/finite-mixture-models-in-stan.html\nmodel {\n  a1 ~ uniform(0,1);\n  a0 ~ uniform(0,1);\n  t ~ uniform(0,1);\n  p ~ uniform(0,1);\n  \n  for(i in 1:N) {  // for each subject rated\n    target += log_sum_exp(log(t)   + binomial_lpmf(count[i] | R[i], p_true(a1,p)),\n                          log(1-t) + binomial_lpmf(count[i] | R[i], p_false(a0,p)));\n  }\n}\n\n\n\nHistograms of MCMC draws in Figure 2 for the t-a0,a1-p model reveal the limitations of the three-parameter version. The plots suggest high confidence that \\(a_0\\) is near 1, and somewhat less confidence that \\(a_1\\) is near zero. There is evidence that the accuracy of negative test results is high, but there is no evidence that the accuracy of positive test results is high. A scatterplot of the two parameters as drawn together from the posterior distributions shows that they cluster around the modes of the distributions in Figure 2. In other words, the induced model prefers the case when the accuracy of negative test results is high, but the accuracy of positive test results is low.\n\n\nShow the code\n# make a scatterplot of the two accuracy parameters\ndraws |&gt; \n  tapModel::extract_param_draws_mcmc() |&gt; \n  ggplot(aes(x = a1, y = a0)) +\n  stat_density_2d_filled(aes(fill = ..level..)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 3: Scatterplot of the two accuracy parameters from the t-a0,a1-p model, showing that they converge to a high value of Class 0 accuracy and a low value of Class 1 accuracy.\n\n\n\n\n\nThe use of additional parameters to clarify the cases of unbalanced data can be expanded to accommodate a wide range of rater models.\nIt seems contradictory to say that we can classify Class 0 with confidence, but not Class 1, because these seem like two sides of the same coin. However, when there are many Class 0 cases, and only a few Class 1 cases, we could have 99% accuracy in true positives for Class 0 and nearly zero true positives for Class 1. One problem with the traditional kappa measures comes from condensing those two classification statistics into one.\n\n\n4 Individual p parameters\nWe could take this one step further and split \\(p\\) into two parameters that correspond to the two classes. In this t-a0,a1-p0,p1 case, we may get data sets that are not identifiable for label-switching reasons. That is, we may be able to find the maximum likelihood mixture of binomial distributions, but not know which is Class 0 and which is Class 1.\nWith the \\(a_0,a_1,p_0,p_1\\) parameters, label-switching means that we swap \\(\\bar{t}\\) for \\(t\\) and the two corresponding means swap as well. That requires some second set of the parameters \\(a_0',a_1',p_0',p_1'\\) such that\n\\[\n\\begin{aligned}\na_1 + \\bar{a_1}p_1 &= \\bar{a_0}'p_0' \\\\\na_1' + \\bar{a_1}'p_1' &= \\bar{a_0}p_0 \\\\\n\\end{aligned}\n\\tag{2}\\]\nIf Equation 2 holds, we can swap the primed parameters for the non-primes, and exchange \\(t\\) for \\(1-t\\), and the distribution is the same, with the same likelihood. There are four free parameters and two constraints shown above, plus the constraints that all parameters are in [0,1]. A example of a label-switching set is when \\(\\mu_0 = \\bar{a_0}p_0 &gt; a_1 + \\bar{a_1}p_1 = \\mu_1\\), so that there are more Class 1 ratings for Class 0 cases that there are for true Class 1 cases. Suppose \\(a_0 = .5\\), \\(a_1 = .2\\), \\(p_0 = .7\\), and \\(p_1 = .1\\). Then we have \\(\\mu_0 = (.5)(.7) = .35\\) and \\(\\mu_1 = .2 + (.8).1 = .28\\), so the mean of the Class 1 ratings is to the left of the mean of the Class 0 ratings.\nSuch situations point to poor quality data combined to too many parameters. For example, suppose the example above is wine tasting ratings, and it’s easier to for judges to agree on poor wine than excellent wine (this seems to be the case in practice). Suppose further that some of the raters are motivated to over-rate poor wines, pushing up the \\(p_0\\) rate to .7. This rating inflation makes the poor wines seem better than they are, and by comparison the good wines seem worse. From the data, we can’t tell which class is which under these model assumptions and with this data. However, the MCMC results will at least show us a multi-model distribution. In general, splitting the accuracy or randomness parameters should probably only be done for a specific reason, as with the paradox example, where we saw a bimodal distribution for accuracy.\n\n\n5 Estimating Parameters\nThe tapModel library and the advanced version of the interactive application can both be used to generate MCMC results from the t-a-p models expanded to include two \\(a\\) parameters and (optionally) two \\(p\\) parameters. There is an illustration of this in Chapter 8: The t-a-p App.\n\n\n\n\n\nReferences\n\nBonett, D. G. (2022). Statistical inference for g-indices of agreement. Journal of Educational and Behavioral Statistics, 47(4), 438–458.\n\n\nByrt, T., Bishop, J., & Carlin, J. B. (1993). Bias, prevalence and kappa. Journal of Clinical Epidemiology, 46(5), 423–429.\n\n\nCicchetti, D. V., & Feinstein, A. R. (1990). High agreement but low kappa: II. Resolving the paradoxes. Journal of Clinical Epidemiology, 43(6), 551–558.\n\n\nGwet, K. L. (2008). Computing inter-rater reliability and its variance in the presence of high agreement. British Journal of Mathematical and Statistical Psychology, 61(1), 29–48.\n\n\nKrippendorff, K. (2013). Commentary: A dissenting view on so-called paradoxes of reliability coefficients. Annals of the International Communication Association, 36(1), 481–499."
  },
  {
    "objectID": "t-a-p diagram-dev.html",
    "href": "t-a-p diagram-dev.html",
    "title": "t-a-p diagram",
    "section": "",
    "text": "Modified to separate branches"
  },
  {
    "objectID": "tapmodel.html",
    "href": "tapmodel.html",
    "title": "Chapter 2: The t-a-p Model",
    "section": "",
    "text": "This chapter examines the assumptions and implications of the t-a-p model in detail to build a theoretical foundation for estimation and inference. This work also paves the way to connect the three parameter model to widely-used rater agreement statistics (kappas) and machine learning algorithms."
  },
  {
    "objectID": "tapmodel.html#the-expectation-step",
    "href": "tapmodel.html#the-expectation-step",
    "title": "Chapter 2: The t-a-p Model",
    "section": "7.1 The Expectation Step",
    "text": "7.1 The Expectation Step\nWith our starting guess of \\(t = .5\\), \\(a = .5\\), and \\(p = .5\\), we can use a trick to infer what the \\(T_i\\) true classes are by asking for the conditional probability of the true class given the observed ratings. This is the E-step, where we compute the expected values of the latent variables based on the current parameter estimates.\nIf we examine this question for a single rating, we get an average result.\n\\[\n\\begin{aligned}\nPr[T_i = 1 | R_{ij} = 1] &= \\frac{Pr[T_i = 1]Pr[C_{ij} = 1 | T_i = 1] }{Pr[C_{ij} = 1]} \\\\\n&= \\frac{t(a + \\bar{a}p)}{t(a + \\bar{a}p) + \\bar{t}\\bar{a}p}\\\\\n&= \\frac{t(a + \\bar{a}p)}{ta + \\bar{a}p}\\\\\n\\end{aligned}\n\\]\nHowever, this ignores the whole point of the rater-agreement question. We’re interested in the probability of the pattern of results for a subject, not just a single rating. So we need to compute the expected value of the true class for each subject based on all of their ratings. Recall that the latent truth value is fixed for each subject (hence the \\(i\\) index), so we need to switch to a binomial probability distribution that takes into account all the ratings for a subject. For some given subject \\(i\\) with \\(N_r\\) raters, of whom \\(N_c\\) assigned Class 1, the probability of getting this count is\n\\[\n\\begin{aligned}\nPr[T = 1 | N_c = k] &= \\frac{Pr[T = 1]Pr[N_c = k , T = 1] }{Pr[N_c = k]} \\\\\n&= \\frac{t\\,\\text{binom}(N_r,k,a + \\bar{a}p)}{t\\,\\text{binom}(N_r,k,a + \\bar{a}p)+\\bar{t}\\,\\text{binom}(N_r,k,\\bar{a}p)}.\\\\\n\\end{aligned}\n\\]\nThe function \\(\\text{binom}(N_i,k,p)\\) is the binomial probability of getting \\(k\\) successes in \\(N_i\\) trials with a given success probability. We can compute this for each subject in our sample ratings data, and then take the average across all subjects to get the expected value of the true class for each subject.\n\n\nShow the code\n# set the initial guess for params\nt &lt;- .5\na &lt;- .5\np &lt;- .5\n\ne_step &lt;- ratings |&gt; \n  group_by(subject_id) |&gt; \n  summarize(\n    N_r = n(), # number of raters\n    R_i = sum(rating), # number of Class 1 ratings\n    t_i = (t * dbinom(R_i , N_r, a + (1-a)*p)) / \n            (t * dbinom(R_i , N_r, a + (1-a)*p) + \n             (1-t) * dbinom(R_i , N_r, (1-a)*p))\n  )\n\ne_step |&gt; \n  ggplot(aes(x = t_i)) +\n  geom_histogram() +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 5: E-step results for sample ratings show as a histogram of estimated probability of Class 1\n\n\n\n\n\nThe mean of the \\(t_i\\) values is the expected value of the true class \\(t\\), which in this case is 1. Since we know the assigned ratings, we can compare ratings to the \\(t_i\\) estimate. Intuitively, if accuracy is high, we’d expect to see more Class 1 ratings when \\(t_i\\) is near 1, and more Class 0 ratings when \\(t_i\\) is near 0."
  },
  {
    "objectID": "tapmodel.html#the-maximization-step",
    "href": "tapmodel.html#the-maximization-step",
    "title": "Chapter 2: The t-a-p Model",
    "section": "7.2 The Maximization Step",
    "text": "7.2 The Maximization Step\nIf we are given the (approximate) true values for the class of each subject, we can (approximately) evaluate the fraction of accurate ratings, since we have all the information we need. The likelihood of the t-a-p model will be maximized when the values of \\(a\\) and \\(p\\) also produce those rates. For example, suppose the true classes are split evenly between Class 1 and Class 0. If we compare these to the assigned ratings and find a perfect match, we know that \\(a=1\\). If the ratings look like coin flips, the pattern can be explained with \\(a=0\\) and \\(p=1\\). The point of using maximum likelihood estimators was to find the parameter values that best fit these rates, informally. Here we’ll use that logic in the other direction, arguing that by fitting the parameter to the rates induced from the E-step, we’re maximizing likelihood.\nThe first entry of the confusion matrix is true positive proportion (TPP), where ratings match true values for Class 1 cases 1. The false positive proportion (FPP) is the fraction of ratings of Class 1 when the underlying class is actually Class 0. Since the true classes are latent parameters, we can’t directly calculate TPP or FPP, but given our \\(t_i\\) estimates, we can approximate the values with\n\\[\n\\begin{aligned}\n\\text{TPP} &= \\frac{1}{N} \\sum_{ij}T_i C_{ij} \\approx \\frac{1}{N} \\sum_{ij}t_i C_{ij} \\\\\n\\text{FPP} &= \\frac{1}{N} \\sum_{ij}\\overline{T}_i C_{ij} \\approx \\frac{1}{N} \\sum_{ij}\\bar{t}_i C_{ij} \\\\\n\\text{TNP} &= \\frac{1}{N} \\sum_{ij}\\overline{T}_i \\overline{C}_{ij} \\approx \\frac{1}{N} \\sum_{ij} \\bar{t}_i \\overline{C}_{ij} \\\\\n\\text{FNP} &= \\frac{1}{N} \\sum_{ij}T_i \\overline{C}_{ij} \\approx \\frac{1}{N} \\sum_{ij}t_i \\overline{C}_{ij},\n\\end{aligned}\n\\]\nwhere \\(i\\) indexes subjects and \\(j\\) indexes raters, \\(C_{ij}\\) is the rating of the \\(j\\)th rater for the \\(i\\)th subject, and \\(N\\) is the total number of ratings. With the definitions on the right, we have\n\\[\n\\begin{aligned}\nt & \\approx \\sum t_i =  \\text{TPP} + \\text{FNP} \\\\\n\\bar{t} & \\approx \\text{FPP} + \\text{TNP} \\\\\nc &= \\frac{1}{N} \\sum C_{ij} = \\text{TPP} + \\text{FPP} \\\\\n\\bar{c} &= \\text{TNP} + \\text{FNP}.\n\\end{aligned}\n\\tag{5}\\]\nThe t-a-p model is a model of ratings, so we can write another expression for these rates that come from the model parameters, computing the expected values from the conditional probabilities, with\n\\[\n\\begin{aligned}\nE[\\text{TPP}] &= ta + t\\bar{a}p  \\\\\nE[\\text{FPP}] &= \\bar{t} \\bar{a}p \\\\\nE[\\text{TNP}] &= \\bar{t}a + \\bar{t}\\bar{a}\\bar{p}  \\\\\nE[\\text{FNP}] &= t \\bar{a}\\bar{p} \\\\\n\\end{aligned}\n\\]\nIf we approximate \\(t\\) from the average of \\(t_i\\), then we have approximate values for the left side of each equation as well as the \\(t\\) parameter, allowing us to solve for the other two parameters, \\(a\\) and \\(p\\) via\n\\[\n\\begin{aligned}\na &= \\frac{\\text{TPP}}{t} - \\frac{\\text{FPP}}{\\bar{t}} \\\\\np &= \\frac{\\text{FPP}}{\\bar{t}\\bar{a}}.\n\\end{aligned}\n\\]\nThis derivation uses the TPP and FPP, but we could use TNP and FNP instead, and get the same result. This can be seen from the relations in Equation 5. Note that if we have separate accuracy parameters \\(a_0\\) and \\(a_1\\) for Class 0 and Class 1, we could alter the EM altgorithm accordingly.\nWe estimate \\(a\\) and \\(p\\) with this M-step.\n\n\nShow the code\nt &lt;- mean(e_step$t_i)\n\nm_step &lt;- ratings |&gt;\n      left_join(e_step, by = \"subject_id\") |&gt;\n      summarize(tpp = mean(t_i*rating),\n                fpp = mean((1-t_i)*rating)) |&gt;\n      mutate(\n        a = mean(tpp/t - fpp/(1-t)), # solve for a\n        p = mean(fpp/(1-t)/(1-a))) # solve for p\n\nm_step |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 3: M-step results for sample ratings\n\n\n\n\n\n\ntpp\nfpp\na\np\n\n\n\n\n0.52\n0.07\n0.64\n0.51"
  },
  {
    "objectID": "tapmodel.html#likelihood-maximization",
    "href": "tapmodel.html#likelihood-maximization",
    "title": "Chapter 2: The t-a-p Model",
    "section": "7.3 Likelihood Maximization",
    "text": "7.3 Likelihood Maximization\nI skipped over an important bit above, in assuming that the shortcut approximation of \\(a\\) and \\(p\\) were the maximum likelihood estimates. It’s useful to spell out what that means. The provisional likelihood \\(\\ell^*\\) when given values for \\(t_i\\) can be written as\n\\[\n\\begin{aligned}\n\\ell^* &= \\sum_{i,j} \\log Pr[R_{ij}, T_i; a, p]\\\\\n&= \\sum_{i,j} R_{ij} T_i \\log(a+\\bar{a}p) + \\sum_{i,j} R_{ij} \\overline{T_i} \\log(\\bar{a}p) \\\\\n&+ \\sum_{i,j} \\overline{R}_{ij} T_i \\log(\\bar{a}\\bar{p}) + \\sum_{i,j} \\overline{R}_{ij} \\overline{T_i} \\log(a+\\bar{a}\\bar{p}) ,\n\\end{aligned}\n\\]\nwhere \\(t\\) is the average over all the \\(T_i\\) values, and as usual \\(a\\) is the average accuracy, and \\(p\\) is the average guessing probability. In practice, I’ll use the mean instead of sum, so that log likelihood is in units of bits per rating.\nThis represents the log probability of all the ratings of Class 1. The first term is the true positives, where the rating matches the true class, due to either accurate ratings or guessing. The second term is the false positives, where the rating is Class 1 but the true class is Class 0, due to guessing.\nThe sums are just the TPP, etc. proportions defined earlier, except they aren’t divided by the number of ratings, so they are just counts, e.g.\n\\[\nTP := \\sum_{i,j} R_{ij} T_i \\quad \\text{and} \\quad FP := \\sum_{i,j} R_{ij} \\overline{T_i}.\n\\]\nThe other two cases, for false negatives and true negatives are defined similarly. We can get the rate TPR by dividing by the number of ratings \\(N\\).\nUpon differentiating and dividing by the number of ratings \\(N\\) (to convert from TP to TPP, etc.), we obtain\n\\[\n\\begin{aligned}\n\\frac{1}{N} \\frac{\\partial \\ell^*}{\\partial a} &= TPP \\frac{\\bar{p}}{a+\\bar{a}p} - FPP \\frac{1}{\\bar{a}} - FNP \\frac{1}{\\bar{a}}  + TNP \\frac{p}{a + \\bar{a}\\bar{p}}\\\\\n\\frac{1}{N} \\frac{\\partial \\ell^*}{\\partial p} &= FPP \\frac{1}{p} + TPP \\frac{\\bar{a}}{a+\\bar{a}p} - FNP \\frac{1}{\\bar{p}}  - TNP \\frac{\\bar{a}}{a + \\bar{a}\\bar{p}} \\, ,\\\\\n\\end{aligned}\n\\]\nwhere TPP, FPP, etc. are the approximate values for the confusion matrix derived from the E-step discussed above. These two functions will be set to zero to find the critical value by solving for \\(a\\) and \\(p\\). In the shortcut derivation, we used the fact that for the true values of \\(t\\), \\(a\\), and \\(p\\), we will have \\(E[TPP] = ta + t\\bar{a}p\\), \\(E[FPP] = \\bar{t}\\bar{a}p\\), and so on. If we substitute these into the derivatives, it’s easy to see that they are in fact zero."
  },
  {
    "objectID": "tapmodel.html#implementation",
    "href": "tapmodel.html#implementation",
    "title": "Chapter 2: The t-a-p Model",
    "section": "7.4 Implementation",
    "text": "7.4 Implementation\nThe code below implements the EM algorithm as described above and plots the convergence of the parameters.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(tapModel)\n\nap_log_likelihood &lt;- function(params, ratings, confusion) {\n  a &lt;- params[1]\n  p &lt;- params[2]\n  \n  a_ &lt;- 1 - a\n  p_ &lt;- 1 - p\n  \n  ll = confusion$TP*log(a + a_*p ) +\n    confusion$FP*log(a_*p) +\n    confusion$TN*log(a + a_*p_) +\n    confusion$FN*log(a_*p_)\n  \n  if(is.nan(ll) | is.infinite(ll)) ll &lt;- -1e6\n  \n  \n  return(-ll/log(2))  # Return negative for minimization\n}\n\nset.seed(123) # for reproducibility\nratings &lt;- tapModel::generate_sample_ratings(\n  N_s = 300,\n  N_r = 50,\n  params = list( t = .3,  a = .2, p = .8),\n  details = TRUE # includes the latent variables in the output\n)\n# direct optimization\n\nt &lt;- .5\na &lt;- .5\np &lt;- .5\n\n# the average parameters are the closest to \"true\" values of the params\navg_param &lt;- data.frame(\n  t = mean(ratings$T_i),\n  a = mean(ratings$A_ij),\n  p =  mean(ratings$P_ij))\n\nn_steps = 25\nestimates &lt;- data.frame(step = 1:n_steps, t = NA, a = NA, p = NA) |&gt;\n  mutate(ll = NA, method = \"TPP\")\n\nfor(i in 1:n_steps) {\n  # both methods use the same E step\n  e_step &lt;- ratings |&gt;\n    group_by(subject_id) |&gt;\n    summarize(\n      N_r = n(), # number of raters\n      R_i = sum(rating), # number of Class 1 ratings\n      t_i = (t * dbinom(R_i , N_r, a + (1-a)*p)) /\n        (t * dbinom(R_i , N_r, a + (1-a)*p) +\n           (1-t) * dbinom(R_i , N_r, (1-a)*p)))\n\n  # summary statistics from the E step\n  t &lt;- mean(e_step$t_i)\n\n  # add column with provisional t_i values\n  m_ratings &lt;- ratings |&gt;\n    left_join(e_step, by = \"subject_id\")\n\n  # confusion matrix of proportions \n  confusion &lt;- m_ratings |&gt;\n    summarize(TP = mean(rating*t_i),\n              FP = mean(rating*(1-t_i)),\n              TN = mean((1-rating)*(1-t_i)),\n              FN = mean((1 - rating)*t_i))\n\n  # method = TPR ################################\n  m_step &lt;- confusion |&gt;\n    mutate(\n      a = TP/t - FP/(1-t), # solve for a\n      p = FP/(1-t)/(1-a)) # solve for p\n\n\n  a &lt;- m_step$a\n  p &lt;- m_step$p\n\n  estimates$t[i] &lt;- t\n  estimates$a[i] &lt;- a\n  estimates$p[i] &lt;- p\n  estimates$ll[i] &lt;- ap_log_likelihood(c(a, p), ratings, confusion)\n\n}\n\nestimates |&gt;\n  gather(param, value, -step, -method) |&gt;\n  ggplot(aes(x = step, y = value, color = param, group = param)) +\n  geom_line() +\n  geom_hline(aes(yintercept = value, color = param, group = param),\n             linetype = \"dashed\",\n             data = avg_param |&gt; gather(param, value)) +\n  theme_bw() +\n  facet_wrap(~method, scales = \"free_y\")\n\n\n\n\n\n\n\n\nFigure 6: The EM method for estimating t, a, and p parameters where t = .3, a = .2, p = .8 with 300 subject rated 50 times each. Dotted lines show the averages of the indicator variables T, A, and P as generated by the rating simulation. The green line shows the log likelihood in bits per rating.\n\n\n\n\n\nThe optimization problem in Figure 6 is hard, since the accuracy is only .2, and it takes about 20 E-M steps to reach convergence. The dashed lines show the average values of \\(T_i\\), \\(A_{ij}\\), and \\(P_{ij}\\), which are the empirical values of the parameters that avoid sampling error. The bits per rating (log likelihood) is in green. The final estimates are\n\n\nShow the code\nestimates |&gt; \n  filter(step == n_steps) |&gt; \n  select(-step, -ll, -method) |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 4: Final estimates of t, a, and p parameters after 25 iterations\n\n\n\n\n\n\nt\na\np\n\n\n\n\n0.27\n0.21\n0.81\n\n\n\n\n\n\n\n\nThe tapModel library function fit_counts() uses a quasi-Newton method to simultaneously optimize the three t-a-p parameters with maximum likelihood. It reaches the same solution as the EM algorithm.\n\n\nShow the code\n# direct optimization\nratings |&gt; \n  as_counts() |&gt; \n  fit_counts() |&gt; \n  select(t, a, p) |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 5: Direct optimization results for sample ratings\n\n\n\n\n\n\nt\na\np\n\n\n\n\n0.27\n0.21\n0.81"
  },
  {
    "objectID": "tapmodel.html#footnotes",
    "href": "tapmodel.html#footnotes",
    "title": "Chapter 2: The t-a-p Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m calling these proportions instead of rates to distinguishing from the usual way of talking about, for example, the true positive rate (TPR), which is the fraction of true Class 1 cases that are rated as Class 1, so the denominator is not all observations, just the true Class 1 cases. For the TPP, it’s the fraction of all ratings that are true Class 1 cases that are also classified as Class 1, so the denominator is all ratings.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Kappa Zoo",
    "section": "",
    "text": "The goal of this project is to unify and extend research on classification, in particular the statistics used to assess rater agreement or classification accuracy. For example a panel of wine judges independently rates a series of vintages. This type of data is widespread in consumer data (product ratings), medicine (diagnostics, trials), education (evaluation), social science (surveys), and machine learning context (text annotation). The commonality between these is a set of raters (evaluators, annotators) who independently assign categories to subjects, usually from a small number of choices.\nThe most basic question about such data sets is “is it just random numbers?” This can be answered by comparing the variation of responses within each subject to the variation between subjects. There are multiple research methods that can be applied, such as ANOVA, Item Response Theory, and other continuous-scale approaches, but the focus here is methods that directly or indirectly use exact rater agreement as the basis for analysis. There are two distinct research traditions in rater agreement. One is from social sciences and medicine that has proliferated a number of rater agreement statistics, each with different assumptions. These “kappas” prompted the title of the website, since there is by now a “zoo” of these statistics to choose from. On a parallel track more associated nowadays with Machine Learning, there has been development of model-based classification statistics (e.g. MACE) that embody a tree-like set of conditional probabilities used to generate (often) hierarchical parameters for the subjects being classified and the raters who assign the classes to the subjects.\nThe body of work here includes statistical derivations, an R library for implementing them, and examples of use. It’s intended to be particularly useful for those who are still scratching their heads about which kappa statistic to use. The answer is: don’t. There’s a better way to approach the problem.\n\n\nIf you are new to t-a-p models, I recommend starting with the Chapter 1, which introduces rater agreement as a classification problem and lays out the philosophical and statistical assumptions. Chapter 2 provides the basic statistical derivations needed to work with t-a-p models. Chapter 3 derives the relationship to some existing rater agreement statistics, for example showing how the Fleiss kappa is a special case of a t-a-p model. Chapter 4expands the number of parameters and shows how this relates to the “kappa paradox.” Chapter 5 develops a hierarchical model that allows each rater and subject to have individual parameters for accuracy and truth. This model is equivalent to a binary version of MACE, from the machine learning literature (Hovy et al., 2013). Chapter 6 shows how we can assess model fit and the independence assumption. Chapter 7 introduces the tapModel R package, which allows for easy estimation of the model parameters and lots of utilities. Chapter 8 introduces the t-a-p app, an interactive way to use real or simulated data to estimate the model parameters without coding. It is currently under construction, since I’ve made major changes to the tapModel package. An appendix is included to provide more detailed statistical derivations and proofs for some results. There is a growing list of examples that use data sets to illustrate the concepts.\n\n\n\n\nChapter 1: Quantifying Agreement\nChapter 2: The t-a-p Model\nChapter 3: Kappa Statistics\nChapter 4: The Kappa Paradox\nChapter 5: Hierarchical Models\nChapter 6: Assessing Model Fit\nChapter 7: The tapModel R Package\nChapter 8: The t-a-p App\nReferences\nExamples\n\nSimulated Uniform Raters\nPTSD Assessment\nWine ratings\nNo Vehicles in the Park\n\nAppendix A: Statistical Details"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Welcome to the Kappa Zoo",
    "section": "",
    "text": "If you are new to t-a-p models, I recommend starting with the Chapter 1, which introduces rater agreement as a classification problem and lays out the philosophical and statistical assumptions. Chapter 2 provides the basic statistical derivations needed to work with t-a-p models. Chapter 3 derives the relationship to some existing rater agreement statistics, for example showing how the Fleiss kappa is a special case of a t-a-p model. Chapter 4expands the number of parameters and shows how this relates to the “kappa paradox.” Chapter 5 develops a hierarchical model that allows each rater and subject to have individual parameters for accuracy and truth. This model is equivalent to a binary version of MACE, from the machine learning literature (Hovy et al., 2013). Chapter 6 shows how we can assess model fit and the independence assumption. Chapter 7 introduces the tapModel R package, which allows for easy estimation of the model parameters and lots of utilities. Chapter 8 introduces the t-a-p app, an interactive way to use real or simulated data to estimate the model parameters without coding. It is currently under construction, since I’ve made major changes to the tapModel package. An appendix is included to provide more detailed statistical derivations and proofs for some results. There is a growing list of examples that use data sets to illustrate the concepts."
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "Welcome to the Kappa Zoo",
    "section": "",
    "text": "Chapter 1: Quantifying Agreement\nChapter 2: The t-a-p Model\nChapter 3: Kappa Statistics\nChapter 4: The Kappa Paradox\nChapter 5: Hierarchical Models\nChapter 6: Assessing Model Fit\nChapter 7: The tapModel R Package\nChapter 8: The t-a-p App\nReferences\nExamples\n\nSimulated Uniform Raters\nPTSD Assessment\nWine ratings\nNo Vehicles in the Park\n\nAppendix A: Statistical Details"
  },
  {
    "objectID": "index.html#unification",
    "href": "index.html#unification",
    "title": "Welcome to the Kappa Zoo",
    "section": "2.1 Unification",
    "text": "2.1 Unification\nIt turns out that the legacy kappa statistics are special cases of the probability-tree statistics. We can see this common link by starting with a simple probability tree for binary classification with three parameters: \\(t\\) is the prevalence of Class 1 (as opposed to Class 0), \\(a\\) is the rater accuracy, and \\(p\\) is a probability for inaccurate assignments. This tree approach imitates the Justified True Belief model of knowledge, and naturally incorporates Gettier problems (getting the right answer for the wrong reason).\nApplying this t-a-p model to the Fleiss Kappa, one of the most well-known of the rater agreement statistics, shows that the kappa is a special case with the assumption that \\(t = p\\), meaning that when raters make mistakes, they at least retain the distribution of the correct class proportions. I call this the unbiased case."
  },
  {
    "objectID": "index.html#surprises",
    "href": "index.html#surprises",
    "title": "Welcome to the Kappa Zoo",
    "section": "2.2 Surprises",
    "text": "2.2 Surprises\nThe first aha moment was finding that if the t-a-p model’s \\(t=p\\) condition is met, the Fleiss kappa is just the square of accuracy. Another unexpected result was that when rater accuracy approaches zero, expected estimates of rater accuracy increase. I called this the Dunning-Kruger effect, as a tongue-in-cheek reference to the social science research on meta-ignorance. At first I thought this was probably just estimation error, since lower accuracy means noisier data. However, I was able to find an exact closed formula for accuracy when \\(t=p\\), and show that it’s a real effect: when accuracy is low enough, the chances of an overestimate increase. Weird, right?\nIt was surprising to me that not only can the three parameter t-a-p model be estimated with a simple Expectation-Maximization algorithm, but that the hierarchical version can be as well. It was also surprising that the ad hoc procedure I put together for doing this was, in fact, a provable E-M algorithm.\nA recurrent theme in the literature was the suggestion that we needed more coefficients to handle the possibilities in a 2x2 confusion matrix (assigned class versus true class). It sounds odd to say that raters are more accurate when classifying Class 1 cases than Class 0 cases, but it makes sense when we take into account the pattern of false positives and false negatives. This approach–separate accuracies for classes–gives a satisfying explanation of the so-called Kappa Paradox concerning unbalanced data.\nThe models can be fit with E-M algorithms or Bayesian MCMC methods, both of which maximize likelihood. It turns out that log likelihood has a natural interpretation as bits of entropy per rating, which is a nice index for assessing model fit."
  },
  {
    "objectID": "index.html#todo",
    "href": "index.html#todo",
    "title": "Welcome to the Kappa Zoo",
    "section": "2.3 Todo",
    "text": "2.3 Todo\nThis is an ongoing project. A future chapter will be devoted to the connection between these rater statistics and causality measures, which is how this project actually started (Eubanks, 2014). The larger issue is to understand the strengths and weaknesses of MCMC estimation for the hierarchical models, e.g. how pooling should work. When all that’s done, I’ll start working through the collection of data samples I’ve gathered over the years to create a catalog of cases. If you’d like to contribute to any of this, email me."
  },
  {
    "objectID": "Example_wine.html",
    "href": "Example_wine.html",
    "title": "Example: Wine Ratings",
    "section": "",
    "text": "Show the code\nlibrary(tidyverse)\nlibrary(tapModel)\nlibrary(LaplacesDemon) # for mode and rbern\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(cowplot)\n\ndata(wine)\n\n# ordinal data sets corresponding to the repeated measures design\nwine1 &lt;- wine |&gt; \n         filter(trial == 1)\n\nwine2 &lt;- wine |&gt; \n         filter(trial == 2)\n\n\n\n1 Example: Wine Ratings\nThe wine ratings came from private correspondence with the author of Hodgson (2008). Here’s the abstract of the article.\n\nWine judge performance at a major wine competition has been analyzed from 2005 to 2008 using replicate samples. Each panel of four expert judges received a flight of 30 wines imbedded with triplicate samples poured from the same bottle. Between 65 and 70 judges were tested each year. About 10 percent of the judges were able to replicate their score within a single medal group. Another 10 percent, on occasion, scored the same wine Bronze to Gold. Judges tend to be more consistent in what they don’t like than what they do. An analysis of variance covering every panel over the study period indicates only about half of the panels presented awards based solely on wine quality.\n\nThe data comes from a wine-judging event, where (p. 106)\n\n[F]our triplicate samples [are] served to 16 panels of judges. A typical flight consists of 30 wines. When possible, triplicate samples of all four wines were served in the second flight of the day randomly interspersed among the 30 wines. A typical day’s work involves four to six flights, about 150 wines. Each triplicate was poured from the same bottle and served on the same flight. The overriding principle was to design the experiment to maximize the probability in favor of the judges’ ability to replicate their scores. The judges first mark the wine’s score independently, and their scores are recorded by the panel’s secretary. Afterward the judges discuss the wine. Based on the discussion, some judges modify their initial score; others do not. For this study, only the first, independent score is used to analyze an individual judge’s consistency in scoring wines.\n\nI only have a part of the total data set, comprising 1,464 ratings by four judges of 183 wines. Each judge rated each wine twice (blind), and this is annotated with a “trial” column. The ratings are taken independently, except for the question of how to treat the repeated measures. One way to look at this data set is to think of it as two (almost) independent data sets with the same subjects and raters.\nThe original analysis in Hodgson (2008) uses a variance-based method, which is a powerful way to describe the contributions of raters, subjects, etc. to the variance in the ratings. The probability of identical repeated measures (same rater and subject resulting in the)\nThe judges’ award system is based on a score from 80 to 100, which on advice from the author, has the following encoding.\n\n\n\nScore Range\nRating Value\nMeaning\n\n\n\n\n80\n1\nNo medal\n\n\n82-86\n2\nBronze medal\n\n\n88-92\n3\nSilver medal\n\n\n94-100\n4\nGold medal\n\n\n\nThere are three natural cut-points:\n\n1|2 divides the 1 ratings from higher ones\n2|3 divides the {1,2} ratings from the {3,4} ratings\n3|4 divides the {1,2,3} ratings from the 4 ratings.\n\nEach cut point defines a binary classification system, which we can estimate the t-a-p parameters for. Note that Class 1 is always identified with the lower (left) category set.\n\n\nShow the code\nwine |&gt; \n  ggplot(aes(x = rating)) +\n  geom_bar(fill = \"steelblue\") +\n  theme_bw() +\n  facet_grid(. ~ trial)\n\n\n\n\n\n\n\n\nFigure 1: Distribution of ratings\n\n\n\n\n\nThe counts of ratings over both trials shows that there were significantly more ratings of 1 in the first trial. The general question of interest here is whether or not we can consider the two trials to be independent draws from a common distribution, or if there was some change between the two trials. The design was intended to minimize the latter factor. Another measure of variability is the difference in ratings per vintage across the two trials by rater.\n\n\nShow the code\nwine |&gt; \n    spread(trial, rating) |&gt; \n  mutate(Difference = `2` - `1`) |&gt; \n  ggplot(aes(x = Difference)) +\n  geom_bar(fill = \"steelblue\") +\n  theme_bw() +\n  facet_grid(. ~ rater_id )\n\n\n\n\n\n\n\n\nFigure 2: Difference in ratings between trials by judge.\n\n\n\n\n\nFigure 2 shows that most differences in subject average rating (trial 2 minus trial 1) were small, and that the ratings tended to increase for trial 2. Note, however, that the two trials were interwoven, not sequential, so I don’t think we can say that judges became more lenient over time.\n\n\n2 Independence\nCan we just combine the two trials as one data set? The basic t-a-p model doesn’t have an extra accommodations for multiple ratings of a subject by the same rater. These would be treated like any other rating, assumed independent of all other ratings. We can test for independence with the covariance test, to see if combining the trials runs into a problem there. As an example, I’ll use the 1|2 cut point, so that ratings of 1 are Class 1 and ratings of 2-4 are Class 0.\n\n\nShow the code\n# trial 1\ncutpoint = 1\n\nrating_params1 &lt;- wine1 |&gt; \n        as_binary_ratings(cutpoint) |&gt; \n        fit_ratings() \n\n# get rater covariances\ncov1 &lt;- rating_params1 |&gt; \n        tapModel::rater_cov()\n\n# get predicted range of covariance from trial 1\nrange1 &lt;- tapModel::rater_cov_quantile(rating_params1)\n\n# trial 2\nrating_params2 &lt;- wine2 |&gt; \n  as_binary_ratings(cutpoint) |&gt; \n  fit_ratings() \n\n# get rater covariances\ncov2 &lt;- rating_params2 |&gt; \n        tapModel::rater_cov()\n\n# get predicted range of covariance from trial 2\nrange2 &lt;- tapModel::rater_cov_quantile(rating_params2)\n\n# average these \nrange &lt;- (range1 + range2)/2\n\n# now all together\nwine |&gt; \n  # make the second trial look like different raters\n  mutate(rater_id = rater_id + (trial - 1)*10) |&gt; \n  as_binary_ratings(cutpoint) |&gt; \n  fit_ratings()  |&gt; \n  rater_cov() |&gt; \n  plot_rater_cov(range, bins = 20)\n\n\n\n\n\n\n\n\nFigure 3: Covariances of raters on the whole data set, with expected range marked.\n\n\n\n\n\nThe outliers in Figure 3 suggest that treating repeat ratings as independent is a bad assumption. The approach here seems crude compared to an ANOVA, which can precisely parcel out contributions to variance, but the covariance plot is more general in that it might spot non-independence that we can’t identify the cause of. This shouldn’t be surprising given the obvious self-consisantcy of ratings in Figure 1.\nIt seems best to either consider the two trials as draws from a distribution, or combine them in a way that avoids violating independence.\n\n\n3 Ordinal Parameters\nThere’s a helper function in fit_ordinal_tap(ratings) for ordinal scales, which you can see employed in the code below. The idea is to convert the ordinal scale into a sequence of binary scales by using cut points between the rating values. A cut point of 2|3, for example, separates the 1 and 2 ratings into Class 1 and the 3 and 4 ratings into Class 0.\n\n\nShow the code\nwine1_params &lt;- fit_ordinal_tap(wine1) |&gt; \n                filter(type == \"t-a-p\") |&gt; \n                mutate(trial = 1) \n\nwine2_params &lt;- fit_ordinal_tap(wine2) |&gt; \n                filter(type == \"t-a-p\") |&gt; \n                mutate(trial = 2) \n\nrbind(wine1_params, wine2_params) |&gt; \n  select(trial, CutPoint, t, a, p, ll) |&gt; \n  mutate(rll = expected_bits_per_rating(params = list(t = .5, a = a, p = p))) |&gt; \n  kable(digits = 2, format = \"html\", escape = FALSE) |&gt; \n  kable_styling(\n    full_width = FALSE,           # &lt;- stops spanning the full page\n    position   = \"center\",\n    bootstrap_options = c(\"striped\",\"hover\",\"condensed\")\n  )\n\n\n\n\nTable 1: Ordinal coefficients for rating cut points, where ll is expected log likelihood given the parameters, and rll is the same but with t = .5 to represent balanced classes, isolating the entropy from just the raters.\n\n\n\n\n\n\ntrial\nCutPoint\nt\na\np\nll\nrll\n\n\n\n\n1\n1|2\n0.26\n0.54\n0.22\n0.59\n0.70\n\n\n1\n2|3\n0.54\n0.44\n0.59\n0.84\n0.85\n\n\n1\n3|4\n0.71\n0.26\n0.94\n0.45\n0.58\n\n\n2\n1|2\n0.27\n0.60\n0.09\n0.42\n0.59\n\n\n2\n2|3\n0.59\n0.60\n0.38\n0.73\n0.71\n\n\n2\n3|4\n0.87\n0.59\n0.84\n0.43\n0.64\n\n\n\n\n\n\n\n\nThe parameter estimates Table 1 are not stable between the two trials, which casts doubt on the model fit generally. There’s either some source of variance not accounted for, or the sampling assumptions aren’t being met.\n\n\nShow the code\nplot_results &lt;-  vector(\"list\", 6)\n\nfor(i in 1:3){ # cut point\n  for(j in 1:2){ # trial \n\n      temp_ratings &lt;- wine |&gt; \n        filter(trial == j) |&gt; \n        as_binary_ratings(1:i)\n      \n      temp_params &lt;- temp_ratings |&gt; \n        as_counts() |&gt; \n        fit_counts()\n      \n      temp_plot&lt;- temp_ratings |&gt; \n        as_rating_params(temp_params) |&gt; \n        subject_calibration()\n      \n      plot_results[[i + (j-1)*3]] &lt;- temp_plot\n  }\n}\n\ntitle1 &lt;- ggplot() + ggtitle(\"Trial 1\") + theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\ntitle2 &lt;- ggplot() + ggtitle(\"Trial 2\") + theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\ncowplot::plot_grid(title1, title2, \n                   plot_results[[1]], plot_results[[4]],\n                   plot_results[[2]], plot_results[[5]], \n                   plot_results[[3]], plot_results[[6]],\n                   nrow = 4, ncol = 2, \n                   labels = c(NA, NA, \"1|2\",\"1|2\",\"2|3\",\"2|3\",\"3|4\",\"3|4\"),\n                    rel_heights = c(0.2, 1,1,1),\n                   label_size = 12)\n\n\n\n\n\n\n\n\nFigure 4: Subject calibration for the three cut points. Trial 1 is in the left column, with cut points 1|2 through 3|4. Trial 2 is in the right column with same cut points.\n\n\n\n\n\nThe subject calibrations in Figure 4 is quite good for all except for the 2|3 cut point for trial 2, which is a poor fit.\n\n\n4 Combined Data\nIn this data we have a direct way to assess rater accuracy, by comparing ratings of the same subjects between trials. We saw in Figure 1 that a high proportion of raters found the same classification between trials. This could be either because of reliability in the rating assignments, or a violation of independence whereby raters recognize a wine they’ve tasted before. I don’t know how likely that is, but it’s conceivable that if a rater can identify a vintage, they might assign a rating from memory.\nTo attempt to improve data quality it seems reasonable to just keep cases where raters agree with themselves on a particular wine, and discard ratings where they disagree with themselves. This imagines a new protocol for the wine judging, where after the two trials, any ratings where an individual judge didn’t agree with him/herself is tossed out. Since we are only taking one rating per judge-wine combination, we shouldn’t have an independence problem.\n\n\nShow the code\nwine_clean &lt;- wine |&gt; \n              spread(trial, rating) |&gt; \n              filter(`1` == `2`) |&gt; \n              select(subject_id, rating = `1`, rater_id)\n\n\n\nwine_clean_params &lt;- fit_ordinal_tap(wine_clean) |&gt; \n                     filter(type == \"t-a-p\") \n\nwine_clean_params  |&gt; \n  select(CutPoint, t, a, p, ll) |&gt; \n  mutate(rll = expected_bits_per_rating(params = list(t = .5, a = a, p = p))) |&gt; \n  kable(digits = 2, format = \"html\", escape = FALSE) |&gt; \n  kable_styling(\n    full_width = FALSE,           # &lt;- stops spanning the full page\n    position   = \"center\",\n    bootstrap_options = c(\"striped\",\"hover\",\"condensed\")\n  )\n\n\n\n\nTable 2: Ordinal coefficients for rating cut points, where ll is expected log likelihood given the parameters, and rll is the same but with t = .5 to represent balanced classes, isolating the entropy from just the raters.\n\n\n\n\n\n\nCutPoint\nt\na\np\nll\nrll\n\n\n\n\n1|2\n0.28\n0.62\n0.10\n0.42\n0.58\n\n\n2|3\n0.64\n0.63\n0.22\n0.70\n0.64\n\n\n3|4\n0.90\n0.72\n0.63\n0.50\n0.57\n\n\n\n\n\n\n\n\nGenerally, Table 2 shows higher accuracy and lower entropy. Let’s look at the subject calibration.\n\n\nShow the code\nplot_results &lt;-  vector(\"list\", 6)\n\nfor(i in 1:3){ # cut point\n  for(j in 1:2){ # type\n    \n    temp_ratings &lt;- wine_clean |&gt; \n      as_binary_ratings(1:i)\n    \n    if(j == 1){ # average model\n      temp_params &lt;- temp_ratings |&gt; \n        as_counts() |&gt; \n        fit_counts()\n      \n      temp_plot&lt;- temp_ratings |&gt; \n        as_rating_params(temp_params) |&gt; \n        subject_calibration()\n    } else { # hierarchical model\n      \n      temp_plot&lt;- temp_ratings |&gt; \n        fit_ratings() |&gt; \n        subject_calibration()\n      \n    }\n    \n    plot_results[[i + (j-1)*3]] &lt;- temp_plot\n  }\n}\n\ntitle1 &lt;- ggplot() + ggtitle(\"Average\") + theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\ntitle2 &lt;- ggplot() + ggtitle(\"Hierarchical\") + theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\ncowplot::plot_grid(title1, title2, \n                   plot_results[[1]], plot_results[[4]],\n                   plot_results[[2]], plot_results[[5]], \n                   plot_results[[3]], plot_results[[6]],\n                   nrow = 4, ncol = 2, \n                   labels = c(NA, NA, \"1|2\",\"1|2\",\"2|3\",\"2|3\",\"3|4\",\"3|4\"),\n                   rel_heights = c(0.2, 1,1,1),\n                   label_size = 12)\n\n\n\n\n\n\n\n\nFigure 5: Subject calibration for the three cut points on the cleaned data. The average t-a-p model is in the left column, with cut points 1|2 through 3|4. The hierarchical model is in the right column.\n\n\n\n\n\nThe distributions in Figure 5 look better than the trials taken individually in Figure 4. We want “bumps” in the distribution, because that’s a sign of the binomial mixture separating the two means (meaning larger accuracy). Notice that the model fits the 2|3 case better here than in trial 2. Also, the average parameter estimates are significantly different, especially for the 3|4 cutpoint, where the bias (\\(p-t\\)) changes direction.\nGiven the significant gains in log likelihood, the reasonableness of individual variation by raters, and additional bonus of gaining rater and rating calibration, should we use the average model or the hierarchical one for the next step? This is a subjective decision, but it amounts to a choice about what we think is the most reasonable distribution of true ratings.\n\n\nShow the code\n# we need to estimate all the parameters for the three hierarchical models\nwine_clean_params &lt;- wine_clean_params |&gt; select(t, a, p)\n\nt_results &lt;- vector(\"list\", 3)\na_results &lt;- vector(\"list\", 3)\n\nfor(i in 1:3){\n   rating_params &lt;- wine_clean |&gt; \n         as_binary_ratings(in_class = 1:i) |&gt; \n         fit_ratings()\n  \n  t_results[[i]] &lt;- rating_params |&gt; \n         distinct(subject_id, .keep_all = TRUE) |&gt; \n         mutate(cutpoint = str_c(i,\"|\",i+1)) |&gt; \n         select(cutpoint, subject_id, t) \n  \n  a_results[[i]] &lt;- rating_params |&gt; \n                    distinct(rater_id, .keep_all = TRUE) |&gt; \n                    mutate(cutpoint = str_c(i,\"|\",i+1)) |&gt; \n                    select(cutpoint, rater_id, a,p) \n}\n\nt_results &lt;- bind_rows(t_results)\na_results &lt;- bind_rows(a_results)\n\nempirical &lt;- wine_clean |&gt; \n             count(rating) |&gt; \n             mutate(p = n/sum(n)) |&gt; \n             pull(p)\n\nt_avg &lt;- t_results |&gt; \n         group_by(cutpoint) |&gt; \n         summarize(t = mean(t)) |&gt; \n         pull(t)\n\naverage_model      &lt;- with(wine_clean_params, c(t[1], t[2] - t[1], t[3] - t[2], 1 - t[3]))\nhierarchical_model &lt;- c(t_avg[1], t_avg[2] - t_avg[1], t_avg[3] - t_avg[2], 1 - t_avg[3])\n\nt_compare &lt;- data.frame(rating = 1:4, empirical = empirical, average = average_model, hierarchical = hierarchical_model)\n\nt_compare |&gt; \n  gather(type, value, -rating) |&gt; \n  ggplot(aes(x = rating, y = value, group = type, color = type)) +\n  geom_line() +\n  geom_point() +\n  theme_bw() +\n  ylim(0,NA)\n\n\n\n\n\n\n\n\nFigure 6: Rating distributions, comparing the average model, hierarchical model, and empirical ratings after cleaning.\n\n\n\n\n\nWe can think of Figure 6 as a kind of scale calibration. Compare the average model and empirical rating distributions (red and green). There, the model is asking us to believe that the ratings are slightly inflated; there should be somewhat more ratings of 1 and 2 and fewer of 3 and 4. The hierarchical model, by contrast wants to turn the distribution upside down, saying that both the 1s and 4s are underrepresented in the actual ratings. Subjectively (to me) this seems unreasonable, and is probably a sign of overfitting the data with too many parameters.\nAs a result of this intuition, I’ll use the average model for the rest of the analysis.\n\n\n5 Wine Ratings\nNow that we have a reasonable model of the ratings, we can ask about the qualities of the individual wines; what rating should we assign based on the model’s parameters in combination with the ratings? We can do that by using the E-step of the E-M algorithm to assign \\(t_i\\) ratings to each subject using the average parameter estimates. To assign ratings from the ensemble of three models is a bit awkward, and we might think about either a true categorical method like MACE or an explicit ordinal t-a-p model, which will require more development. I’ll stick with what we’ve got for now, using the average model to estimate \\(t_i\\) for each wine at each cut point. This is found in Figure 7.\n\n\nShow the code\nt_results &lt;- vector(\"list\", 3)\n\nfor(i in 1:3){\n  \n   rating_params &lt;- wine_clean |&gt; \n         as_binary_ratings(in_class = 1:i) |&gt; \n         as_rating_params(wine_clean_params[i,]) |&gt; \n         estimate_ti()\n  \n  t_results[[i]] &lt;- rating_params |&gt; \n         distinct(subject_id, .keep_all = TRUE) |&gt; \n         mutate(cutpoint = str_c(i,\"|\",i+1)) |&gt; \n         select(cutpoint, subject_id, t) \n  \n}\n\nt_results &lt;- bind_rows(t_results)\n\n# estimate rating probabilities\nrating_probs &lt;- t_results |&gt; \n                spread(cutpoint, t) |&gt; \n                mutate(`1` = `1|2`,\n                       `2` = `2|3` - `1|2`,\n                       `3` = `3|4` - `2|3`,\n                       `4` = 1 - `3|4`) |&gt; \n                 select(subject_id, `1`, `2`, `3`, `4`) |&gt; \n                 gather(rating, prob, -subject_id) \n\nraw_plot &lt;- rating_probs |&gt; \n  ggplot(aes(x = prob)) + \n  geom_histogram(fill = \"steelblue\", color = \"white\", bins = 20) +\n  theme_bw() +\n  xlab(\"Probability estimate for some rating.\")\n\nmax_plot &lt;- rating_probs |&gt; \n  group_by(subject_id) |&gt; \n  filter(prob == max(prob)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(x = prob)) +\n  geom_histogram(fill = \"steelblue\", color = \"white\", bins = 20) +\n  theme_bw() +\n  xlab(\"Probability estimate for some rating.\") +\n  xlim(0,1)\n\ncowplot::plot_grid(raw_plot, max_plot, labels = c(\"A\",\"B\"))\n\n\n\n\n\n\n\n\nFigure 7: Histogram of (A) all induced probabilities for each subject to have a given rating, and (B) the probability of the most likely rating.\n\n\n\n\n\nThe results in Figure 7 show the limitations of cobbling together three binary models instead of having a true ordinal model, e.g. some of the computed probabilities are negative in figure A. However, there are few of these, and the distribution of probabilities in (A) shows a clearly-defined mixture distribution, even though it complicated with a cluster around .5. The probabilities for the ratings 1-4 will not generally sum to one for a given subject in this ensemble model. However, we can still get some use out of the results by finding the most likely rating per subject, and then insisting on some level of estimated confidence. The histogram of probabilities for these most likely ratings (B) has most of the cases with 70% or more probability. I’ll use a threshold of 70% probability to classify a wine into a given rating. For subjects that lack that level of confidence, I’ll call it “complex” to indicate a lack of agreement.\n\n\nShow the code\nmodel_rating &lt;- rating_probs|&gt; \n                group_by(subject_id) |&gt; \n                filter(prob == max(prob)) |&gt; \n                ungroup() |&gt; \n                mutate(model_rating = if_else(prob &gt;= .7, str_c(rating), \"Complex\"))\n\nwine_stats &lt;- wine_clean |&gt; \n              left_join(model_rating |&gt; select(subject_id, model_rating)) |&gt; \n              group_by(model_rating) |&gt; \n              summarize(wines = n_distinct(subject_id),\n                        avg = mean(rating),\n                        sd  = sd(rating))\n  \n\nwine_stats |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 3: After classifying wines by the modeled probabilities, show the counts, average ratings, and standard deviation of ratings in each group. “Complex” means there wasn’t enough agreement to assign a rating using a 70% probability threshold.\n\n\n\n\n\n\nmodel_rating\nwines\navg\nsd\n\n\n\n\n1\n43\n1.33\n0.61\n\n\n2\n32\n2.39\n0.58\n\n\n3\n37\n3.01\n0.49\n\n\n4\n22\n3.80\n0.45\n\n\nComplex\n49\n2.41\n0.88\n\n\n\n\n\n\n\n\nThe figures in Table 3 show that most of the wines can be classifed by the model using the probability threshold, and that the rating averages look as expected. The standard deviation of the “complex” group is high, as we’d expect. In a real judging event, it’s probably not acceptable to classify so many vintages as unratable, but it would increase the validity of the ratings and might add an interesting element. What does it mean in practice for wine judges to find no agreement? It’s qualitatively different from the ordinal rating scale, but do we really expect a single dimension to capture the complexities of wine?\nI tried a dimensionality analysis using the singular value decomposition on the wine ratings by subject and by rater to see if there was a relationship between the singular vectors and the classification as a complex wine. The results were suggestive, but only if I squint and hope for the best. The connection between the factor 1 loading and the probabilities were not clear enough to include here. I thought that the complex wines might have loadings on the first component that were more extreme than the average, but I didn’t find that pattern. There are other types of dimensional analysis, which might lead down a forking path to an explanation.\n\n\n6 Evaluating Raters\nGiven the final model ratings, we can hope to assess the qualities of the raters using the statistics so far.\n\n\nShow the code\nmatch_rate &lt;- wine |&gt;   spread(trial, rating) |&gt; \n              group_by(rater_id) |&gt; \n              summarize(self_match = mean(`1` == `2`)) \n\nwine_clean |&gt; \n  left_join(model_rating |&gt; select(subject_id, model_rating)) |&gt; \n  filter(model_rating != \"Complex\") |&gt; \n  mutate(model_rating = as.integer(model_rating)) |&gt; \n  group_by(rater_id) |&gt; \n  summarize(rated_wines = n(),\n            model_match = mean(rating == model_rating),\n            rating_diff = mean(rating - model_rating)) |&gt; \n  left_join(match_rate) |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 4: Wine judge statistics, showing the number of non-complex wines rated, their match rate with the model, the average rating - model_rating difference, and their self-match rate in the full data set with two trials.\n\n\n\n\n\n\nrater_id\nrated_wines\nmodel_match\nrating_diff\nself_match\n\n\n\n\n1\n109\n0.79\n0.17\n0.79\n\n\n2\n126\n0.67\n0.22\n0.92\n\n\n3\n118\n0.73\n0.19\n0.85\n\n\n4\n121\n0.78\n0.17\n0.86\n\n\n\n\n\n\n\n\nTable 4 shows that the raters are similar in their performance; they each matched ratings for the same wine at a high rate, and also matched the non-complex wine ratings at a high rate. The difference column show (again) that the raters slightly inflate ratings compared to the fitted average t-a-p model.\n\n\n7 A Final Model\nI can’t resist running one more filter, to remove the “complex” wines, just to see how well the model fits.\n\n\nShow the code\nplot_results &lt;-  vector(\"list\", 6)\n\nwine_cleaner &lt;- wine_clean |&gt; \n  left_join(model_rating |&gt; select(subject_id, model_rating)) |&gt; \n  filter(model_rating != \"Complex\") \n\nplot_results &lt;- vector(\"list\", 6)\nmodel_results &lt;- vector(\"list\",3) # save the hierarchical models\n\nfor(i in 1:3){ # cut point\n  for(j in 1:2){ # type\n    \n    temp_ratings &lt;- wine_cleaner |&gt; \n      as_binary_ratings(1:i)\n    \n    if(j == 1){ # average model\n      temp_params &lt;- temp_ratings |&gt; \n        as_counts() |&gt; \n        fit_counts()\n      \n      temp_plot&lt;- temp_ratings |&gt; \n        as_rating_params(temp_params) |&gt; \n        subject_calibration()\n    } else { # hierarchical model\n      temp_model &lt;- temp_ratings |&gt; \n        fit_ratings() |&gt; \n        mutate(cutpoint = str_c(i,\"|\",i+1))\n      \n      model_results[[i]] &lt;- temp_model\n      \n      temp_plot &lt;- temp_model |&gt; \n        subject_calibration()\n      \n    }\n    \n    plot_results[[i + (j-1)*3]] &lt;- temp_plot\n  }\n}\n\nmodel_results &lt;- bind_rows(model_results)\n\ntitle1 &lt;- ggplot() + ggtitle(\"Average\") + theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\ntitle2 &lt;- ggplot() + ggtitle(\"Hierarchical\") + theme_void() +\n  theme(plot.title = element_text(hjust = 0.5))\n\ncowplot::plot_grid(title1, title2, \n                   plot_results[[1]], plot_results[[4]],\n                   plot_results[[2]], plot_results[[5]], \n                   plot_results[[3]], plot_results[[6]],\n                   nrow = 4, ncol = 2, \n                   labels = c(NA, NA, \"1|2\",\"1|2\",\"2|3\",\"2|3\",\"3|4\",\"3|4\"),\n                   rel_heights = c(0.2, 1,1,1),\n                   label_size = 12)\n\n\n\n\n\n\n\n\nFigure 8: Subject calibration for the three cut points on the cleaned data with complex cases removed. The average t-a-p model is in the left column, with cut points 1|2 through 3|4. The hierarchical model is in the right column.\n\n\n\n\n\nThe average and hierarchical models agree well on the average parameters, and the entropy has decreased again except for the bottom right plot, where the hierarchical model got slightly worse. I think we could trust the hierarchical model here to give us rater information. Let’s look at the scale calibration again.\n\n\nShow the code\nt_results &lt;- vector(\"list\", 3)\na_results &lt;- vector(\"list\", 3)\n\nwine_cleaner_params &lt;- fit_ordinal_tap(wine_cleaner) |&gt; \n                     filter(type == \"t-a-p\") \n\nfor(i in 1:3){\n   rating_params &lt;- wine_cleaner |&gt; \n         as_binary_ratings(in_class = 1:i) |&gt; \n         fit_ratings()\n  \n  t_results[[i]] &lt;- rating_params |&gt; \n         distinct(subject_id, .keep_all = TRUE) |&gt; \n         mutate(cutpoint = str_c(i,\"|\",i+1)) |&gt; \n         select(cutpoint, subject_id, t) \n  \n  a_results[[i]] &lt;- rating_params |&gt; \n                    distinct(rater_id, .keep_all = TRUE) |&gt; \n                    mutate(cutpoint = str_c(i,\"|\",i+1)) |&gt; \n                    select(cutpoint, rater_id, a,p) \n}\n\nt_results &lt;- bind_rows(t_results)\na_results &lt;- bind_rows(a_results)\n\nempirical &lt;- wine_cleaner |&gt; \n             count(rating) |&gt; \n             mutate(p = n/sum(n)) |&gt; \n             pull(p)\n\nt_avg &lt;- t_results |&gt; \n         group_by(cutpoint) |&gt; \n         summarize(t = mean(t)) |&gt; \n         pull(t)\n\naverage_model      &lt;- with(wine_cleaner_params, c(t[1], t[2] - t[1], t[3] - t[2], 1 - t[3]))\nhierarchical_model &lt;- c(t_avg[1], t_avg[2] - t_avg[1], t_avg[3] - t_avg[2], 1 - t_avg[3])\n\nt_compare &lt;- data.frame(rating = 1:4, empirical = empirical, average = average_model, hierarchical = hierarchical_model)\n\nt_compare |&gt; \n  gather(type, value, -rating) |&gt; \n  ggplot(aes(x = rating, y = value, group = type, color = type)) +\n  geom_line() +\n  geom_point() +\n  theme_bw() +\n  ylim(0,NA)\n\n\n\n\n\n\n\n\nFigure 9: Rating distributions, comparing the average model, hierarchical model, and empirical ratings after cleaning and de-complexifying.\n\n\n\n\n\nThis time (compare with Figure 6) there’s nothing obviously wrong with the hierarchical model.\n\n\nShow the code\nmodel_results |&gt; \n  distinct(cutpoint, rater_id, a, p) |&gt; \n  kable(digits = 2, format = \"html\", escape = FALSE)\n\n\n\n\nTable 5: Rater statistics after cleaning and removing complex cases, using the hierarchical t-a-p model at each cutpoint.\n\n\n\n\n\n\ncutpoint\nrater_id\na\np\n\n\n\n\n1|2\n1\n0.65\n0.00\n\n\n1|2\n2\n0.54\n0.00\n\n\n1|2\n3\n0.79\n0.00\n\n\n1|2\n4\n0.84\n0.00\n\n\n2|3\n1\n0.79\n0.27\n\n\n2|3\n2\n0.74\n0.33\n\n\n2|3\n3\n0.61\n0.44\n\n\n2|3\n4\n0.70\n0.24\n\n\n3|4\n1\n0.60\n0.96\n\n\n3|4\n2\n0.49\n0.89\n\n\n3|4\n3\n0.75\n0.77\n\n\n3|4\n4\n0.63\n0.93\n\n\n\n\n\n\n\n\nThe modeled \\(p_j\\) parameters in Table 4 say that when raters make inaccurate ratings at the end of the scale, they always favor the 1 rating and almost always favor the 4 rating. The mid-point 2|3 is more balanced. This pattern can be found in all the model estimates to some extent. I wonder if it’s a model limitation or a psychological effect, or perhaps is just peculiar to this data set for some reason.\n\n\n8 Final Thoughts\nMore usual types of analysis on a data set like this are a preference model like proportional odds or a score variance model like ANOVA (hierarchical regression with some level of pooling). The lack of a true ordinal t-a-p model means it’s awkward to work with the cut points, and the dual trials added more complexity that a score variance model can easily accommodate. However, I think the t-a-p model adds a philosophical component that variance-based models don’t, viz addressing the question of a “true” rating. What would it even mean for a wine to have a true score?\nAs with any instrument, we can imagine a causal pathway whereby the inputs, comprising the appearance, aroma, and tastes of a vintage, lead predictably to a human assessment based on experience. Here the wine judge is the instrument. Humans can act reliably as causes if the conditions are right. A factory with human workers would be impossible otherwise. So it’s plausible that the judges have a reliable response to the perceived characteristics of the wine samples. It would be hard to explain the self-consistency of the repeated tastings otherwise. As noted earlier, this could be partly to recognition of samples and recall of the prior rating, rather than a true independent trial. That’s not a question we can answer with this data.\nThe winnowing process in the analysis here first consolidated the two trials, which is a rather drastic reduction in data. The second filter was to take out the wines with low-confidence probabilities for the modeled ratings. From the usual regression modeling perspective, this is a terrible idea; we don’t throw out samples unless they are clear outliers, and even then we want to think hard about it. However, we’re after something different here: the plausibility of metaphysical truth. This is because the t-a-p model assumption of a latent truth state is a strong assumption (strong means stringent in this context). The convergence of the average and hierarchical models for the last (most-reduced) data set makes me think that a “true wine rating” is not a crazy idea, but subject to some limitations. First we have to insist on high quality ratings, hence the first filter requiring identical ratings across trials. The models are clearly sensitive to noisy data. The second proviso is that the wine scale is but one dimension of the complexities related to wine quality, and therefore some of the wines are not likely to reach consensus. If we can swallow all that, then maybe the final ratings do have some connection to physical reality that starts with sense data, wends through a neurological classification, and ends up with a 1-4 rating or a separate category for unratable or complex wines.\n\n\n\n\n\nReferences\n\nHodgson, R. T. (2008). An examination of judge reliability at a major US wine competition. Journal of Wine Economics, 3(2), 105–113."
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Appendix A: Statistical Details",
    "section": "",
    "text": "1 Closed Form Accuracy Coefficients\nWe’ll show that\n\\[\na^2 = \\sum_{k=0}^R\\beta_k\\eta_k\n\\] for unbiased raters (\\(t=p\\)), so that the Fleiss kappa can be expressed as a weighted sum of the proportions of subject counts of Class 1 ratings. This provides details for the section on exact formulas in Chapter 2. A closed form for the coefficients is derived below.\nThe Fleiss kappa calculates match rates per subject. Suppose we have \\(R\\) raters who assigned \\(k\\) ratings of Class 1 for subject \\(i\\), then the number of matched ratings is the sum of Class 1 matches and Class 0 matches, or \\(M_i = k(k-1)/2 + \\bar{k}(\\bar{k}-1)/2\\), where \\(\\bar{k} = R - k\\). The match rate is this count out of the number of possible matches, which is \\(R(R-1)/2\\). With \\(R\\) raters, \\(k \\epsilon \\{0,1,..,R\\}\\), and over the \\(N\\) subjects there will be some count \\(n_k\\) for the number of the occurances of each possible value of \\(k\\). The Fliess kappa calculation averages over these subject match rates. This average match rate \\(m_o\\) can be simplified with\n\\[\n\\begin{aligned}\nm_o &=  \\frac{2}{NR(R-1)}\\sum_{k=0}^{R} n_k M_k \\\\\n&= \\frac{1}{R(R-1)}\\sum_{k=0}^{R} \\eta_k \\left( k(k-1) + \\bar{k}(\\bar{k}-1) \\right)\\\\\n&= \\frac{1}{R(R-1)}\\sum_{k=0}^{R} \\eta_k \\left( 2k^2 -2Rk+R^2-R\\right)\\\\\n&= \\frac{1}{R-1}\\sum_{k=0}^{R} \\eta_k \\left( R-2k-1\\right) + \\frac{2}{R(R-1)}\\sum_{k=0}^{R} k^2\\eta_k \\\\\n&= \\frac{R - 1 -2 \\sum_{k=0}^{R} k\\eta_k  }{R-1} +  \\frac{2}{R(R-1)}\\sum_{k=0}^{R} k^2\\eta_k \\\\\n&= 1 - \\frac{2}{R-1}\\sum_{k=0}^{R} k\\eta_k + \\frac{2}{R(R-1)}\\sum_{k=0}^{R} k^2\\eta_k \\\\\n\\end{aligned}\n\\]\nwhere \\(\\eta_k = n_k / N\\) is the proportion of subjects with \\(k\\) ratings of Class 1, so that \\(\\sum \\eta_k = 1\\). Note that the average rate of Class 1 ratings is\n\\[\nc = \\frac{1}{R}\\sum_{k=0}^{R} k\\eta_k\\,,\n\\] since the sum adds up the number of Class 1 ratings over all subjects divided by the number of subjects, and \\(NR\\) is the total number of ratings. Using that idea, we can express\n\\[\n\\begin{aligned}\nm_o &= 1 - \\frac{2R}{R-1}c + \\frac{2}{R-1}\\sum_{k=0}^{R} k^2\\eta_k/R \\\\\n&:= 1 - 2\\frac{Rc-\\gamma}{R-1} \\, ,\n\\end{aligned}\n\\] where \\(\\gamma = \\sum k^2\\eta_k/R\\) to simplify the notation. The Fleiss kappa is then the difference between the average match rate and the expected match rate under independence, which is the sum of the products of the marginal proportions of Class 1 ratings for each rater.\nThe Fleiss formula is\n\\[\n\\kappa = \\frac{m_o - m_c}{1 - m_c},\n\\]\nwhere \\(m_c = c^2 + \\bar{c}^2 = 2c^2 - 2c + 1\\). Here \\(c\\) is the average Class 1 rate over all ratings, so that we can express the kappa formula as \\[\n\\begin{aligned}\n\\kappa &= \\frac{1 - 2\\frac{Rc-\\gamma}{R-1}-2c^2 + 2c - 1  }{1-2c^2 + 2c - 1} \\\\\n&= \\frac{\\gamma/c-R}{(R-1)(1-c)} + 1\n\\end{aligned}\n\\] Under the unbiased rater assumption we have \\(p=t=c\\), and if we fix \\(t\\) to a constant, with the only unknown being \\(a\\), the formula reduces to\n\\[\n\\begin{aligned}\n\\kappa_t = a^2 &=\\frac{\\frac{1}{Rt} \\sum_{k=0}^{R} k^2\\eta_k-R}{(R-1)\\bar{t}} + 1 \\\\\n         &= \\sum_{k=0}^{R} \\left( \\frac{k^2}{R(R-1)t\\bar{t}}   \\right)\\eta_k + 1 - \\frac{R}{(R-1)\\bar{t}} \\\\\n         &:= \\sum_{k=0}^{R} \\beta_k\\eta_k  \\,, \\\\\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\beta_k &= \\frac{k^2}{R(R-1)t\\bar{t}}+ 1 - \\frac{R}{(R-1)\\bar{t}}. \\\\\n\\end{aligned}\n\\] It seems odd algebraically to bring the constants inside the sum as if they were multiplied by the \\(\\eta_k\\) proportions, but this works because \\(\\sum \\eta_k = 1\\).\nThe coefficients aren’t unique if the distribution of \\(\\eta_k\\) is symmetrical, as is the case we worked out by hand in Chapter 2, with \\(R=2\\) and \\(t = p= .5\\). In case of symmetrical proportions of Class 1 ratings, we could use\n\\[\n\\begin{aligned}\n\\dot\\beta_k &= \\frac{1}{2}(\\beta_k+\\beta_{R-k}). \\\\\n\\end{aligned}\n\\] In the special case when \\(t = p = 1/2\\) and \\(R = 2\\), we have \\(\\beta_0 = -3\\), \\(\\beta_1 = -1, \\beta_2 = 5\\), so \\(\\dot \\beta_0 = \\dot \\beta_2 = 1\\) and \\(\\dot \\beta_1 = -1\\) as we found with the direct derivation. In general we can’t use this method, because \\(\\eta_k\\) isn’t usually symmetrical.\n\n\nShow the code\nR &lt;- 30\n\nbetas &lt;- data.frame(\n  k = 0:R,\n  beta1 = tapModel::exact_accuracy_coefs(R, .1),\n  beta2 = tapModel::exact_accuracy_coefs(R, .3),\n  beta3  =tapModel::exact_accuracy_coefs(R, .5)) |&gt; \ngather(beta, val, -k) |&gt; \n  mutate(tp = case_when( beta == \"beta1\" ~ \".1\",\n                           beta == \"beta2\" ~ \".3\",\n                           TRUE ~ \".5\"))\n\nbetas |&gt; \n  ggplot(aes(x = k, y = val, group = tp, color = tp)) +\n  geom_point() +\n  geom_line() +\n  theme_bw() +\n  ylab(\"beta_k\") +\n  xlab(\"Number of Class 1 ratings, k\")\n\n\n\n\n\n\n\n\nFigure 1: A sample of weights for exact formulation of accuracy squared, with 30 raters and a selection of values for t=p.\n\n\n\n\n\nGiven the rapid growth of these coefficients, which are all positive, we would not expect the exact formula to be numerically stable, espeically for large numbers of raters. An even more severe restriction is that the \\(\\eta\\) coefficients need to be exact proportions for a t-a-p model. Sampling error will violate that assumption for most data sets. In practice, it’s better to use the Fleiss formula, which also assumes \\(t=p\\), although without specifying the common value.\n\n\nShow the code\nparam_grid &lt;- expand.grid(t = seq(.05, .95, .05),\n                          a = seq(.05, .95, .05)) |&gt; \n  mutate( a = a + t/30,\n          a_calc = NA)\n\nfor(i in 1:nrow(param_grid)){\n  t &lt;- param_grid$t[i]\n  a &lt;- param_grid$a[i]\n  props &lt;- tapModel:::exact_count_probabilites(10,t,a,t)\n  tap_coefs &lt;- exact_accuracy_coefs(10,t)\n  param_grid$a_calc[i] &lt;- sqrt(sum(props*tap_coefs))\n}\n\nparam_grid |&gt; \n  ggplot(aes(x = a, y = a_calc, color = t)) +\n  geom_point() +\n  geom_abline() +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 2: Validity check, calculating accuracy from the linear representation for a range of values of t = p.\n\n\n\n\n\nThe linear expression works precisely across the range of parameters in the grid illustrated in Figure 2. However, the calculation is quite sensitive to violations of the unbiased rater assumption.\n\n\nShow the code\nparam_grid &lt;- expand.grid(t = seq(.05, .95, .05),\n                          a = seq(.05, .95, .05)) |&gt; \n  mutate( a = a + t/30,\n          a_calc = NA,\n          a_kappa = NA)\n\nfor(i in 1:nrow(param_grid)){\n  t &lt;- param_grid$t[i]\n  a &lt;- param_grid$a[i]\n  props &lt;- tapModel:::exact_count_probabilites(10,t-.02,a,t+.02)\n  tap_coefs &lt;- exact_accuracy_coefs(10,t)\n  param_grid$a_calc[i] &lt;- sqrt(sum(props*tap_coefs))\n  counts &lt;- data.frame(N_r = 10, N_c = 0:10, n = props)\n  param_grid$a_kappa[i] &lt;- fleiss_kappa(counts)$a\n}\n\nparam_grid |&gt; \n  ggplot(aes(x = a, y = a_calc, color = t)) +\n  geom_point() +\n  geom_abline() +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 3: Calculations of accuracy using the linear combination with a minor violation of the unbiased rater assumption.\n\n\n\n\n\nThe results in Figure 3 come from calculating the coefficients with a value of \\(t\\), then replacing \\(t\\) and \\(p\\) in the simulated data with \\(\\hat{t} = t - .02\\) and \\(\\hat{p} = t + .02\\). The resulting estimates of accuracy can become quite poor. This is not true of the Fleiss kappa, which has more robust approximation when the unbiased rater assumption is violated.\n\n\nShow the code\nparam_grid |&gt; \n  ggplot(aes(x = a, y = a_kappa, color = t)) +\n  geom_point() +\n  geom_abline() +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 4: Accuracy approximations by Fleiss kappa when the unbiased rater assumption is slightly violated.\n\n\n\n\n\nThe relatively low approximation error in Figure 4 shows a direct comparison to the linear combination approximation. The latter method isn’t useful for real data sets because of this sensitivity, but is still useful as a theoretical tool.\n\n\n2 Correlation of Ratings between Raters\nGiven two distinct raters \\(i\\) and \\(j\\) with common accuracy \\(a\\) and guess probability \\(p\\), what’s the correlation between their ratings? Let \\(c = E[C_i] = E[C_j] = ta + p\\bar{a}\\). Capital letters denote random binary variables, so that \\(A_i\\) is one if the first rater made an accurate assessment and zero if not. \\(T\\) is the true value of a common subject being rated. The covariance between the two raters’ ratings is\n\\[\n\\begin{aligned}\n\\textrm{Cor}(C_i, C_j) &= \\frac{\\textrm{Cov}(C_i, C_j)}{\\sqrt{\\textrm{Var}(C_i)\\textrm{Var}(C_j)}} \\\\\n&= \\frac{E[(TA_i + \\bar{A_i}P_i)(TA_j + \\bar{A_j}P_j)] - c^2}{\\textrm{Var}(C)} \\\\\n&= \\frac{ta^2 + 2ta\\bar{a}p + \\bar{a}^2p^2  - (ta + p\\bar{a})^2}{(ta + p\\bar{a})\\overline{(ta + p\\bar{a})  }} & (\\textrm{since }T^2 = T)\\\\\n&= \\frac{ta^2 + 2ta\\bar{a}p - t^2a^2 - 2tap\\bar{a}}{(ta + p\\bar{a})\\overline{(ta + p\\bar{a})  }} \\\\\n&= \\frac{a^2t\\bar{t}}{c\\bar{c }} \\\\\n\\end{aligned}\n\\]\nRater accuracy can be obtained via\n\\[\na^2 =\\frac{c\\overline{c }}{t\\bar{t}} \\text{Cor}(C_a, C_b) = \\frac{c\\overline{c }}{t\\bar{t}}\\kappa_{fleiss}\n\\tag{1}\\]\nThe correlation between two raters’ ratings of the same subject is the intraclass correlation coefficient (ICC) for a two-way random effects model Shrout & Fleiss (1979), which has been shown to be equivalent to the Fleiss kappa as described in Fleiss et al. (2013), p. 611-12. Under the \\(t = p\\) unbiased rater assumption, \\(c = ta + \\bar{a}p = p\\), so that the Fliess kappa is (again) shown to be \\(a^2\\) under that condition. The relation Equation 1 suggests that the Fliess kappa could be adjusted for cases when \\(t \\ne p\\) by making assumptions about those two parameters. For example, maybe the true rate is known from other information. The overall rate of Class 1 ratings \\(c\\) can be estimated directly from the data, but estimating \\(t\\) requires either prior knowledge of the context or using the full t-a-p estimation process, in which case there’s no need to compute the Fliess kappa.\n\n\n3 Correlation Between Ratings and True Values\nIt is of interest to find the correlation between \\(T_i\\) the truth value of subject \\(i\\) and the resulting classification \\(C_i\\). Note that both of the random variables \\(T_i\\) and \\(C_i\\) take only values of zero or one, so squaring them doesn’t change their values. This fact simplifies computations, for example \\(E[C_i^2] = E[C_i] = ta + p\\bar{a}\\). The variance of \\(C\\) is therefore \\[\n\\begin{aligned}\n\\textrm{Var}(C) &= E[C^2] - E^2[C] \\\\\n       &= c - c^2 \\\\\n       &= c\\bar{c} \\\\\n       &= (ta + p\\bar{a})\\overline{(ta + p\\bar{a})}. \\\\\n\\end{aligned}\n\\] Similarly, \\(Var(T) = t\\bar{t}\\). The correlation between true values and ratings is then\n\\[\n\\begin{aligned}\n\\text{Cor}(T, C) &= \\frac{\\text{Cov}(T, C)}{\\sqrt{\\text{Var}(T)\\text{Var}(C)}} \\\\\n&= \\frac{E[T(Ta + p\\bar{a}) ] - t(ta + p\\bar{a})}{\\sqrt{t\\bar{t} c\\bar{c}}  } \\\\\n&= \\frac{t(a + p\\bar{a})  - t(ta + p\\bar{a})}{\\sqrt{t\\bar{t} c\\bar{c}}  } \\\\\n&= a\\frac{\\sqrt{t\\bar{t}}}{\\sqrt{c\\bar{c}}} \\\\\n&= a\\frac{\\sigma_T}{\\sigma_C}.\n\\end{aligned}\n\\tag{2}\\]\nWhere \\(\\sigma\\) is the standard deviation (square root of variance). The relationship in Equation 2 can also be seen as \\(a = \\text{Cor}(T, C) \\frac{\\sigma_C}{\\sigma_T}\\), which means \\(a\\) can be interpreted as the slope of the regression line \\(C = \\beta_0 + \\beta_1T + \\varepsilon\\), i.e. \\(a = \\beta_1\\). In the unbiased rater case \\(p = t\\), \\(\\sigma_C = \\sigma_T\\) and so \\(\\text{Cor}(T, C) = a\\). It can also be shown that for a \\(t\\)-\\(a_1,a_0\\)-\\(p\\) model, the \\(t=p\\) assumption leads to \\(a = \\sqrt{a_1a_0}.\\) See Eubanks (2014).\nThe two correlations derived here are related by \\(\\text{Cor}^2(T, C) = \\text{Cor}(C_i, C_j)\\).\n\n\n4 Alternate Derivation of Fleiss Kappa Relationship\nThis appendix gives an alternative derivation for the Fleiss kappa’s relationship to rater accuracy under the unbiased rater assumption.\nThe Fleiss kappa Fleiss (1971) is a particular case of Krippendorf’s alpha Krippendorff & Fleiss (1978) and a multi-rater extension of Scott’s pi Scott (1955). The statistic compares the overall distribution of ratings (ignoring subjects) to the average over within-subject distributions. These distributions are used to compute the number of observed matches (i.e. agreements) \\(m_o\\) over subjects \\(i = 1 \\dots N\\). For a two-category classification with a fixed number of raters \\(R&gt;1\\) per subject the number of matched ratings for a given subject \\(i\\) is\n\\[\n\\begin{aligned}\nm_o &= \\frac{ {\\binom{k_i}{2}} + {\\binom{R - k_i}{2}}}{\\binom{R}{2}} \\\\\n&= \\frac{k_i(k_i-1)+ (R-k_i)(R - k_i-1)}{R(R-1)} \\\\\n&= \\frac{2k_i^2 - 2k_iR + R^2-R}{R(R-1)}\n\\end{aligned}\n\\]\nwhere \\(k_i\\) is the count of Class 1 ratings for the \\(i\\)th subject. The match rates are averaged over the subjects to get \\(\\text{E}[m_o]\\) and then a chance correction is applied with\n\\[\n\\kappa = \\frac{\\text{E}[m_i] - \\text{E}[m_c]}{1-\\text{E}[m_c]},\n\\]\nwhere \\(\\text{E}[m_c]\\) is the expected number of matches due to chance. Recall that different agreement statistics make different assumptions about this chance. Using the t-a-p model, and assuming \\(t = p\\), the true rate of Class 1 \\(t\\) is assumed to be \\(\\text{E}[c_{ij}]\\), so \\(\\text{E}[m_c] = t^2 + (1-t)^2\\), the asymptotic expected match rate for independent Bernoulli trials with success probability \\(t\\).\nBy replacing \\(p\\) with \\(t\\) in the t-a-p model’s mixture distribution for the number \\(k\\) of Class 1 ratings a subject is assigned we obtain\n\\[ Pr(k) = t \\binom{R}{k} (a + \\bar{a}t)^k (\\bar{a}\\bar{t})^{R - k} + \\bar{t} \\binom{R}{k} (\\bar{a}t)^k (1 - \\bar{a}t)^{R - k} \\] so it suffices for large \\(N\\) to write the expected match rate as\n\\[\n\\begin{aligned}\n      \\text{E}[m(a)] &= \\sum_{k=0}^R \\frac{2k^2 - 2kR + R^2-R}{R(R-1)}\\text{Pr}(k;a,t) \\\\\n      &= \\sum_{k=0}^R \\frac{2k^2 - 2kR + R^2-R}{R(R-1)} \\left[ t\\binom{R}{k}(a + \\bar{a}t)^{k}(\\bar{a}\\bar{t})^{R-k_i} + \\bar{t}\\binom{R}{k}(\\bar{a}t)^{k}(1-\\bar{a}t)^{R-k} \\right] \\\\\n      &= \\frac{2}{R(R-1)} \\sum_{k=0}^R k^2 \\left[ t \\text{ Binom}(R,k,a+\\bar{a}t) + \\bar{t} \\text{ Binom}(R,k,\\bar{a}t) \\right] \\\\\n      &- \\frac{2R}{R(R-1)} \\sum_{k=0}^R k \\left[ t \\text{ Binom}(R,k,a+\\bar{a}t) + \\bar{t} \\text{ Binom}(R,k,\\bar{a}t) \\right] \\\\\n      &+ \\frac{R(R-1)}{R(R-1)} \\sum_{k=0}^R \\left[ t \\text{ Binom}(R,k,a+\\bar{a}t) + \\bar{t} \\text{ Binom}(R,k,\\bar{a}t) \\right] \\\\\n      &= \\frac{2}{R(R-1)} \\left[ tR(a+\\bar{a}t)\\bar{a}\\bar{t}+tR^2(a+\\bar{a}t)^2 + \\bar{t}R(\\bar{a}t)(1-\\bar{a}t)+\\bar{t}R^2(\\bar{a}t)^2\\right] \\\\\n      &- \\frac{2}{R-1} \\left[ tR(a+\\bar{a}t) +  \\bar{t}R(\\bar{a}t)\\right] +1 \\\\\n      &= 2a^2(t-t^2) + 2t^2 -2t + 1,\n\\end{aligned}\n\\]\nusing the moment identities to gather the sums. Here, \\(t\\) and \\(R\\) are fixed, and \\(m(a)\\) is the average match rate over cases, which depends on unknown \\(a\\) and fixed \\(t = \\text{E}[c_{ij}]\\). Now we can compute the Fleiss kappa with\n\\[\n\\begin{aligned}\n\\kappa_{fleiss} &= \\frac{\\text{E}[m_i] - \\text{E}[m_*]}{1-\\text{E}[m_*]} \\\\\n            &= \\frac{2a^2(t-t^2) + 2t^2 -2t + 1 - (t^2+(1-t)^2)}{1-(t^2+(1-t)^2)} \\\\\n            &= a^2.\n\\end{aligned}\n\\]\nSo kappa is the square of accuracy under the unbiased rater assumption, with constant rater accuracy and fixed number of raters. The relationship does not depend on the true distribution \\(t\\) of Class 1 cases.\n\n\n\n\n\nReferences\n\nEubanks, D. A. (2014). Causal interfaces. Arxiv.org Preprint. http://arxiv.org/abs/1404.4884v1\n\n\nFleiss, J. L. (1971). Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5), 378.\n\n\nFleiss, J. L., Levin, B., & Paik, M. C. (2013). Statistical methods for rates and proportions. john wiley & sons.\n\n\nKrippendorff, K., & Fleiss, J. L. (1978). Reliability of binary attribute data. Biometrics, 34(1), 142–144.\n\n\nScott, W. A. (1955). Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly, 321–325.\n\n\nShrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing rater reliability. Psychological Bulletin, 86(2), 420."
  },
  {
    "objectID": "app.html",
    "href": "app.html",
    "title": "Interactive t-a-p Analysis",
    "section": "",
    "text": "The interactive app associated with tapModel was created before the R package, as a way for me to easily try out the methods and visualize results. I recommend it primarily as a learning tool. You can quickly simulate data to specifications and then create models that recover the parameters. The app is no longer being developed, since I’m spending my time on extending the theory and the functionality of the package. The app can only create average models, not hierarchical ones.\nFor real work, I recommend writing scripts or Quarto reports.\nThe R programming language has been extended to include the creation of interactive apps using a framework called Shiny. You’ll need to install the library(shiny) to run it, with install.packages(\"shiny\").\nYou can install the shiny app from github. There are instructions there on installing it. To take advantage of the Bayesian estimation methods, you’ll need to install additional R packages and tools. The Bayesian estimation is done with a Markov chain Monte Carlo (MCMC) process of sampling from a probability distribution. For that, the Stan programming language is used, with the stancmdr package. That link has installation instructions. The library LaplacesDemon is also needed to compute the modes of distributions.\n\n\nThe simplest way to get started is with the tapModel library. It provides these functions:\n\nLoad data from comma-delimited value (CSV) files that are formatted as\n\nOutcome ratings in columns, where multiple traits have rating data, one subject per row, and multiple traits per subject. For example if a jury of reviewers rates a musical performance on style, technique, and musicality, each performer would have multiple rows, each with those three columns.\nRaters in columns, where the same type of jury data can be reformatted to have a Category column (style, technique, musicality), and each rater appears as a single column. In this format each subject only appears on a single row.\nLong format, where the subject ID, optional category, and rating appear as the three columns of the CSV.\n\nSimulate a data set by specifying the t-a-p parameters and sample sizes.\nEstimate t-a-p parameters from a (simulated or real) data set.\nEstimate ordinal t-a-p parameters from a data set. This assumes that the rating scale is sorted alphabetically in the correct order. For example, a numerical survey response scale is usually in the right order, but the labels may not be (“neutral” doesn’t sort in the middle of “strongly agree” and “strongly disagree”). You may need to adjust the rating labels accordingly, e.g. “1 - strongly disagree”, … “5 - strongly agree”.\n\n\n\n\nIf you launch the app from the github source code and have the stancmdr package installed, some additional features become available. These derive from using Bayesian modeling written in the Stan programming language to make parameter estimates from maximum likelihood models built to reflect variations of the t-a-p model. Markov chain Monte Carlo (MCMC) methods are used to explore the model’s probability distribution, which you can then see within the app. This is advantageous because a parameter estimate may have a bimodal distribution when parameters are not cleanly identifiable. In those cases relying on an average for a parameter estimate is a mistake.\nAnother advantage of MCMC’s numerical simulation is that the basic t-a-p model can be extended to include more parameters without being rigidly tied to the binomial mixture model. Finally, the simple methods used for three-parameter estimation will fail when confronted with many parameters. In the interactive application, there is limited ability to add parameters. For complete flexibility, Stan scripts are provided in the chapter on hierarchical models.\nThe following sections describe the functionality of both the package app and the stand-alone version, with the latter’s additional features marked as “advanced.”"
  },
  {
    "objectID": "app.html#basic-functions",
    "href": "app.html#basic-functions",
    "title": "Interactive t-a-p Analysis",
    "section": "",
    "text": "The simplest way to get started is with the tapModel library. It provides these functions:\n\nLoad data from comma-delimited value (CSV) files that are formatted as\n\nOutcome ratings in columns, where multiple traits have rating data, one subject per row, and multiple traits per subject. For example if a jury of reviewers rates a musical performance on style, technique, and musicality, each performer would have multiple rows, each with those three columns.\nRaters in columns, where the same type of jury data can be reformatted to have a Category column (style, technique, musicality), and each rater appears as a single column. In this format each subject only appears on a single row.\nLong format, where the subject ID, optional category, and rating appear as the three columns of the CSV.\n\nSimulate a data set by specifying the t-a-p parameters and sample sizes.\nEstimate t-a-p parameters from a (simulated or real) data set.\nEstimate ordinal t-a-p parameters from a data set. This assumes that the rating scale is sorted alphabetically in the correct order. For example, a numerical survey response scale is usually in the right order, but the labels may not be (“neutral” doesn’t sort in the middle of “strongly agree” and “strongly disagree”). You may need to adjust the rating labels accordingly, e.g. “1 - strongly disagree”, … “5 - strongly agree”."
  },
  {
    "objectID": "app.html#advanced-functions",
    "href": "app.html#advanced-functions",
    "title": "Interactive t-a-p Analysis",
    "section": "",
    "text": "If you launch the app from the github source code and have the stancmdr package installed, some additional features become available. These derive from using Bayesian modeling written in the Stan programming language to make parameter estimates from maximum likelihood models built to reflect variations of the t-a-p model. Markov chain Monte Carlo (MCMC) methods are used to explore the model’s probability distribution, which you can then see within the app. This is advantageous because a parameter estimate may have a bimodal distribution when parameters are not cleanly identifiable. In those cases relying on an average for a parameter estimate is a mistake.\nAnother advantage of MCMC’s numerical simulation is that the basic t-a-p model can be extended to include more parameters without being rigidly tied to the binomial mixture model. Finally, the simple methods used for three-parameter estimation will fail when confronted with many parameters. In the interactive application, there is limited ability to add parameters. For complete flexibility, Stan scripts are provided in the chapter on hierarchical models.\nThe following sections describe the functionality of both the package app and the stand-alone version, with the latter’s additional features marked as “advanced.”"
  },
  {
    "objectID": "app.html#exploring-binary-mixtures",
    "href": "app.html#exploring-binary-mixtures",
    "title": "Interactive t-a-p Analysis",
    "section": "3.1 Exploring Binary Mixtures",
    "text": "3.1 Exploring Binary Mixtures\nIf you’re starting to use the resources on this site to analyze real data sets, it’s a good idea to develop an intuition for the assumptions. The decision tree that starts with the true class, proceeds to accuracy and then randomness, results in a particular pattern of ratings: the collection of true Class 1 subject ratings will look different from the Class 0 collection as long as accuracy is greater than zero. On average, there should be more Class 1 ratings for true Class 1 subjects, and the gap between the average Class 1 and Class 0 ratings will increase as accuracy increases.\nTo see this effect, increase the sample size and number of raters (top two sliders) to the maximum, leave the rest of the sliders alone and generate the data.\n\n\n\nData with large number of raters and all parameters = .5\n\n\nThe table at the top of the display gives statistics for the model specification, which may vary from the actual samples if the Random option is chosen. These correpond to the statistics found in the Kappa chapter. The class proportions in the first row are the \\(c\\) and \\(\\bar{c}\\) statistics, the match rates are \\(a^2\\) and \\(\\bar{a}^2\\), and total match rate is the sum. These last two rows will be different calculations if individual accuracies are used. The random match rate is \\(m_r = p^2 + \\bar{p}^2\\) unless the individual parameters are used. The mixed rate is \\(m_x\\).\nThe shape of the rating distribution is a histogram of the counts per subject of Class 1 ratings per subject. The true Class 1 cases comprise the right bump, since accurate ratings add to the number for each subject half the time (since \\(a = .5\\)). On average, true Class 1 cases are receiving 75/100 ratings of Class 1. The true Class 0 cases–the left bump in the histogram–only ever receive inaccurate ratings of Class 1. The difference between the two peaks is 50, and that’s because 50 = .5(100), or rater accuracy times the number of raters. The more accurate the raters are, the further the two bumps will be apart.\nIf you dial in different specifications for the three parameters, you’ll quickly develop an intuition for how these models work. For example what happens if we leave everything the same, but change to \\(t = .8\\)? You should see that changing the truth parameter only increases the pile of Class 1 ratings; it doesn’t change where they are. This is the “mixture” parameter in the binomial mixture. We already know that \\(a\\) represents the average amount of Class 1 votes between the two bumps in the histogram, but trying out different values of \\(a\\) will allow you to visualize that.\nThe default settings of .5 mean that \\(t = p\\), the unbiased rater case (see the chapter on Kappa for more on that). If you change the sliders so that \\(t \\ne p\\), you can see the effect of bias on the rating distribution.\nFor all of these cases, you can switch to the next tab in the app (the one labeled “t-a-p”) to see if the solver can recover the correct values of the parameters you specified at data generation."
  },
  {
    "objectID": "app.html#feasibility-study",
    "href": "app.html#feasibility-study",
    "title": "Interactive t-a-p Analysis",
    "section": "3.2 Feasibility Study",
    "text": "3.2 Feasibility Study\nThe data simulator can be used for a feasibility study (see power analysis). Suppose a group of graders is to read and evaluate student writing samples as passing or failing. If there are anticipated to be 21 subjects, how many raters for each would we need to be able to assess rater accuracy with a t-a-p model? If we guess that about 80% of the students should be passing, and–based on other data–that rater accuracy is around 50%, we can try varying numbers of raters to see how well the parameters can be recovered.\n\n\n\nSimulated data with 21 subjects, two raters each, 80% true Class 1 rate, and 50% accuracy. The “unbiased” box checked means that p = t = .8. The data set is generated without sampling error in this case.\n\n\nIt’s less obvious in this distribution how we might separate out the Class 1 from Class 0 cases, and the question is whether or not the solver can recover the parameters. If not, then it’s worth considering the design of the anticipated study."
  },
  {
    "objectID": "app.html#additional-parameters",
    "href": "app.html#additional-parameters",
    "title": "Interactive t-a-p Analysis",
    "section": "3.3 Additional Parameters",
    "text": "3.3 Additional Parameters\nAs discussed in the chapter on the Kappa Paradox, it’s possible to expand the t-a-p model to include \\(a\\) and \\(p\\) parameters that are estimated separately for Class 1 and Class 0 cases. These parameters can be set by using the options\n\nUse a0, a1, which creates the two sliders and unlocks the next option:\nUse p0, p1, which creates those two sliders\n\nWith that much flexibility over the probability distribution, it’s possible to create non-identifiable data sets, where the Class 0 mean is larger than the Class 1 mean. In those cases there will effectively be two solutions to the expanded t-a-p model, one with the Class 1 mean to the right (as normal) and one to the left. Any use of the full parameter set should be assumed to be non-identifiable, no matter where the class means lie. Analyzing these models requires the Bayesian methods included in the advanced features. You can, however, generate data from the complex model and then see how the three-parameter t-a-p solver does at finding a plausible solution."
  },
  {
    "objectID": "app.html#interpreting-results",
    "href": "app.html#interpreting-results",
    "title": "Interactive t-a-p Analysis",
    "section": "4.1 Interpreting Results",
    "text": "4.1 Interpreting Results\n\n\n\nModeled distribution (line) compared to empirical distribution (lollipops)\n\n\nThe top display after clicking Compute gives the estimates for the three parameters at the top of a plot. For the wine ratings with Class 1 = {1} and Class 0 = {2, 3, 4} is shown here. That choice is asking the question “how well can the judges distinguish the lowest quality wines from the rest?” The estimate is that 27% of the wines are actually Class 1 (a rating of 1), that rater accuracy is 54%, and that when random assignments are made, Class 1 is chosen 22% of the time. Since \\(t = .27\\) is close to \\(p = .22\\), the ratings are nearly unbiased in the sense discussed in the Kappa chapter.\nThe dashed line in the plot is the expected distribution of Class 1 ratings per subject. The vertical black lines with dots (lollipops) show the actual (empirical) distribution from the data. The extent to which these two agree is a measure of model fit. In this example, there are four raters for each subject (each wine), so there are a maximum of four ratings of Class 1 (the lowest quality rating of 1, as we specified with the selectors). The agreement between model and data looks better for the 0 and 1 counts than for the 2, 3, and 4 counts, implying that the model fit is better for higher ratings (Class 0).\nWe can separate the model’s distributions for the two classes by unchecking the box “show combined distribution.” The box “scale density by t” is checked as well.\n\n\n\nModeled distributions of the two classes\n\n\nThe outclass (Class 0) is modeled by the t-a-p coefficients with a spike at zero, meaning that by far the mostly likely number of Class 1 ratings in cases where the subject (the wine) is truly Class 0 is that no Class 1 ratings are assigned by the four raters. Translating that back to the original question, it means if the wine should, in truth, be rated as 2, 3, or 4 on the scale, it’s quite likely that all four wine judges will assign one of those ratings instead of a 1. In statistics notation, the spike at zero would be written as\n\\[\nPr[\\text{all wine ratings &gt; 1} | \\text{wine is actually 2, 3, or 4 quality}] = .48.\n\\]\nOn the other hand, if the wine is, in truth, a quality rating 1 wine, the ratings are not as unanimous. The most likely case is that three of the four judges will assign a 1 rating (what we’re calling Class 1, or in-class), and it’s a mound-shaped distribution rather than the spike as for Class 0.\nThe average rater accuracy is the difference between the averages for the two distributions shown, after dividing by the number of raters (4). Estimating from the plot, the mean of Class 0 is about .5, and the mean of Class 1 is about 2.7, for a difference of 2.2. Dividing by four gives .55, which is quite close to the numerical estimate of .54.\nIt sounds contradictory, but raters can be better at classifying Class 0 than Class 1, as it seems to be in this case. We could try splitting the accuracy parameter into two separate ones to improve model fit. This is described in the chapter on the Kappa Paradox.\n\n\n\nLikelihood trace for the t parameter\n\n\nThe second plot shows the shape of the log-likelihood for each of the parameters. It ranges over [0,1] for the selected parameter (here it’s \\(t\\)), while holding the other parameters constant at their estimated values (here it is \\(a= .54\\) and \\(p = .22\\)). The red marker shows the model estimate, which should be at the highest point on the graph. The green circle illustrates where the Fleiss kappa solution would be. That assumes unbiased raters (see the chapter on the kappas). Here, there’s not much bias, so the Fleiss kappa estimate is close to the optimal one.\nIf the MCMC results have been generated, the log-likelihood plot will be augmented with an illustration of the confidence interval around the mean value.\n\n\n\nMCMC results\n\n\nUsing the advanced features, a Bayesian estimate for each of the parameters is created, which gives more insights into the convergence properties of the parameter estimates. The top plot in the figure above shows the shading for the middle 5%, 50%, and 95% of the distribution shown in the bottom plot. The bottom plot gives the smoothed density function of the draws from the MCMC exploration of the likelihood space for this parameter. We want to see a normal-like (mound shaped) density, as is the case here. Sometimes this density is pushed up against the edge at zero or one, or can even be bimodal. In those cases, it is probably better to use a mode rather than the mean value for the estimate.\nThe dashed line in the plot is the average log likelihood for each of the values. Generally we’d like to see the peaks coincide."
  }
]