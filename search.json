[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Kappa Zoo",
    "section": "",
    "text": "This site explains rater agreement and associated reliability statistics. Much of the content is new, using a general model to explain the features of existing methods from the measurement literature, in the context of more modern approaches from the machine learning (ML) literature. The synthesis of these two fields is the t-a-p model, which combines the intuitiveness of the measurement ideas with the statistical power of the ML algorithms.\nIf you are new to t-a-p models, I recommend starting with the Chapter 1, which introduces rater agreement as a classification problem and lays out the philosophical and statistical assumptions. Chapter 2 provides the basic statistical derivations needed to work with t-a-p models. Chapter 3 derives the relationship to some existing rater agreement statistics, for example showing how the Fleiss kappa is a special case of a t-a-p model. Chapter 4 expands the number of parameters and shows how this relates to the “kappa paradox.” Chapter 5 allows each rater and/or subject to have individual parameters for accuracy and truth, and allows for independent variables to be used as regression inputs. Chapter 6 shows how we can assess the robustness of the results. Chapter 7 introduces the tapModel R package, which allows for easy estimation of the model parameters. Chapter 8 introduces the t-a-p app, an interactive way to use real or simulated data to estimate the model parameters without coding.\nAn appendix is included to provide more detailed statistical derivations and proofs for some results.\nThe Kappa Zoo will eventually include a collection of real-world data sets and the model parameters estimated from them.\nThis is a work in progress. The unlinked chapters aren’t finished yet.\n\n\n\nChapter 1: Quantifying Agreement\nChapter 2: The t-a-p Model\nChapter 3: Kappa Statistics\nChapter 4: The Kappa Paradox\nChapter 5: Hierarchical Models\n[Chapter 6: Assessing Model Fit)(fit.qmd)\nChapter 7: The tapModel R Package\nChapter 8: The t-a-p App\nReferences\nExamples\nAppendix A: Statistical Details"
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "Welcome to the Kappa Zoo",
    "section": "",
    "text": "Chapter 1: Quantifying Agreement\nChapter 2: The t-a-p Model\nChapter 3: Kappa Statistics\nChapter 4: The Kappa Paradox\nChapter 5: Hierarchical Models\n[Chapter 6: Assessing Model Fit)(fit.qmd)\nChapter 7: The tapModel R Package\nChapter 8: The t-a-p App\nReferences\nExamples\nAppendix A: Statistical Details"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This chapter gives a brief background on rater agreement with a focus on the goals of the project and how legacy methods fall short. It contrasts psychological measurement methods to machine learning algorithms. A general model is introduced that combines the best features of both of those cultures, and an example is given.\n\n\nHumans often use consensus to assess knowledge. We might seek a second opinion for a diagnosis, ask around for restaurant recommendations, or look for online reviews of a product. Intiuitively, we put more weight on opinions that have more agreement. This chapter describes how agreement or disagreement can be quantified. These agreement statistics are restricted to situations where the assessments are made independently and where each subject is assessed at least twice. The combination of independence and repeatability are abbreviated in probability theory as “iid” for “independent and identically distributed.” This is a powerful assumption that allows us to reach strong conclusions. However, it is not always realistic.\nIn behavioral science, the focus of these statistics is on human observers who we’ll call “raters.” Raters assign “ratings” to “subjects.” This describes a familiar case like a product review with a five-star rating system. However, the same idea applies to any categorization task, like classifying images of cats and dogs, or diagnosing diseases from medical images. A fascinating example comes from an article in The Atlantic Magazine, here describing an analysis of a very old text called the Voynich manuscript.\n\nDavis magnified the handwriting, she noticed subtle variations. In certain places, the script was more cramped or more likely to slant as it crossed the page. She tested this observation by picking a letter that didn’t appear often and tracking it across the manuscript’s pages. Its style, she saw, varied among groups of pages but not within those groups. This suggested that the differences—larger or smaller loops, straighter or curvier crossbars, longer or shorter feet—were the product of different scribes rather than of one scribe writing the same letter in different ways.\n\nSee Davis’s paper for details1. For our purposes, the “raters” are scholars who categorize writing styles, the “subjects” are sections (folios) of the manuscript they are reviewing, and the presumed different scribes are the categories we generically refer to as “ratings.”\n\n\n\nTable 1: Selected rows from a table at the website for the manuscript: https://voynich.nu/index.html, comparing an early attempt to classify writing “hands” (Currier) to the recent classification (Davis). Hypothesized identity is unique to each column with the numbers denoting individual scribes.\n\n\n\n\n\nFolio(s)\nPrescott Currier\nLisa Fagin Davis\n\n\n\n\nf41, f48, f57r\n2\n5\n\n\nfRos (obverse)\n3\n2\n\n\nfRos (main)\n3\n4\n\n\nf87, f88, f93, 96\n4\n1\n\n\nf94, f95\n5\n3\n\n\nf103, f104, f106\nX\n3\n\n\nf105\nY\n3\n\n\n\n\n\n\nIn Table 1 it’s recorded that Currier thought that the obverse and main folio fRos were in a single hand, whereas Davis has two scribes. Conversely, Davis has the last three rows as the same author while Currier has three. This is an example of how science works toward consensus by placing observations into categories. Agreements are good for pointing to what might be real, and disagreements are for honing or discarding theories.\nThe statistical question for data like this is to compare variation within groups to variation between groups. One application is an idea called “signal-to-noise ratio,” which is a measure of how much of the variation in the data is due to the thing we’re interested in (signal) versus how much is due to random variation (noise). In the manuscript example, the signal is the commonality of handwriting within contiguous sections, and the noise is variation within each section. Reliability of measures is the same idea in different clothing, where we might formulate reliability as between-subject variation divided by total variation. As an example, college grade averages (GPA) can have a reliability of about 0.8, meaning that 80% of the variation in grades is due to differences between students, and 20% is due to variation in how individual students earn grades. This makes GPA useful as a statistical measure, for example in predicting career outcomes.\nHow do we summarize a data set to sort signal from noise? This question involves a deeper question about the true categorization of subjects.\n\n\n\nIt turns out that the questions “what is true?” and “what is knowledge, anyway?” are difficult to answer. There is a great body of work in epistemology, the philosophical study of knowledge, from which we can take a clever idea to get started.\nTo start, we have to imagine that “truth” exists. Since we are talking about classifications of subjects into one of two categories, we must commit to the idea that these categories (words that humans made up) might correspond to something in objective reality (whatever that is). We can easily make up descriptions that are fantasies, like unicorns or flying whales, and the goal is to have a way to distinguish these phantasms from real things like penguins and taxes. To do so, we have to imagine that by some mysterious means, our intellects and discourse can access Nature and name things.\nWords are slippery things, however. Although there are definitions, the real meaning of “pizza” is (ironically) ineffable; one could write volumes on the subject. Let’s take Platos’s definition of a man as a featherless biped. Our goal is a set of classifications “man” or “not man” generated from independent observers (we call them raters) who each inspect some objects in the world (we call them subjects) and assign “man” or “not man” to each. Then we’ll take that data and apply the between- and within-variance idea.\nTherefore the definition isn’t enough. We also need a procedure for generating the data the way we want. To ensure independence of raters, we don’t want them to collaborate on decisions, for example. In the sciences, procedures can be quite elaborate, as in detecting and classifying Higgs bosons, which requires a huge particle accelerator.\nThe story is that Diogneses the Cynic tossed a plucked chicken at Plato’s feet and declared “here’s your man.” The example drives a wedge between the definition “featherless biped” and what is actually meant by “man.” Outside of math and science, definitions and procedures are fuzzy in this way. Rater agreement statistics attempt to defuzz the ratings, to understand the reality behind them.\nHere’s where the idea from epistemology is key, because it gets to the nature of the fuzziness of classification. First we define knowledge as justified true belief (JTB), and then sort out where the fuzziness is.\nThere are three parts to JTB, and we already talked about truth; as a prerequisite we must believe that true statements can be made about the world, which means accessing something real and describing it. The third piece of JTB is belief, which we’ll take to be the classification assigned by a rater. If our rater looks at a horse and writes down “not unicorn,” we’ll assume that the classification represents what the rater believes. This runs into problems if we want to consider adversarial raters, like someone paid to rate consumer products, or an angry customer rating a service poorly for extraneous reasons. We won’t consider those possibilities here.\nThe first piece of JTB is where the fuzziness lives: justification. Outside of math, justification in words isn’t complete2. In the case of Diogenes versus Plato, an follower of the latter might reason that the plucked chicken meets the great philosopher’s definition, and therefore is a man. Others might use common sense to conclude that it’s not a man. There is more than one way to justify belief, but not all of them result in true classifications.\nThe JTB idea was dealt a blow by Gettier, who posed a number of examples where justification goes awry, but the idea is much older. Consider this example from Dharmottara in 779 AD3.\n\nA desert traveller is searching for water. He sees, in the valley ahead, a shimmering blue expanse. Unfortunately, it’s a mirage. But fortunately, when he reaches the spot where there appeared to be water, there actually is water, hidden under a rock. Did the traveller know, as he stood on the hilltop hallucinating, that there was water ahead?\n\nThe traveller had a belief that turned out to be true, but the justification was incorrect. The philosphers are concerned with whether or not this constitutes knowledge4, but we’re more interested in something simpler. That is to categorize the ways truth and belief can happen. If they don’t coincide (if we believe something false), then we’ll assume that the justification was incorrect. As the Gettier problems and the discussion they generated demonstrate, there are a lot of complicated ways in which justification can go awry.\nIn the literature of rater agreement, one can find similar language. In Aickin (1990), we find “The \\(\\alpha\\) agreement parameter is defined as the proportion of a population of items that are classified identically ‘for cause’ by two classifiers, the remaining items being classified at random […].” The author suggests that some subjects are more difficult to rate than others, which hints at modeling each subject’s probability of being in a category, in truth. This idea was explored in Dawid & Skene (1979), who also allowed that raters might have different proficiencies.\nAnother instance comes from the derivation of a rater agreement statistic called AC1 Gwet (2008). The author notes that “the assumption [is] that any rating that is not random will automatically lead to a correct classification, while a random rating leads to a correct classification with probability 1/2.” The idea is that we conceptually separate accurate ratings from random ones, which then means we must consider the probability distribution of the random ones. The random assignments will sometimes be correct, but for the wrong reason–just like the Gettier problems.\n\n\n\nDiagnosing illness is an important classification problem, and the simplest version is a blood test that returns a positive or negative result, perhaps scanning for an antibody. There are four possibilities, which can be organized into what is commonly called a confusion matrix. Suppose that we consider a statement S about the world, like “this patient has the flu.” Using a definition and process we reach a conclusion, so that we believe S to be true or false. This generates the classifications we’re interested in: this is what the raters do, generating lots of S or not-S data on cases.\nThe truth is often inaccessible, but recall that we assume that it exists, so that the statement really is true or false: each patient really does have the flu or does not. Here are the four possibilities:\n\n\n\nTable 2: Sample confusion matrix\n\n\n\n\n\n\n\n\n\n\n\nS is true\nS is false\n\n\n\n\nBelieve S is true\nTrue positive cases\nFalse positive cases\n\n\nBelieve S is false.\nFalse negative cases\nTrue negative cases\n\n\n\n\n\n\nNotice that there are four spots in the table where data can go: if we had perfect knowledge we could count the number of true positive cases, etc. If we converted these numbers to proportions by dividing by the table, we have three parameters left. This foreshadows what comes later: the heart of the classification problem (in its simple form) is a three-parameter problem5.\nThe limitation of casting the confusion matrix into a single-parameter statistic was noted by Cicchetti & Feinstein (1990), who suggested “two separate indexes of proportionate agreement in the observers’ positive and negative decisions. […] Why do we want a single index?” Another example comes from the motivation for the AC1 rater statistic Gwet (2008). It separately considers rater true positives and true negatives, but then assumes that these are identical for each rater, so that the result is a single parameter.\nAs a concrete illustration, consider the wine judging data used in Hodgson (2008). The first five rows look like this:\n\n\n\nTable 3: Wine ratings showing rows 1-5 of 183, taken from the study by Hodgson (personal communication).\n\n\n\n\n\nWine\nJ1\nJ2\nJ3\nJ4\n\n\n\n\n1\n3\n3\n3\n3\n\n\n2\n3\n3\n3\n3\n\n\n3\n3\n3\n2\n4\n\n\n4\n3\n4\n3\n1\n\n\n5\n4\n2\n1\n1\n\n\n\n\n\n\nThe 1-4 scale is ordinal, meaning that we imagine quality to increase with the numerical value. For this discussion, we will reduce the scale to a binary classification: acceptable wine (ratings 2-4) or wine to avoid (rating 1). This simplifies the table to.\n\n\n\nTable 4: Simplified wine ratings, where 1 means “acceptable” and 0 means “avoid.”\n\n\n\n\n\nWine\nJ1\nJ2\nJ3\nJ4\n\n\n\n\n1\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n1\n\n\n3\n1\n1\n0\n1\n\n\n4\n1\n1\n1\n0\n\n\n5\n1\n0\n0\n0\n\n\n\n\n\n\nIntuitively there is more reason to believe wines 1 and 2 are acceptable than with 3 or 4, for which one judge found them not metal-worthy. It seems safe to avoid wine 5, since all but one of the judges found it not to be metal-worthy.\nWe can put this information into the confusion matrix, but we’re missing information.\n\n\n\nTable 5: Confusion matrix for the first five wines, with missing information as question marks.\n\n\n\n\n\n\ntrue value of 2-4\ntrue value of 1\nTotal\n\n\n\n\nrating of 2-4\n?\n?\n15\n\n\nrating of 1\n?\n?\n5\n\n\n\n\n\n\nThere is some awkwardness in imagining that the physical universe encompasses a reality including true wine ratings. It seems absurd on the face of it that the physical laws of the universe imply “wine science.” It’s useful to think of it in reverse: this reification of the ratings is a hypothesis that may be rejected by the statistics we derive from the data. If we find that raters are assigning ratings at random, there’s no evidence here for the reality of wine asthetics. On the other hand, strong indications of non-randomness need to be explained somehow.\nTaking the leap that all non-randomness is causal, and physical reality has a monopoly on causality, then something concrete in the world is behind the rating consistency. We could imagine the chemical similarities and differences of wines and their effects on human anatomy, and so on. Being “real” in this case doesn’t mean that the universe appreciates wine, only that the universe includes physical blobs called humans, and they have mechanical consistencies that correlate observations in this case. The confusion matrix is also used for calculating the causal effect of an experiment. In a medical context, the rows might be treatment categories and the columns patient outcomes. See Eubanks (2014) for more on that.\nBy using the rating counts for each subject instead of the total, we can–surprisingly–estimate the whole confusion matrix. We can even estimate the true classification values for each subject. For that level of detail see Chapter 4: Hierarchical Models.\n\n\n\nSignal and noise as variability between and variability within subjects translates into rater agreement within subjects versus between subjects. The usual way to turn this idea into a number is to count the number of actual agreements and divide by the total number possible.\n\n\n\nTable 6: Rater agreements count, showing the maximum agreements for N raters, the number matches that are evidence versus logical implications and the ratio of evidence to matches.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of raters\nMaximum agreements\nEvidentiary agreements\nImplied agreements\nEvidence / Maximum\n\n\n\n\n1\n0\n-\n-\n-\n\n\n2\n1\n1\n0\n1\n\n\n3\n3\n2\n1\n.67\n\n\n4\n6\n3\n3\n.5\n\n\n5\n10\n4\n6\n.4\n\n\nN\n\\(N(N-1)/2\\)\n\\(N-1\\)\n\\(N(N-1)/2 - N + 1\\)\n\\(2/N\\)\n\n\n\n\n\n\nThe maximum agreements in the table come from the combinatorics formula “N choose 2,” counting the number of possible pairs out of N. If a group of 5 people meet and all shake hands, it’s 10 handshakes. The number of pairs grow much more rapidly than the number of raters does6.\nThere’s a rule in thumb in statistics that it’s preferable to use standard deviation instead of variance when understanding the data. That’s because the units of standard deviation are the same as the original data, whereas the variance has squared units. The same effect is going on with counting agreement. In some sense, what we want looks more like the square root of agreement.\nIntuitively, three raters agreeing doesn’t seem like three times as much evidence as two agreeing. And if we look at where those agreements are coming from, most of them are purely logical. Agreement is transitive, meaning if A agrees with B and B agrees with C, then A agrees with C. This is a logical necessity, and it’s not evidence of anything. The Evidentiary and Implied columns of the table record the distinction between the two types of matches. As the number of raters \\(N\\) increases, the logically necessary agreements greatly outnumber the evidentiary ones, so the ratio in the last column of the table decreases toward zero.\nIf we only counted the evidentiary agreements, the maximum for \\(N\\) raters would be \\(N - 1\\), which is roughly proportional to the square root of the total number of agreements. We’ll come back to that idea later. However, the well-trodden path is to estimate the probability of agreement over all the subjects, which leads to the kappa statistics.\nFor the five wine ratings that are reduced to the binary category “avoid” or “don’t avoid” in Table 4, we can calculate the agreement as follows.\n\n\n\nTable 7: Agreement calculation for the first five wines, showing the maximum possible agreements, the actual agreements, and the agreement proportion out of the maximum.\n\n\n\n\n\nWine\nRaters\nPossible\nActual\nAgreement\n\n\n\n\n1\n4\n6\n6\n1\n\n\n2\n4\n6\n6\n1\n\n\n3\n4\n6\n3\n.5\n\n\n4\n4\n6\n3\n.5\n\n\n5\n4\n6\n3\n.5\n\n\n\n\n\n\nFor the fifth vintage in Table 7, there are more matches with the zeros than there are with the ones; both are counted. It is at this point what seemed like a good starting point begins to turn into a muddle, because since both the zeros and the ones can create agreement, how do we know what’s good enough agreement? What’s the worst agreement possible?\nA group of \\(n_1\\) raters who agree on the 1 ratings produces a number of agreements about proportional to \\(n_1^2\\), and similarly the \\(n_0\\) raters of 0 agreements produce about \\(n_0^2\\) agreements. Together that’s around \\(n_1^2 + n_0^2\\) agreements, which is less than or equal to \\((n_1 + n_0)^2\\), which is about the number of agreements if everyone agreed on a single category. So we get more agreement when everyone agrees than we do when the raters split into groups. That means that the least agreement happens when the raters are evenly split, giving us a floor for agreement.\n\n\n\nTable 8: Minimum rater agreement rates\n\n\n\n\n\n\n\n\n\n\n\nNumber of raters\nMaximum agreements\nMinimum agreements\nMinimum rate\n\n\n\n\n4\n6\n2\n.33\n\n\n6\n15\n6\n.40\n\n\n8\n28\n12\n.43\n\n\n10\n45\n20\n.44\n\n\neven N\n\\(N(N-1)/2\\)\n\\(N(N/2-1)/2\\)\n\\((N/2-1)/(N - )\\)\n\n\n\n\n\n\nFor six or more raters, there’s at least a 40% agreement rate, even when there’s the least possible amount of agreement. It’s annoying that the minimum rate of agreement changes with the number of raters. For real cases when we want to use these ideas, we might have different numbers of raters for different subjects, complicating an average minimum rate, which defines the worst case agreement.\n\n\n\nWe saw in the previous section that the minimum frequency of agreement among raters is quite large, tending toward one half of the maximum possible. Whatever statistic we create as a measure of agreement needs to take that into account if we are to make sense of the results. We’ll get maximum agreement if all the raters choose the same category (of the two choices), and we’ll get minimum agreement if they split evenly between the categories, as if they each flipped a coin to decide. So there’s an association between worst-case random raters and the minimum agreement rate. This is the basis of the chance correction in the S statistic, which assumes coin-flipping raters as the worst-case benchmark from which to measure agreement.\nThe general formula for taking into account a worst-case (random) match rate \\(m_r\\) first computes the practical range of our scale, which is \\(1 - m_r\\), since the statistic can’t be less than \\(m_r\\). The amount of agreement observed \\(m_o\\) is then pinned to this scale as a “chance-corrected” match rate \\(\\kappa\\) (kappa) with\n\\[\\kappa = \\frac{m_o - m_r}{1 - m_r}\\]\nThe S statistic is a special case of this formula, where \\(m_r = .5\\). The sample of wine ratings can be used to illustrate. Recall that earlier we simplified the judges’ scale of “no medal” to “gold medal” to a binary classification between “no medal” (coded as zero) or “any medal” (coded as one). There were 30 maximum agreements among the four raters over the five wines (six per wine), and 21 actual agreements. The minimum is when the ratings are evenly split on each case, generating two agreements for each wine, or 10 total. An S-like statistic would be calculated like this:\n\\[\n\\begin{aligned}\nm_o &= 21/30 = .70 \\\\\nm_r &= 10/30 = .33 \\\\\n\\kappa_s &= \\frac{.70 - .33}{1 - .33} = .47\n\\end{aligned}\n\\]The kappa is now on a scale of zero (worse) to one (perfect agreement). Interpreting what the value means is difficult because it’s on the wrong scale as noted above. A value close to one half sounds mediocre, but not terrible, but we’re really looking for something like the square root of the agreement rate, which is .69. That value corresponds better to the intuition that the agreement in the table looks pretty good.\nThe coin-flipping assumption for randomness can be replaced with other assumptions. This has the effect of increasing the assumed worst-case agreement rate and hence decreasing kappa values. Perhaps the most popular choice is to reason that the raters are more likely to randomly pick the most common category so the coin should be weighted according to the rating distribution.\nFor example, the table of simplified wine ratings has 15 ratings of 1 and 5 of 0, or 75% 1-ratings. The worst case match rate is then the product of the proportions of 1 ratings, or \\(m_r = (.75)(.75) + (.25)(.25) = .625\\). This is the assumption of the Fleiss kappa. Note that the actual worst case match rate is still close to one half (because of the math), so it’s possible to get a negative Fleiss kappa if the assumption about the distribution of randomness fails.\nOne calculation under this proportionality assumption is:\n\\[\n\\begin{aligned}\nm_o &= 21/30 = .70 \\\\\nm_r &= .625 \\\\\n\\kappa_f &= \\frac{.70 - .625}{1 - .625} = .20\n\\end{aligned}\n\\]The actual calculations of these statistics are complicated by the choice of using exact combinatorical counts or long-run estimates. This distinction need not concern us in here.\nThe deflation of the rater agreement statistic in the Fleiss calculation above makes it clear that assumptions about random agreement are critical. A researcher may justifiably wonder which value is correct: are the wine judges in pretty good agreement, or is the judging mostly random? For historical reasons, rather than treating rater disagreement as a parameter to be discovered, the field has evolved to propose many variations of the kappa formula with different assumptions about the worst-case agreement rate.\nA limitation of most of the kappa statistics is that they make the simplifying assumption that a pair of raters reaches agreement randomly if both raters are random. This omits the possibility that one rater is accurate and the other isn’t.\n\n\n\nThere are by now several statistics advertised as rater agreement statistics, including Cohen’s kappa, Fleiss’s kappa, Scott’s pi, Krippendorff’s alpha, S, and AC1. For convenience, these statistics will be referred to generically as “kappas.” Researchers who want a simple answer are faced with a bewildering set of options and claims about them. In Vach & Gerke (2023) we get a sense of the situation.\n\nGwet’s AC1 has been proposed as an alternative to Cohen’s kappa in evaluating the agreement between two binary ratings. This approach is becoming increasingly popular, and researchers have been criticized for still using Cohen’s kappa. However, a rigorous discussion of properties of Gwet’s AC1 is still missing.\n\nThe authors illustrate both the need for clarification and the faddishness that research communities can adopt when lacking real criteria. The kappas are ill-suited to answer such questions, and the proliferation of agreement statistics makes it difficult to compare results across studies or to know which one to use. Given pressures to publish results, there may be a tendency to use the statistic that gives the highest value, or to use the one that is most familiar. See Button et al. (2020) for a discussion of rater agreement statistics in the context of the “crisis in confidence in psychological research.”\nIt’s not just difficult to know what kappa to use, there are no meaningful guides to interpreting the results. “There is a wide distinction in the elucidation of Kappa values, and several efforts have been made to assign practical meaning to calculated Kappa values,” wrote the authors of Chaturvedi & Shweta (2015), who mention a widely-used heuristic found in Landis & Koch (1977) that proposed a translation between numerical values of kappa and qualitative descriptions of agreement, such as \\(\\kappa \\ge .81\\) is “almost perfect.” The categories are arbitrary, do not translate well between different agreement statistics (which can give different values for the same data) and their assumptions, and do not provide insight into how to improve ratings.\nThe agreement statistics for ratings have problems that should now be evident. The goal is to understand a three-dimensional relationship between ratings and true values, but the statistics are single parameters. A second parameter, the worst case baseline, is buried in an assumption, which varies by kappa, and is not tested for fit to the data. The result is a bewildering array of choices for rater agreement measures. As a result of this confusion, different cultures have emerged. If getting published is the goal, then higher agreement rates are more desirable, so a researcher can shop around for the “best” one.\nA research agenda was suggested in Landis & Koch (1977) that can be paraphrased as understanding (1) the true category of each subject, (2) the accuracy of raters, (3) truth and accuracy within sub-populations of subjects, (4) conditions that cause disagreement, and (5) what distinguishes “for cause” agreement and random agreement. Some of this can be accomplished by taking a fresh look at the kappas.\n\n\n\nThe geyser of data produced in the information age has led to new methods of analyzing it that fall generally in the description of “machine learning (ML).” Assigning categories to subjects is a common task in machine learning, and the field has developed a number of methods to do so, including neural networks, support vector machines, and random forests. The goal is to assign categories to subjects in a way that generalizes to new subjects. The methods are often evaluated by comparing the predicted categories to the true categories, and the results are summarized in a confusion matrix. See Carpenter (2008) for a good example of this literature.\nIt may come as a surprise to both behavioral scientists and machine learning researchers that they seem to have independently worked on the rater agreement problem, since the two fields’ literatures don’t seem to overlap. Articles on rater agreement don’t cite machine learning papers, and vice versa. However, the two fields are working on the same problem: how to measure the accuracy of a categorization when we don’t have the true categories to reference.\nThere are differences in the methods and philosophies of the two cultures. The kappa approach is implicitly backward-looking, asking “how accurate were these ratings,” with the assumption that the measured accuracy (a kappa) will carry forward to future instances. The ML approach is more forward-looking, with a suite of tools like cross-validation and a vocabulary (bias-variance trade-off) to measure generalizability. As we have seen, the kappa approach is to reason out a plausible chance-correction calculation and use it to produce the single-parameter kappa. The ML approach is to estimate three parameters with regression models. Finally, the kappas are grounded in the psychology of human classification and come with philosophical links to epistemology, which I described earlier in Section 1.2. The ML algorithms are just statistics and code, barren of philosophical considerations.\nThe advantage of the ML approaches over the kappas is that the three parameter models avoid the confusions of the kappa zoo. But because they lack philosophical grounding, they run into an embarrassment of riches: three parameters is sometimes too many parameters for a model, so there can be multiple solutions. There are work-arounds, but it’s the same species of ad hoc reasoning that causes the problems of the kappa zoo. Fortunately, there’s a way to combine the best of both cultures to avoid most of these problems.\n\n\n\nWe can now describe a regression model that allows us to estimate the four proportions that appear in the confusion matrix (see Section 1.3). Since the four cells sum to one, there are three free parameters to estimate.\n\n: Confusion matrix with entries to be filled in by the model estimates. The four numbers are proportions and sum to one, leaving three free parameters to estimate. C0 = class zero, and C1 = class one, standing in for any binary categories we might choose.\n\n\n\nTrue C1\nTrue C0\n\n\n\n\nClassified C1\n\n\n\n\nClassified C0\n\n\n\n\n\nTwo philosophical assumptions are needed to get started:\n\nThe classifications are real in the sense that true values exist.\nThe true values have a binary causal effect on the classification process.\n\nThese assumptions are provisional like hypothesis in statistics; the evidence will provide some support for the assumptions, ranging from no support at all to quite good. The reality of the truth values posed in assumption one is not like Plato’s cave shadows. In the case of wine judging, we can’t say there is a universal ideal for good wine, but if rater agreement is high we can say that some physical causal process exists for translating the observable subject (tasting the wine) into a category. So in that sense the category exists as part of the world. The translation is imperfect, because the conditions are not always perfect for the cause to happen, as with the Gettier problems7.\nThis may seem fiddly, but it gives us a place to start. For any given subject to be rated (e.g. object to be classified) we provisionally assume that there’s a latent truth value that we can never know, but might find evidence for. The second assumption then allows us to provisionally assume that the causal effect of the true value in the context of the classification process has the following nature:\n\nThe rater either assigns the correct class due to the causal pathway operating to connect the observation to the class (justified true knowledge), or\nSomething goes wrong with the causal pathway (the conditions weren’t quite right, etc.) and the classification is assigned non-causally, which is to say randomly.\n\nThe key to this is that the cause either works to connect the true value to the rater’s assigned value, or it fails completely. There’s no “partial cause.” When it fails, the rating is generated from a random process called a Bernoulli trial. It’s the simplest possible type of randomness, taking only two values with some fixed probability, like flipping a weighted coin.\n\n\nCode\n%%{init: {'themeCSS': '.node text { font-family: cursive; }'}}%%\n%%| label: fig-tap-concept\n%%| fig-cap: Conceptual map of t-a-p model\nflowchart TB\n  A(Rating) --&gt; |\"proportion a\"|B(Accurate)\n  A --&gt; |\"proportion 1-a\"|C(Inaccurate)\n  B --&gt; |\"proportion t\"|D[True Class 1]\n  B --&gt; |\"proportion 1-t\"|F[True Class 0]\n  C --&gt; |\"proportion 1-p\"|H[Random Class 0]\n  C --&gt; |\"proportion p\"|G[Random Class 1]\n\n\n\n\n\n%%{init: {'themeCSS': '.node text { font-family: cursive; }'}}%%\n%%| label: fig-tap-concept\n%%| fig-cap: Conceptual map of t-a-p model\nflowchart TB\n  A(Rating) --&gt; |\"proportion a\"|B(Accurate)\n  A --&gt; |\"proportion 1-a\"|C(Inaccurate)\n  B --&gt; |\"proportion t\"|D[True Class 1]\n  B --&gt; |\"proportion 1-t\"|F[True Class 0]\n  C --&gt; |\"proportion 1-p\"|H[Random Class 0]\n  C --&gt; |\"proportion p\"|G[Random Class 1]\n\n\n\n\n\n\nLet’s take inventory of the three parameters that come from the reasoning displayed in the diagram.\n\nAmong the subjects being rated there is a fraction \\(t\\) that are in reality Class 1, with the remaining \\(1-t\\) being Class 0.\nAmong the ratings there is a fraction \\(a\\) that are accurate ratings (justified true knowledge) where the causal connection worked.\nFor the remaining \\(1-a\\) ratings, the causal connection failed (as with a Gettier problem), and there is some probability \\(p\\) that describes the frequency that ratings are randomly assigned to Class 1. The remaining \\(1-p\\) are assigned Class 0.\n\nAll three of these parameters are proportions ranging from zero to one, and can be treated as probabilities that we estimate from appropriate regression models. Setting it up this way, instead of the usual machine learning parameterization avoids the most significant problem with non-identifiability (multiple solutions), which is class-switching. That happens when the model can fit the data, but is agnostic about which class is which, and so attempts to fit the model both ways at once.\n\n\n\n\n\n\nFigure 1: tap diagram\n\n\n\nThe diagram in Figure 1 shows probabilistic links between states of the world and rater classifications for a single rating. Upper case letters will be used here for binary states, with\n\n\\(C_{ij}\\) being the rating assigned to the \\(i\\)th subject by the \\(j\\)th rater. The value of \\(C_{ij}\\) is determined at the bottom of the diagram, contingient on the classification process.\n\\(A_{ij}\\) is 1 if that rating was accurate (JTB), or 0 otherwise.\n\\(T_i\\) is the true class (zero or one) of the \\(i\\)th subject.\n\\(P_{ij}\\) matters only if the \\(i,j\\) rating was inaccurate, (\\(A_{ij} = 0\\)). In that case, a random assignment of zero or one is made.\n\nThese binary events are assumed to be independent of one another, except that the \\(T_i\\) true classification is fixed over all ratings. We’ll also assume that each of these binary outcomes has a fixed average value. In the notation, these are lower-case letters corresponding to the ones in Figure 1.\n\n\\(c\\) is the proportion of Class 1 ratings assigned.\n\\(t\\) is the proportion of true Class 1 cases.\n\\(a\\) is the fraction of classifications that are accurate.\n\\(p\\) is the proportion of randomly-assigned classifications that are Class 1.\n\nThese averages replace the individual binary states to comprise the average t-a-p model.\n\n\n\n\n\n\nFigure 2: tap diagram\n\n\n\nIn Figure 2, the variables \\(c, a, t, p\\) are all probabilities, with a bar over the symbol to denote its complement, i.e. \\(\\bar{t} := 1-t\\). At the end of each branch is the classification (zero or one) and its probability.\nTo illustrate the ideas here, consider a judge (the rater) tasting one of the wines in a competition (the subject). Because the classification is binary, assume that the rating is either “1 = acceptable” or “0 = not acceptable.” The model assumes that each wine being tasted has a true value of acceptability. Over all the wines, the proportion of acceptable wines is \\(t\\). Suppose this one is, in fact, acceptable, so that \\(T_i = 1\\) in Figure 1. There’s some probability \\(a\\), which we’ll call accuracy, that the judge’s perceptive powers will reveal this true quality of the wine. If this happens, then \\(A_{ij] = 1\\), recording an accurate rating, and the resulting classification is necessarily Class 1. That event would be tracing down the left side of the tree diagram, and over all judges and wines, the probability of that is \\(at\\). If the judge’s perspicacity desserts him, and he makes an inaccurate rating, this doesn’t mean he automatically gets the wrong answer! Recall the Gettier-like problems, where we can accidentally get the correct answer even though our reasoning is flawed. Instead, there’s a random chance \\(p\\) of assigning a Class 1 rating. The overall probability that a random (inaccurate) Class 1 rating is assigned is the branch ending with \\(\\bar{a}p = (1-a)p\\) at bottom right in Figure 2.\nIt’s a critical point that the true classifcation of each subject (\\(T_i\\)) is the same regardless of who’s rating it. So when four judges all rate the same wine, they are all either on the left side of the diagram (if the wine is acceptable in reality) or on the right side (if not). The ratings are determined only by \\(a\\) and \\(p\\) at that point. It’s this commonality of truth that allows us to study within-subject variation versus between-subject variation.\nFor a wine chosen at random, we can compute the probabilities of rater classifications. An acceptable wine will be classified accurately with a proportion of \\(ta\\), multiplying the probabilities along the leftmost edge from top to bottom. An inaccurate rating of \\(\\hat{C_1}\\) (acceptability) can come from either the left or right side of the diagram, and if we add those together we get \\(t\\bar{a}p + \\bar{t}\\bar{a}p\\), and since \\(t + \\bar{t} = 1\\) that expression reduces to \\(\\bar{a}p\\).\nAssuming we can estimate the three parameters from the data, we can then populate the confusion matrix by tracing the diagram down to each of the four outcomes, and multiplying probabilities as we go.\n\n\n\nTable 9: The t-a-p model’s correspondence to the confusion matrix. Terms in parentheses are inaccurate ratings.\n\n\n\n\n\n\n\n\n\n\n\nTrue C1\nTrue C0\n\n\n\n\nClassified C1\n\\(ta + (t\\bar{a}p)\\)\n\\((\\bar{t}\\bar{a}p)\\)\n\n\nClassified C0\n\\((t\\bar{a}\\bar{p})\\)\n\\(\\bar{t}a + (\\bar{t}\\bar{a}\\bar{p})\\)\n\n\n\n\n\n\nThe entries in Table 9 demonstrate that if the t-a-p model fits the data and we are able to estimate the three parameters, it is a general answer to the rater agreement question. The later sections show how S, Fleiss kappa, and other statistics are special cases of t-a-p models.\n\n\n\nThe sections above used five rows of the wine judging data for illustration. We now use the whole data set of 183 wines each rated by four judges to estimate the t-a-p parameters. The included app was used (see Chapter 7) for the analysis.\nThe binary classification illustrated above was to convert the original scale (1 = no medal, 2 = bronze, 3 = silver, 4 = gold medal) to a binary outcome where 1 = any medal (bronze, silver, gold), and 0 = no medal. We called the Class 1 category “acceptable.” Using the app to analyze that binary comparison we get the following output.\n\n\n\n\n\n\nFigure 3: Wine rating t-a-p analysis comparing C1 = ratings 1,2,3 (in-class) to C0 = rating 0.\n\n\n\nThe acceptable wines are the primary (in-) class, and the plot shows the distribution of ratings for acceptability. Since there are four judges, the possibilities are:\n\nNone of them could find the wine acceptable. This is the 0 at the left end of the plot. It happens for about 6% of the wines.\nAll four of them could find the wine acceptable. This is the 4 (four judges) at the right end of the plot. That happened for about 49% of the wines.\nSomething in between, where the judges disagree. In that case we can have from one to three ratings of “acceptable,” corresponding to those points on the plot.\n\nThe lollipops (black lines with dots on top) in Figure 3 show the empirical data straight out of the ratings. The dashed blue line is the model fit, showing what the distribution would be if the model were exactly correct. Here, “model” means applying the estimated parameters to generate the distribution.\nThe estimated parameters are found at the top of the plot:\n\n\\(t\\) = .73 estimates that 73% of wines are acceptable in reality. This is more than the rate of unanimous agreement, which we saw above was only 49%.\n\\(a\\) = .54 estimates that the judges make accurate ratings (justified true belief) more than half the time. The rest of the ratings are assumed to be random.\n\\(p\\) = .78 estimates that when random ratings are made, judges choose “acceptable” 78% of the time. Notice that this is close to the estimated actual rate of 73% (the estimated value of \\(t\\) above). This turns out to be a desirable quality in judges. See Chapter 3 for details, especially the section on the Fleiss kappa.\n\nNote that the randomness of inaccurate ratings is not a conscious choice of raters. They aren’t giving up and flipping a coin. They still have a reasonable basis for making a rating, and may be quite confident about it. The assumption is that something Gettier-like has gone wrong with the causal process that links the input (wine) to output (rating), which flips the process from deterministic to stochastic. As with any regression model, the world doesn’t have to actually be that way; it’s just an assumption to allow us to create the model. Then we check to see if the data matches the model. In this case, the data matches the model pretty well, as we can visually assess by comparing the blue dashed line to the lollipops. More formal tests of agreement will have to wait until later.\nThe wine ratings comprise four categories that are naturally ordered from 1 = “no medal” to 4 = “gold medal.” That kind of data is common: from surveys with Strongly Disagree to Strongly Agree scales, from consumer ratings, and many other sources. We humans seem to have a natural facility to think of descriptions along an imaginary intensity scale. This data type is called “ordinal,” meaning we can put the categories in order.\nThe t-a-p model is designed to work with binary data. It’s possible to switch to a multinomial model, but it’s common to analyze ordinal scales using binary distinctions. I’ll show how that works with the wine data. Along the 1,2,3,4 scale, there are three sequential cut-points where the commas are. The first one we can denote “1|2”, which splits the scores into two sets: an in-class {1} and and out-class {2,3,4}. This is the same thing as the Class 1 and Class 0 distinction from earlier. The second cut-point is at 2|3, splitting the data into ratings {1,2} and {3,4}. The plot shows the fraction of ratings in the in- and out-class for each cut-point.\n\n\n\n\n\n\nFigure 4: Wine ratings divided into binary groups by cutpoint.\n\n\n\nThe 3|4 bar of the plot shows us that 88% of ratings are in {1,2,3}, so the remaining 12% are 4s (gold medal). As a validity check, there are 183 wines, each rated by four judges, so that should be 732 ratings. That looks like the height on the bars, so it checks out.\nFor a given cut-point, a wine rating will either be in the in-class or out-class This converts the scale to binary, and then we can estimate the model parameters. We repeat the t-a-p model for each cut-point to create a pattern like the one in the plot below.\n\n\n\n\n\n\nFigure 5: t-a-p parameter estimates for ordinal scale based on cutpoints\n\n\n\nThe plot in Figure 5 shows each t-a-p parameter for each cut-point. As a reference, the Fleiss kappa estimates are also included as dotted lines. The significance of the Fleiss kappa is discussed later.\nThe accuracy parameter \\(a\\) at the top shows good accuracy for the lowest cut-point, meaning that the judges were good at distinguishing the least worthy wines from the rest. This is the case we analyzed earlier when we called the 1 rating unacceptable. As the quality rating increases, moving to the right on the top plot, accuracy decreases to less than half its value for the first cut-point (focus on the solid line). This would be the case if poor wines have more basis in physiology (sourness, etc.) and as the assessments become more aesthetic, they become more individualized and have less group agreement.\nThe second plot, showing estimates for \\(p\\) show the probability with which judges place a wine randomly into the in-class for inaccurate ratings. This will happen less often for the left-most cut-points, since accuracy is higher there. The dotted line is useful here: it shows what the \\(p\\) parameter would look like if the raters assigned ratings proportionally when making inaccurate classifications. For example, we noted earlier that 88% of the ratings are 1-3 (rightmost bar of the previous plot), so for the 3|4 cut-point, proportional random ratings would assign 88% of the inaccurate ratings into the {1,2,3} in-class. That’s where the dotted line is, at 88%. The actual parameter estimate (solid line) is at about 84%, meaning that judges are probably too conservative about assigning the gold medal category. Accuracy is low for the gold medals, and when the judges rate inaccurately, the ratings are slightly biased toward the lower ratings.\nThe bottom plot in Figure 5 estimates the true proportions for each cut-point, after taking into account the other two parameters. For the 3|4 cut-point on the right, it shows a proportion of {1,2,3} wines of about 65%. This is much lower than the 88% of ratings that are {1,2,3}. That’s the combined effect of inaccurate ratings at the top end of the scale combined with the bias toward lower ratings for inaccurate ratings. Looking at the two ends of the scale for the \\(t\\) plot, we can estimate that about 26% of wines are truly in the 1 = no medal category, and about 35% are in the 4 = gold medal category (1 - .65 = .35). That 35% figure comes from reading the estimated value of \\(t\\) at the 3|4 cut point, which is about 65%, and subtracting from one.\nIt’s possible to tell good wine from bad wine pretty reliably in this data set, but beyond that individual tastes may not be discerning enough to sort out four levels of wine quality. The scale could possibly be reduced to three ratings, or else keep the existing scale but collapse the 3 and 4 ratings into a single category before reporting the results. The effect of the existing rating system is leaving a lot of probably excellent wines with poorer ratings than they deserve. An alternative approach is to try to improve the accuracy of the higher ratings, which can be facilitated by reducing the rater bias against the highest award. One study showed that it’s possible to improve reliability in college grade assignment through feedback Millet (2010). This method might work more generally for reducing rater bias."
  },
  {
    "objectID": "introduction.html#sec-intro-overview",
    "href": "introduction.html#sec-intro-overview",
    "title": "Introduction",
    "section": "",
    "text": "Humans often use consensus to assess knowledge. We might seek a second opinion for a diagnosis, ask around for restaurant recommendations, or look for online reviews of a product. Intiuitively, we put more weight on opinions that have more agreement. This chapter describes how agreement or disagreement can be quantified. These agreement statistics are restricted to situations where the assessments are made independently and where each subject is assessed at least twice. The combination of independence and repeatability are abbreviated in probability theory as “iid” for “independent and identically distributed.” This is a powerful assumption that allows us to reach strong conclusions. However, it is not always realistic.\nIn behavioral science, the focus of these statistics is on human observers who we’ll call “raters.” Raters assign “ratings” to “subjects.” This describes a familiar case like a product review with a five-star rating system. However, the same idea applies to any categorization task, like classifying images of cats and dogs, or diagnosing diseases from medical images. A fascinating example comes from an article in The Atlantic Magazine, here describing an analysis of a very old text called the Voynich manuscript.\n\nDavis magnified the handwriting, she noticed subtle variations. In certain places, the script was more cramped or more likely to slant as it crossed the page. She tested this observation by picking a letter that didn’t appear often and tracking it across the manuscript’s pages. Its style, she saw, varied among groups of pages but not within those groups. This suggested that the differences—larger or smaller loops, straighter or curvier crossbars, longer or shorter feet—were the product of different scribes rather than of one scribe writing the same letter in different ways.\n\nSee Davis’s paper for details1. For our purposes, the “raters” are scholars who categorize writing styles, the “subjects” are sections (folios) of the manuscript they are reviewing, and the presumed different scribes are the categories we generically refer to as “ratings.”\n\n\n\nTable 1: Selected rows from a table at the website for the manuscript: https://voynich.nu/index.html, comparing an early attempt to classify writing “hands” (Currier) to the recent classification (Davis). Hypothesized identity is unique to each column with the numbers denoting individual scribes.\n\n\n\n\n\nFolio(s)\nPrescott Currier\nLisa Fagin Davis\n\n\n\n\nf41, f48, f57r\n2\n5\n\n\nfRos (obverse)\n3\n2\n\n\nfRos (main)\n3\n4\n\n\nf87, f88, f93, 96\n4\n1\n\n\nf94, f95\n5\n3\n\n\nf103, f104, f106\nX\n3\n\n\nf105\nY\n3\n\n\n\n\n\n\nIn Table 1 it’s recorded that Currier thought that the obverse and main folio fRos were in a single hand, whereas Davis has two scribes. Conversely, Davis has the last three rows as the same author while Currier has three. This is an example of how science works toward consensus by placing observations into categories. Agreements are good for pointing to what might be real, and disagreements are for honing or discarding theories.\nThe statistical question for data like this is to compare variation within groups to variation between groups. One application is an idea called “signal-to-noise ratio,” which is a measure of how much of the variation in the data is due to the thing we’re interested in (signal) versus how much is due to random variation (noise). In the manuscript example, the signal is the commonality of handwriting within contiguous sections, and the noise is variation within each section. Reliability of measures is the same idea in different clothing, where we might formulate reliability as between-subject variation divided by total variation. As an example, college grade averages (GPA) can have a reliability of about 0.8, meaning that 80% of the variation in grades is due to differences between students, and 20% is due to variation in how individual students earn grades. This makes GPA useful as a statistical measure, for example in predicting career outcomes.\nHow do we summarize a data set to sort signal from noise? This question involves a deeper question about the true categorization of subjects."
  },
  {
    "objectID": "introduction.html#sec-intro-knowledge",
    "href": "introduction.html#sec-intro-knowledge",
    "title": "Introduction",
    "section": "",
    "text": "It turns out that the questions “what is true?” and “what is knowledge, anyway?” are difficult to answer. There is a great body of work in epistemology, the philosophical study of knowledge, from which we can take a clever idea to get started.\nTo start, we have to imagine that “truth” exists. Since we are talking about classifications of subjects into one of two categories, we must commit to the idea that these categories (words that humans made up) might correspond to something in objective reality (whatever that is). We can easily make up descriptions that are fantasies, like unicorns or flying whales, and the goal is to have a way to distinguish these phantasms from real things like penguins and taxes. To do so, we have to imagine that by some mysterious means, our intellects and discourse can access Nature and name things.\nWords are slippery things, however. Although there are definitions, the real meaning of “pizza” is (ironically) ineffable; one could write volumes on the subject. Let’s take Platos’s definition of a man as a featherless biped. Our goal is a set of classifications “man” or “not man” generated from independent observers (we call them raters) who each inspect some objects in the world (we call them subjects) and assign “man” or “not man” to each. Then we’ll take that data and apply the between- and within-variance idea.\nTherefore the definition isn’t enough. We also need a procedure for generating the data the way we want. To ensure independence of raters, we don’t want them to collaborate on decisions, for example. In the sciences, procedures can be quite elaborate, as in detecting and classifying Higgs bosons, which requires a huge particle accelerator.\nThe story is that Diogneses the Cynic tossed a plucked chicken at Plato’s feet and declared “here’s your man.” The example drives a wedge between the definition “featherless biped” and what is actually meant by “man.” Outside of math and science, definitions and procedures are fuzzy in this way. Rater agreement statistics attempt to defuzz the ratings, to understand the reality behind them.\nHere’s where the idea from epistemology is key, because it gets to the nature of the fuzziness of classification. First we define knowledge as justified true belief (JTB), and then sort out where the fuzziness is.\nThere are three parts to JTB, and we already talked about truth; as a prerequisite we must believe that true statements can be made about the world, which means accessing something real and describing it. The third piece of JTB is belief, which we’ll take to be the classification assigned by a rater. If our rater looks at a horse and writes down “not unicorn,” we’ll assume that the classification represents what the rater believes. This runs into problems if we want to consider adversarial raters, like someone paid to rate consumer products, or an angry customer rating a service poorly for extraneous reasons. We won’t consider those possibilities here.\nThe first piece of JTB is where the fuzziness lives: justification. Outside of math, justification in words isn’t complete2. In the case of Diogenes versus Plato, an follower of the latter might reason that the plucked chicken meets the great philosopher’s definition, and therefore is a man. Others might use common sense to conclude that it’s not a man. There is more than one way to justify belief, but not all of them result in true classifications.\nThe JTB idea was dealt a blow by Gettier, who posed a number of examples where justification goes awry, but the idea is much older. Consider this example from Dharmottara in 779 AD3.\n\nA desert traveller is searching for water. He sees, in the valley ahead, a shimmering blue expanse. Unfortunately, it’s a mirage. But fortunately, when he reaches the spot where there appeared to be water, there actually is water, hidden under a rock. Did the traveller know, as he stood on the hilltop hallucinating, that there was water ahead?\n\nThe traveller had a belief that turned out to be true, but the justification was incorrect. The philosphers are concerned with whether or not this constitutes knowledge4, but we’re more interested in something simpler. That is to categorize the ways truth and belief can happen. If they don’t coincide (if we believe something false), then we’ll assume that the justification was incorrect. As the Gettier problems and the discussion they generated demonstrate, there are a lot of complicated ways in which justification can go awry.\nIn the literature of rater agreement, one can find similar language. In Aickin (1990), we find “The \\(\\alpha\\) agreement parameter is defined as the proportion of a population of items that are classified identically ‘for cause’ by two classifiers, the remaining items being classified at random […].” The author suggests that some subjects are more difficult to rate than others, which hints at modeling each subject’s probability of being in a category, in truth. This idea was explored in Dawid & Skene (1979), who also allowed that raters might have different proficiencies.\nAnother instance comes from the derivation of a rater agreement statistic called AC1 Gwet (2008). The author notes that “the assumption [is] that any rating that is not random will automatically lead to a correct classification, while a random rating leads to a correct classification with probability 1/2.” The idea is that we conceptually separate accurate ratings from random ones, which then means we must consider the probability distribution of the random ones. The random assignments will sometimes be correct, but for the wrong reason–just like the Gettier problems."
  },
  {
    "objectID": "introduction.html#sec-intro-confusion",
    "href": "introduction.html#sec-intro-confusion",
    "title": "Introduction",
    "section": "",
    "text": "Diagnosing illness is an important classification problem, and the simplest version is a blood test that returns a positive or negative result, perhaps scanning for an antibody. There are four possibilities, which can be organized into what is commonly called a confusion matrix. Suppose that we consider a statement S about the world, like “this patient has the flu.” Using a definition and process we reach a conclusion, so that we believe S to be true or false. This generates the classifications we’re interested in: this is what the raters do, generating lots of S or not-S data on cases.\nThe truth is often inaccessible, but recall that we assume that it exists, so that the statement really is true or false: each patient really does have the flu or does not. Here are the four possibilities:\n\n\n\nTable 2: Sample confusion matrix\n\n\n\n\n\n\n\n\n\n\n\nS is true\nS is false\n\n\n\n\nBelieve S is true\nTrue positive cases\nFalse positive cases\n\n\nBelieve S is false.\nFalse negative cases\nTrue negative cases\n\n\n\n\n\n\nNotice that there are four spots in the table where data can go: if we had perfect knowledge we could count the number of true positive cases, etc. If we converted these numbers to proportions by dividing by the table, we have three parameters left. This foreshadows what comes later: the heart of the classification problem (in its simple form) is a three-parameter problem5.\nThe limitation of casting the confusion matrix into a single-parameter statistic was noted by Cicchetti & Feinstein (1990), who suggested “two separate indexes of proportionate agreement in the observers’ positive and negative decisions. […] Why do we want a single index?” Another example comes from the motivation for the AC1 rater statistic Gwet (2008). It separately considers rater true positives and true negatives, but then assumes that these are identical for each rater, so that the result is a single parameter.\nAs a concrete illustration, consider the wine judging data used in Hodgson (2008). The first five rows look like this:\n\n\n\nTable 3: Wine ratings showing rows 1-5 of 183, taken from the study by Hodgson (personal communication).\n\n\n\n\n\nWine\nJ1\nJ2\nJ3\nJ4\n\n\n\n\n1\n3\n3\n3\n3\n\n\n2\n3\n3\n3\n3\n\n\n3\n3\n3\n2\n4\n\n\n4\n3\n4\n3\n1\n\n\n5\n4\n2\n1\n1\n\n\n\n\n\n\nThe 1-4 scale is ordinal, meaning that we imagine quality to increase with the numerical value. For this discussion, we will reduce the scale to a binary classification: acceptable wine (ratings 2-4) or wine to avoid (rating 1). This simplifies the table to.\n\n\n\nTable 4: Simplified wine ratings, where 1 means “acceptable” and 0 means “avoid.”\n\n\n\n\n\nWine\nJ1\nJ2\nJ3\nJ4\n\n\n\n\n1\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n1\n\n\n3\n1\n1\n0\n1\n\n\n4\n1\n1\n1\n0\n\n\n5\n1\n0\n0\n0\n\n\n\n\n\n\nIntuitively there is more reason to believe wines 1 and 2 are acceptable than with 3 or 4, for which one judge found them not metal-worthy. It seems safe to avoid wine 5, since all but one of the judges found it not to be metal-worthy.\nWe can put this information into the confusion matrix, but we’re missing information.\n\n\n\nTable 5: Confusion matrix for the first five wines, with missing information as question marks.\n\n\n\n\n\n\ntrue value of 2-4\ntrue value of 1\nTotal\n\n\n\n\nrating of 2-4\n?\n?\n15\n\n\nrating of 1\n?\n?\n5\n\n\n\n\n\n\nThere is some awkwardness in imagining that the physical universe encompasses a reality including true wine ratings. It seems absurd on the face of it that the physical laws of the universe imply “wine science.” It’s useful to think of it in reverse: this reification of the ratings is a hypothesis that may be rejected by the statistics we derive from the data. If we find that raters are assigning ratings at random, there’s no evidence here for the reality of wine asthetics. On the other hand, strong indications of non-randomness need to be explained somehow.\nTaking the leap that all non-randomness is causal, and physical reality has a monopoly on causality, then something concrete in the world is behind the rating consistency. We could imagine the chemical similarities and differences of wines and their effects on human anatomy, and so on. Being “real” in this case doesn’t mean that the universe appreciates wine, only that the universe includes physical blobs called humans, and they have mechanical consistencies that correlate observations in this case. The confusion matrix is also used for calculating the causal effect of an experiment. In a medical context, the rows might be treatment categories and the columns patient outcomes. See Eubanks (2014) for more on that.\nBy using the rating counts for each subject instead of the total, we can–surprisingly–estimate the whole confusion matrix. We can even estimate the true classification values for each subject. For that level of detail see Chapter 4: Hierarchical Models."
  },
  {
    "objectID": "introduction.html#sec-intro-agreement",
    "href": "introduction.html#sec-intro-agreement",
    "title": "Introduction",
    "section": "",
    "text": "Signal and noise as variability between and variability within subjects translates into rater agreement within subjects versus between subjects. The usual way to turn this idea into a number is to count the number of actual agreements and divide by the total number possible.\n\n\n\nTable 6: Rater agreements count, showing the maximum agreements for N raters, the number matches that are evidence versus logical implications and the ratio of evidence to matches.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of raters\nMaximum agreements\nEvidentiary agreements\nImplied agreements\nEvidence / Maximum\n\n\n\n\n1\n0\n-\n-\n-\n\n\n2\n1\n1\n0\n1\n\n\n3\n3\n2\n1\n.67\n\n\n4\n6\n3\n3\n.5\n\n\n5\n10\n4\n6\n.4\n\n\nN\n\\(N(N-1)/2\\)\n\\(N-1\\)\n\\(N(N-1)/2 - N + 1\\)\n\\(2/N\\)\n\n\n\n\n\n\nThe maximum agreements in the table come from the combinatorics formula “N choose 2,” counting the number of possible pairs out of N. If a group of 5 people meet and all shake hands, it’s 10 handshakes. The number of pairs grow much more rapidly than the number of raters does6.\nThere’s a rule in thumb in statistics that it’s preferable to use standard deviation instead of variance when understanding the data. That’s because the units of standard deviation are the same as the original data, whereas the variance has squared units. The same effect is going on with counting agreement. In some sense, what we want looks more like the square root of agreement.\nIntuitively, three raters agreeing doesn’t seem like three times as much evidence as two agreeing. And if we look at where those agreements are coming from, most of them are purely logical. Agreement is transitive, meaning if A agrees with B and B agrees with C, then A agrees with C. This is a logical necessity, and it’s not evidence of anything. The Evidentiary and Implied columns of the table record the distinction between the two types of matches. As the number of raters \\(N\\) increases, the logically necessary agreements greatly outnumber the evidentiary ones, so the ratio in the last column of the table decreases toward zero.\nIf we only counted the evidentiary agreements, the maximum for \\(N\\) raters would be \\(N - 1\\), which is roughly proportional to the square root of the total number of agreements. We’ll come back to that idea later. However, the well-trodden path is to estimate the probability of agreement over all the subjects, which leads to the kappa statistics.\nFor the five wine ratings that are reduced to the binary category “avoid” or “don’t avoid” in Table 4, we can calculate the agreement as follows.\n\n\n\nTable 7: Agreement calculation for the first five wines, showing the maximum possible agreements, the actual agreements, and the agreement proportion out of the maximum.\n\n\n\n\n\nWine\nRaters\nPossible\nActual\nAgreement\n\n\n\n\n1\n4\n6\n6\n1\n\n\n2\n4\n6\n6\n1\n\n\n3\n4\n6\n3\n.5\n\n\n4\n4\n6\n3\n.5\n\n\n5\n4\n6\n3\n.5\n\n\n\n\n\n\nFor the fifth vintage in Table 7, there are more matches with the zeros than there are with the ones; both are counted. It is at this point what seemed like a good starting point begins to turn into a muddle, because since both the zeros and the ones can create agreement, how do we know what’s good enough agreement? What’s the worst agreement possible?\nA group of \\(n_1\\) raters who agree on the 1 ratings produces a number of agreements about proportional to \\(n_1^2\\), and similarly the \\(n_0\\) raters of 0 agreements produce about \\(n_0^2\\) agreements. Together that’s around \\(n_1^2 + n_0^2\\) agreements, which is less than or equal to \\((n_1 + n_0)^2\\), which is about the number of agreements if everyone agreed on a single category. So we get more agreement when everyone agrees than we do when the raters split into groups. That means that the least agreement happens when the raters are evenly split, giving us a floor for agreement.\n\n\n\nTable 8: Minimum rater agreement rates\n\n\n\n\n\n\n\n\n\n\n\nNumber of raters\nMaximum agreements\nMinimum agreements\nMinimum rate\n\n\n\n\n4\n6\n2\n.33\n\n\n6\n15\n6\n.40\n\n\n8\n28\n12\n.43\n\n\n10\n45\n20\n.44\n\n\neven N\n\\(N(N-1)/2\\)\n\\(N(N/2-1)/2\\)\n\\((N/2-1)/(N - )\\)\n\n\n\n\n\n\nFor six or more raters, there’s at least a 40% agreement rate, even when there’s the least possible amount of agreement. It’s annoying that the minimum rate of agreement changes with the number of raters. For real cases when we want to use these ideas, we might have different numbers of raters for different subjects, complicating an average minimum rate, which defines the worst case agreement."
  },
  {
    "objectID": "introduction.html#chance-correction",
    "href": "introduction.html#chance-correction",
    "title": "Introduction",
    "section": "",
    "text": "We saw in the previous section that the minimum frequency of agreement among raters is quite large, tending toward one half of the maximum possible. Whatever statistic we create as a measure of agreement needs to take that into account if we are to make sense of the results. We’ll get maximum agreement if all the raters choose the same category (of the two choices), and we’ll get minimum agreement if they split evenly between the categories, as if they each flipped a coin to decide. So there’s an association between worst-case random raters and the minimum agreement rate. This is the basis of the chance correction in the S statistic, which assumes coin-flipping raters as the worst-case benchmark from which to measure agreement.\nThe general formula for taking into account a worst-case (random) match rate \\(m_r\\) first computes the practical range of our scale, which is \\(1 - m_r\\), since the statistic can’t be less than \\(m_r\\). The amount of agreement observed \\(m_o\\) is then pinned to this scale as a “chance-corrected” match rate \\(\\kappa\\) (kappa) with\n\\[\\kappa = \\frac{m_o - m_r}{1 - m_r}\\]\nThe S statistic is a special case of this formula, where \\(m_r = .5\\). The sample of wine ratings can be used to illustrate. Recall that earlier we simplified the judges’ scale of “no medal” to “gold medal” to a binary classification between “no medal” (coded as zero) or “any medal” (coded as one). There were 30 maximum agreements among the four raters over the five wines (six per wine), and 21 actual agreements. The minimum is when the ratings are evenly split on each case, generating two agreements for each wine, or 10 total. An S-like statistic would be calculated like this:\n\\[\n\\begin{aligned}\nm_o &= 21/30 = .70 \\\\\nm_r &= 10/30 = .33 \\\\\n\\kappa_s &= \\frac{.70 - .33}{1 - .33} = .47\n\\end{aligned}\n\\]The kappa is now on a scale of zero (worse) to one (perfect agreement). Interpreting what the value means is difficult because it’s on the wrong scale as noted above. A value close to one half sounds mediocre, but not terrible, but we’re really looking for something like the square root of the agreement rate, which is .69. That value corresponds better to the intuition that the agreement in the table looks pretty good.\nThe coin-flipping assumption for randomness can be replaced with other assumptions. This has the effect of increasing the assumed worst-case agreement rate and hence decreasing kappa values. Perhaps the most popular choice is to reason that the raters are more likely to randomly pick the most common category so the coin should be weighted according to the rating distribution.\nFor example, the table of simplified wine ratings has 15 ratings of 1 and 5 of 0, or 75% 1-ratings. The worst case match rate is then the product of the proportions of 1 ratings, or \\(m_r = (.75)(.75) + (.25)(.25) = .625\\). This is the assumption of the Fleiss kappa. Note that the actual worst case match rate is still close to one half (because of the math), so it’s possible to get a negative Fleiss kappa if the assumption about the distribution of randomness fails.\nOne calculation under this proportionality assumption is:\n\\[\n\\begin{aligned}\nm_o &= 21/30 = .70 \\\\\nm_r &= .625 \\\\\n\\kappa_f &= \\frac{.70 - .625}{1 - .625} = .20\n\\end{aligned}\n\\]The actual calculations of these statistics are complicated by the choice of using exact combinatorical counts or long-run estimates. This distinction need not concern us in here.\nThe deflation of the rater agreement statistic in the Fleiss calculation above makes it clear that assumptions about random agreement are critical. A researcher may justifiably wonder which value is correct: are the wine judges in pretty good agreement, or is the judging mostly random? For historical reasons, rather than treating rater disagreement as a parameter to be discovered, the field has evolved to propose many variations of the kappa formula with different assumptions about the worst-case agreement rate.\nA limitation of most of the kappa statistics is that they make the simplifying assumption that a pair of raters reaches agreement randomly if both raters are random. This omits the possibility that one rater is accurate and the other isn’t."
  },
  {
    "objectID": "introduction.html#the-kappa-zoo",
    "href": "introduction.html#the-kappa-zoo",
    "title": "Introduction",
    "section": "",
    "text": "There are by now several statistics advertised as rater agreement statistics, including Cohen’s kappa, Fleiss’s kappa, Scott’s pi, Krippendorff’s alpha, S, and AC1. For convenience, these statistics will be referred to generically as “kappas.” Researchers who want a simple answer are faced with a bewildering set of options and claims about them. In Vach & Gerke (2023) we get a sense of the situation.\n\nGwet’s AC1 has been proposed as an alternative to Cohen’s kappa in evaluating the agreement between two binary ratings. This approach is becoming increasingly popular, and researchers have been criticized for still using Cohen’s kappa. However, a rigorous discussion of properties of Gwet’s AC1 is still missing.\n\nThe authors illustrate both the need for clarification and the faddishness that research communities can adopt when lacking real criteria. The kappas are ill-suited to answer such questions, and the proliferation of agreement statistics makes it difficult to compare results across studies or to know which one to use. Given pressures to publish results, there may be a tendency to use the statistic that gives the highest value, or to use the one that is most familiar. See Button et al. (2020) for a discussion of rater agreement statistics in the context of the “crisis in confidence in psychological research.”\nIt’s not just difficult to know what kappa to use, there are no meaningful guides to interpreting the results. “There is a wide distinction in the elucidation of Kappa values, and several efforts have been made to assign practical meaning to calculated Kappa values,” wrote the authors of Chaturvedi & Shweta (2015), who mention a widely-used heuristic found in Landis & Koch (1977) that proposed a translation between numerical values of kappa and qualitative descriptions of agreement, such as \\(\\kappa \\ge .81\\) is “almost perfect.” The categories are arbitrary, do not translate well between different agreement statistics (which can give different values for the same data) and their assumptions, and do not provide insight into how to improve ratings.\nThe agreement statistics for ratings have problems that should now be evident. The goal is to understand a three-dimensional relationship between ratings and true values, but the statistics are single parameters. A second parameter, the worst case baseline, is buried in an assumption, which varies by kappa, and is not tested for fit to the data. The result is a bewildering array of choices for rater agreement measures. As a result of this confusion, different cultures have emerged. If getting published is the goal, then higher agreement rates are more desirable, so a researcher can shop around for the “best” one.\nA research agenda was suggested in Landis & Koch (1977) that can be paraphrased as understanding (1) the true category of each subject, (2) the accuracy of raters, (3) truth and accuracy within sub-populations of subjects, (4) conditions that cause disagreement, and (5) what distinguishes “for cause” agreement and random agreement. Some of this can be accomplished by taking a fresh look at the kappas."
  },
  {
    "objectID": "introduction.html#machine-learning",
    "href": "introduction.html#machine-learning",
    "title": "Introduction",
    "section": "",
    "text": "The geyser of data produced in the information age has led to new methods of analyzing it that fall generally in the description of “machine learning (ML).” Assigning categories to subjects is a common task in machine learning, and the field has developed a number of methods to do so, including neural networks, support vector machines, and random forests. The goal is to assign categories to subjects in a way that generalizes to new subjects. The methods are often evaluated by comparing the predicted categories to the true categories, and the results are summarized in a confusion matrix. See Carpenter (2008) for a good example of this literature.\nIt may come as a surprise to both behavioral scientists and machine learning researchers that they seem to have independently worked on the rater agreement problem, since the two fields’ literatures don’t seem to overlap. Articles on rater agreement don’t cite machine learning papers, and vice versa. However, the two fields are working on the same problem: how to measure the accuracy of a categorization when we don’t have the true categories to reference.\nThere are differences in the methods and philosophies of the two cultures. The kappa approach is implicitly backward-looking, asking “how accurate were these ratings,” with the assumption that the measured accuracy (a kappa) will carry forward to future instances. The ML approach is more forward-looking, with a suite of tools like cross-validation and a vocabulary (bias-variance trade-off) to measure generalizability. As we have seen, the kappa approach is to reason out a plausible chance-correction calculation and use it to produce the single-parameter kappa. The ML approach is to estimate three parameters with regression models. Finally, the kappas are grounded in the psychology of human classification and come with philosophical links to epistemology, which I described earlier in Section 1.2. The ML algorithms are just statistics and code, barren of philosophical considerations.\nThe advantage of the ML approaches over the kappas is that the three parameter models avoid the confusions of the kappa zoo. But because they lack philosophical grounding, they run into an embarrassment of riches: three parameters is sometimes too many parameters for a model, so there can be multiple solutions. There are work-arounds, but it’s the same species of ad hoc reasoning that causes the problems of the kappa zoo. Fortunately, there’s a way to combine the best of both cultures to avoid most of these problems."
  },
  {
    "objectID": "introduction.html#sec-intro-tap",
    "href": "introduction.html#sec-intro-tap",
    "title": "Introduction",
    "section": "",
    "text": "We can now describe a regression model that allows us to estimate the four proportions that appear in the confusion matrix (see Section 1.3). Since the four cells sum to one, there are three free parameters to estimate.\n\n: Confusion matrix with entries to be filled in by the model estimates. The four numbers are proportions and sum to one, leaving three free parameters to estimate. C0 = class zero, and C1 = class one, standing in for any binary categories we might choose.\n\n\n\nTrue C1\nTrue C0\n\n\n\n\nClassified C1\n\n\n\n\nClassified C0\n\n\n\n\n\nTwo philosophical assumptions are needed to get started:\n\nThe classifications are real in the sense that true values exist.\nThe true values have a binary causal effect on the classification process.\n\nThese assumptions are provisional like hypothesis in statistics; the evidence will provide some support for the assumptions, ranging from no support at all to quite good. The reality of the truth values posed in assumption one is not like Plato’s cave shadows. In the case of wine judging, we can’t say there is a universal ideal for good wine, but if rater agreement is high we can say that some physical causal process exists for translating the observable subject (tasting the wine) into a category. So in that sense the category exists as part of the world. The translation is imperfect, because the conditions are not always perfect for the cause to happen, as with the Gettier problems7.\nThis may seem fiddly, but it gives us a place to start. For any given subject to be rated (e.g. object to be classified) we provisionally assume that there’s a latent truth value that we can never know, but might find evidence for. The second assumption then allows us to provisionally assume that the causal effect of the true value in the context of the classification process has the following nature:\n\nThe rater either assigns the correct class due to the causal pathway operating to connect the observation to the class (justified true knowledge), or\nSomething goes wrong with the causal pathway (the conditions weren’t quite right, etc.) and the classification is assigned non-causally, which is to say randomly.\n\nThe key to this is that the cause either works to connect the true value to the rater’s assigned value, or it fails completely. There’s no “partial cause.” When it fails, the rating is generated from a random process called a Bernoulli trial. It’s the simplest possible type of randomness, taking only two values with some fixed probability, like flipping a weighted coin.\n\n\nCode\n%%{init: {'themeCSS': '.node text { font-family: cursive; }'}}%%\n%%| label: fig-tap-concept\n%%| fig-cap: Conceptual map of t-a-p model\nflowchart TB\n  A(Rating) --&gt; |\"proportion a\"|B(Accurate)\n  A --&gt; |\"proportion 1-a\"|C(Inaccurate)\n  B --&gt; |\"proportion t\"|D[True Class 1]\n  B --&gt; |\"proportion 1-t\"|F[True Class 0]\n  C --&gt; |\"proportion 1-p\"|H[Random Class 0]\n  C --&gt; |\"proportion p\"|G[Random Class 1]\n\n\n\n\n\n%%{init: {'themeCSS': '.node text { font-family: cursive; }'}}%%\n%%| label: fig-tap-concept\n%%| fig-cap: Conceptual map of t-a-p model\nflowchart TB\n  A(Rating) --&gt; |\"proportion a\"|B(Accurate)\n  A --&gt; |\"proportion 1-a\"|C(Inaccurate)\n  B --&gt; |\"proportion t\"|D[True Class 1]\n  B --&gt; |\"proportion 1-t\"|F[True Class 0]\n  C --&gt; |\"proportion 1-p\"|H[Random Class 0]\n  C --&gt; |\"proportion p\"|G[Random Class 1]\n\n\n\n\n\n\nLet’s take inventory of the three parameters that come from the reasoning displayed in the diagram.\n\nAmong the subjects being rated there is a fraction \\(t\\) that are in reality Class 1, with the remaining \\(1-t\\) being Class 0.\nAmong the ratings there is a fraction \\(a\\) that are accurate ratings (justified true knowledge) where the causal connection worked.\nFor the remaining \\(1-a\\) ratings, the causal connection failed (as with a Gettier problem), and there is some probability \\(p\\) that describes the frequency that ratings are randomly assigned to Class 1. The remaining \\(1-p\\) are assigned Class 0.\n\nAll three of these parameters are proportions ranging from zero to one, and can be treated as probabilities that we estimate from appropriate regression models. Setting it up this way, instead of the usual machine learning parameterization avoids the most significant problem with non-identifiability (multiple solutions), which is class-switching. That happens when the model can fit the data, but is agnostic about which class is which, and so attempts to fit the model both ways at once.\n\n\n\n\n\n\nFigure 1: tap diagram\n\n\n\nThe diagram in Figure 1 shows probabilistic links between states of the world and rater classifications for a single rating. Upper case letters will be used here for binary states, with\n\n\\(C_{ij}\\) being the rating assigned to the \\(i\\)th subject by the \\(j\\)th rater. The value of \\(C_{ij}\\) is determined at the bottom of the diagram, contingient on the classification process.\n\\(A_{ij}\\) is 1 if that rating was accurate (JTB), or 0 otherwise.\n\\(T_i\\) is the true class (zero or one) of the \\(i\\)th subject.\n\\(P_{ij}\\) matters only if the \\(i,j\\) rating was inaccurate, (\\(A_{ij} = 0\\)). In that case, a random assignment of zero or one is made.\n\nThese binary events are assumed to be independent of one another, except that the \\(T_i\\) true classification is fixed over all ratings. We’ll also assume that each of these binary outcomes has a fixed average value. In the notation, these are lower-case letters corresponding to the ones in Figure 1.\n\n\\(c\\) is the proportion of Class 1 ratings assigned.\n\\(t\\) is the proportion of true Class 1 cases.\n\\(a\\) is the fraction of classifications that are accurate.\n\\(p\\) is the proportion of randomly-assigned classifications that are Class 1.\n\nThese averages replace the individual binary states to comprise the average t-a-p model.\n\n\n\n\n\n\nFigure 2: tap diagram\n\n\n\nIn Figure 2, the variables \\(c, a, t, p\\) are all probabilities, with a bar over the symbol to denote its complement, i.e. \\(\\bar{t} := 1-t\\). At the end of each branch is the classification (zero or one) and its probability.\nTo illustrate the ideas here, consider a judge (the rater) tasting one of the wines in a competition (the subject). Because the classification is binary, assume that the rating is either “1 = acceptable” or “0 = not acceptable.” The model assumes that each wine being tasted has a true value of acceptability. Over all the wines, the proportion of acceptable wines is \\(t\\). Suppose this one is, in fact, acceptable, so that \\(T_i = 1\\) in Figure 1. There’s some probability \\(a\\), which we’ll call accuracy, that the judge’s perceptive powers will reveal this true quality of the wine. If this happens, then \\(A_{ij] = 1\\), recording an accurate rating, and the resulting classification is necessarily Class 1. That event would be tracing down the left side of the tree diagram, and over all judges and wines, the probability of that is \\(at\\). If the judge’s perspicacity desserts him, and he makes an inaccurate rating, this doesn’t mean he automatically gets the wrong answer! Recall the Gettier-like problems, where we can accidentally get the correct answer even though our reasoning is flawed. Instead, there’s a random chance \\(p\\) of assigning a Class 1 rating. The overall probability that a random (inaccurate) Class 1 rating is assigned is the branch ending with \\(\\bar{a}p = (1-a)p\\) at bottom right in Figure 2.\nIt’s a critical point that the true classifcation of each subject (\\(T_i\\)) is the same regardless of who’s rating it. So when four judges all rate the same wine, they are all either on the left side of the diagram (if the wine is acceptable in reality) or on the right side (if not). The ratings are determined only by \\(a\\) and \\(p\\) at that point. It’s this commonality of truth that allows us to study within-subject variation versus between-subject variation.\nFor a wine chosen at random, we can compute the probabilities of rater classifications. An acceptable wine will be classified accurately with a proportion of \\(ta\\), multiplying the probabilities along the leftmost edge from top to bottom. An inaccurate rating of \\(\\hat{C_1}\\) (acceptability) can come from either the left or right side of the diagram, and if we add those together we get \\(t\\bar{a}p + \\bar{t}\\bar{a}p\\), and since \\(t + \\bar{t} = 1\\) that expression reduces to \\(\\bar{a}p\\).\nAssuming we can estimate the three parameters from the data, we can then populate the confusion matrix by tracing the diagram down to each of the four outcomes, and multiplying probabilities as we go.\n\n\n\nTable 9: The t-a-p model’s correspondence to the confusion matrix. Terms in parentheses are inaccurate ratings.\n\n\n\n\n\n\n\n\n\n\n\nTrue C1\nTrue C0\n\n\n\n\nClassified C1\n\\(ta + (t\\bar{a}p)\\)\n\\((\\bar{t}\\bar{a}p)\\)\n\n\nClassified C0\n\\((t\\bar{a}\\bar{p})\\)\n\\(\\bar{t}a + (\\bar{t}\\bar{a}\\bar{p})\\)\n\n\n\n\n\n\nThe entries in Table 9 demonstrate that if the t-a-p model fits the data and we are able to estimate the three parameters, it is a general answer to the rater agreement question. The later sections show how S, Fleiss kappa, and other statistics are special cases of t-a-p models."
  },
  {
    "objectID": "introduction.html#example-wine-judging",
    "href": "introduction.html#example-wine-judging",
    "title": "Introduction",
    "section": "",
    "text": "The sections above used five rows of the wine judging data for illustration. We now use the whole data set of 183 wines each rated by four judges to estimate the t-a-p parameters. The included app was used (see Chapter 7) for the analysis.\nThe binary classification illustrated above was to convert the original scale (1 = no medal, 2 = bronze, 3 = silver, 4 = gold medal) to a binary outcome where 1 = any medal (bronze, silver, gold), and 0 = no medal. We called the Class 1 category “acceptable.” Using the app to analyze that binary comparison we get the following output.\n\n\n\n\n\n\nFigure 3: Wine rating t-a-p analysis comparing C1 = ratings 1,2,3 (in-class) to C0 = rating 0.\n\n\n\nThe acceptable wines are the primary (in-) class, and the plot shows the distribution of ratings for acceptability. Since there are four judges, the possibilities are:\n\nNone of them could find the wine acceptable. This is the 0 at the left end of the plot. It happens for about 6% of the wines.\nAll four of them could find the wine acceptable. This is the 4 (four judges) at the right end of the plot. That happened for about 49% of the wines.\nSomething in between, where the judges disagree. In that case we can have from one to three ratings of “acceptable,” corresponding to those points on the plot.\n\nThe lollipops (black lines with dots on top) in Figure 3 show the empirical data straight out of the ratings. The dashed blue line is the model fit, showing what the distribution would be if the model were exactly correct. Here, “model” means applying the estimated parameters to generate the distribution.\nThe estimated parameters are found at the top of the plot:\n\n\\(t\\) = .73 estimates that 73% of wines are acceptable in reality. This is more than the rate of unanimous agreement, which we saw above was only 49%.\n\\(a\\) = .54 estimates that the judges make accurate ratings (justified true belief) more than half the time. The rest of the ratings are assumed to be random.\n\\(p\\) = .78 estimates that when random ratings are made, judges choose “acceptable” 78% of the time. Notice that this is close to the estimated actual rate of 73% (the estimated value of \\(t\\) above). This turns out to be a desirable quality in judges. See Chapter 3 for details, especially the section on the Fleiss kappa.\n\nNote that the randomness of inaccurate ratings is not a conscious choice of raters. They aren’t giving up and flipping a coin. They still have a reasonable basis for making a rating, and may be quite confident about it. The assumption is that something Gettier-like has gone wrong with the causal process that links the input (wine) to output (rating), which flips the process from deterministic to stochastic. As with any regression model, the world doesn’t have to actually be that way; it’s just an assumption to allow us to create the model. Then we check to see if the data matches the model. In this case, the data matches the model pretty well, as we can visually assess by comparing the blue dashed line to the lollipops. More formal tests of agreement will have to wait until later.\nThe wine ratings comprise four categories that are naturally ordered from 1 = “no medal” to 4 = “gold medal.” That kind of data is common: from surveys with Strongly Disagree to Strongly Agree scales, from consumer ratings, and many other sources. We humans seem to have a natural facility to think of descriptions along an imaginary intensity scale. This data type is called “ordinal,” meaning we can put the categories in order.\nThe t-a-p model is designed to work with binary data. It’s possible to switch to a multinomial model, but it’s common to analyze ordinal scales using binary distinctions. I’ll show how that works with the wine data. Along the 1,2,3,4 scale, there are three sequential cut-points where the commas are. The first one we can denote “1|2”, which splits the scores into two sets: an in-class {1} and and out-class {2,3,4}. This is the same thing as the Class 1 and Class 0 distinction from earlier. The second cut-point is at 2|3, splitting the data into ratings {1,2} and {3,4}. The plot shows the fraction of ratings in the in- and out-class for each cut-point.\n\n\n\n\n\n\nFigure 4: Wine ratings divided into binary groups by cutpoint.\n\n\n\nThe 3|4 bar of the plot shows us that 88% of ratings are in {1,2,3}, so the remaining 12% are 4s (gold medal). As a validity check, there are 183 wines, each rated by four judges, so that should be 732 ratings. That looks like the height on the bars, so it checks out.\nFor a given cut-point, a wine rating will either be in the in-class or out-class This converts the scale to binary, and then we can estimate the model parameters. We repeat the t-a-p model for each cut-point to create a pattern like the one in the plot below.\n\n\n\n\n\n\nFigure 5: t-a-p parameter estimates for ordinal scale based on cutpoints\n\n\n\nThe plot in Figure 5 shows each t-a-p parameter for each cut-point. As a reference, the Fleiss kappa estimates are also included as dotted lines. The significance of the Fleiss kappa is discussed later.\nThe accuracy parameter \\(a\\) at the top shows good accuracy for the lowest cut-point, meaning that the judges were good at distinguishing the least worthy wines from the rest. This is the case we analyzed earlier when we called the 1 rating unacceptable. As the quality rating increases, moving to the right on the top plot, accuracy decreases to less than half its value for the first cut-point (focus on the solid line). This would be the case if poor wines have more basis in physiology (sourness, etc.) and as the assessments become more aesthetic, they become more individualized and have less group agreement.\nThe second plot, showing estimates for \\(p\\) show the probability with which judges place a wine randomly into the in-class for inaccurate ratings. This will happen less often for the left-most cut-points, since accuracy is higher there. The dotted line is useful here: it shows what the \\(p\\) parameter would look like if the raters assigned ratings proportionally when making inaccurate classifications. For example, we noted earlier that 88% of the ratings are 1-3 (rightmost bar of the previous plot), so for the 3|4 cut-point, proportional random ratings would assign 88% of the inaccurate ratings into the {1,2,3} in-class. That’s where the dotted line is, at 88%. The actual parameter estimate (solid line) is at about 84%, meaning that judges are probably too conservative about assigning the gold medal category. Accuracy is low for the gold medals, and when the judges rate inaccurately, the ratings are slightly biased toward the lower ratings.\nThe bottom plot in Figure 5 estimates the true proportions for each cut-point, after taking into account the other two parameters. For the 3|4 cut-point on the right, it shows a proportion of {1,2,3} wines of about 65%. This is much lower than the 88% of ratings that are {1,2,3}. That’s the combined effect of inaccurate ratings at the top end of the scale combined with the bias toward lower ratings for inaccurate ratings. Looking at the two ends of the scale for the \\(t\\) plot, we can estimate that about 26% of wines are truly in the 1 = no medal category, and about 35% are in the 4 = gold medal category (1 - .65 = .35). That 35% figure comes from reading the estimated value of \\(t\\) at the 3|4 cut point, which is about 65%, and subtracting from one.\nIt’s possible to tell good wine from bad wine pretty reliably in this data set, but beyond that individual tastes may not be discerning enough to sort out four levels of wine quality. The scale could possibly be reduced to three ratings, or else keep the existing scale but collapse the 3 and 4 ratings into a single category before reporting the results. The effect of the existing rating system is leaving a lot of probably excellent wines with poorer ratings than they deserve. An alternative approach is to try to improve the accuracy of the higher ratings, which can be facilitated by reducing the rater bias against the highest award. One study showed that it’s possible to improve reliability in college grade assignment through feedback Millet (2010). This method might work more generally for reducing rater bias."
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn earlier paper on this was published under the auspices of the NSA, and has a stamp declaring it unclassified. See https://www.nsa.gov/portals/75/documents/news-features/declassified-documents/tech-journals/application-of-cluster-analysis.pdf↩︎\nIt’s a mystery why humans can agree on mathematical justifications at such a high rate. If agreement indicates reality, then math must be real in some way.↩︎\nI found this on wikipedia here https://en.wikipedia.org/wiki/Gettier_problem↩︎\nwhich seems like an infinite regress of JTB inquiry.↩︎\nConfusion matrices are at the heart of measures of causality too, for reasons that are not coincidental. See Eubanks (2014).↩︎\nThe process of a group of people reaching agreement does not seem to be transitive! It may well be three times as difficult for three people to agree on a movie to watch as two people. The rater agreement models get around this by assuming that raters don’t talk to (argue with) each other, but reach conclusions independently before comparing notes.↩︎\nFor much more on this idea see “Causal Interfaces.”↩︎"
  },
  {
    "objectID": "tapmodel.html",
    "href": "tapmodel.html",
    "title": "Chapter 2: The t-a-p Model",
    "section": "",
    "text": "This chapter examines the assumptions and implications of the t-a-p model in detail to build a theoretical foundation for estimation and inference. This work also paves the way to connect the three parameter model to widely-used rater agreement statistics (kappas) and machine learning algorithms.\n\n1 Introduction\nSuppose that we have \\(N\\) subjects to be classified by at least two raters each, and each subject belongs (in truth) to one of two categories Class 1 or Class 0. For example, if faculty members review portfolios of student work, the two classes could be “pass” and “fail.” We often use ordered classifications like “A, B, C, D, F” or “Excellent, Good, Fair, Poor” but these can be converted to binary classifications by grouping the classes. For example “A, B, C” could be compared to “D, F” or “Excellent, Good” could be compared to “Fair, Poor.” There are also extensions of the t-a-p model that work directly on ordinal or non-binary categorical scales. The exposition is easiest to understand for the binary case, so we will start there.\nWe assume that each subject is independently assigned to one of the two classes by each of \\(R\\) observers (raters). For now, think of \\(R\\) as fixed, so that there are \\(RN\\) total ratings, but that condition is relaxed later on, so that the number of ratings per subject can vary, which is common in real data.\nRater-assigned categories are distinguished from true classes in notation by the use of hats to suggest an estimated value. Ratings of Class 1 are denoted Class 1, but some of them may be Class 0 in truth. This distinction between rated (estimated) values and true values leads to the idea of true/false positives/negatives and the confusion matrix described in the introductory chapter, reproduced here.\n\n\n\nTable 1: The t-a-p model’s correspondence to the confusion matrix. Terms in parentheses are inaccurate ratings.\n\n\n\n\n\n\n\n\n\n\n\nTrue C1\nTrue C0\n\n\n\n\nClassified C1\n\\(ta + (t\\bar{a}p)\\)\n\\((\\bar{t}\\bar{a}p)\\)\n\n\nClassified C0\n\\((t\\bar{a}\\bar{p})\\)\n\\(\\bar{t}a + (\\bar{t}\\bar{a}\\bar{p})\\)\n\n\n\n\n\n\nThe true positive rate of classification in Table 1 can be separated into ratings that are true for a good reason and those that are accidentally true. This chapter expands on that idea to reveal other properties of the t-a-p model and how some existing rater agreement statistics are special cases of it.\nWe will assume that an assigned rating corresponds to the belief of a rater. This rules out raters who are intentionally being deceptive, for example. Then we will say that a rater makes an accurate assignment of Class 1 or Class 0 for a subject if\n\nthe rater-assigned class is the true class, and\nthe rater has justification for the choice.\n\nThese requirements operationalize the Justified True Belief (JTB) definition of knowledge used by philosophers who study epistemology. Inaccurate ratings are those where one of the two listed conditions fails. If the rater’s belief is false and chooses the wrong category or if they choose the correct category but because of an invalid justification. The latter case corresponds loosely to Gettier-type problems, where the chain of reasoning reaches a correct conclusion, but because of flaws in perception the logic isn’t sound. A clear case of failing the justification requirement is if raters flip coins to choose categories. Coin flipping is entirely random, but even good raters have some randomness inherent to the classifications. That randomness is the usual starting point for chance-corrected agreement statistics.\nThe rating process just described lends itself to a tree diagram that illustrates three variables as conditional probabilities: (1) the true Class 1 rate \\(t\\), (2) rater accuracy \\(a\\), and (3) the probability \\(p\\) of choosing Class 1 when rating inaccurately. On the diagram, it’s convenient to use a bar over a variable to denote its complementary probability, e.g. \\(\\bar{a} = 1 - a\\).\n\n\n\n\n\n\nFigure 1: Assumed rating process showing average rates, where \\(c\\) is the rate of assigning Class 1 to subects, \\(t\\) the average true Class 1 rate, and \\(a\\) is average rater accuracy.\n\n\n\nEach rating is conditional on a subject’s true classification (Class 1 or Class 0), which will often be unknown, so that we can only observe the rater-assigned categories \\(C_{ij}\\), where \\(i = 1 \\dots N\\) are the subjects and \\(j = 1 \\dots R\\) are the raters, who independently classify subjects as Class 1 or Class 0. Define \\(k_i := \\sum_j C_{ij}\\) as the total number of ratings for subject \\(i\\), and the average of the classifications made by raters is \\(c = \\sum{C_{ij}}/(NR)\\).\nExample: A wine judge independently and honestly assesses a vintage for excellence. The two categories are Class 1 = “excellent” and Class 0 = “not excellent.” After judging four wines, the situation might be that in the table below.\n\nSample wine ratings showing normally unknown truth values.\n\n\nWine\nTruth\nAccurate?\nClassification\n\n\n\n\n1\nExcellent\nYes\nExcellent\n\n\n2\nExcellent\nNo\nNot Excellent\n\n\n3\nNot Excellent\nNo\nNot Excellent\n\n\n4\nNot Excellent\nYes\nNot Excellent\n\n\n5\nNot Excellent\nYes\nNot Excellent\n\n\n\nIf the judge makes an accurate assessment, the classification recorded matches the true value. But for the third wine, the judge got the correct answer even though the process was flawed and somewhat random (an inaccurate rating). For the second wine, the inaccuracy resulted in the wrong classification being assigned. Those two inaccurate cases are illustrated in the process diagram in the middle, marked Random.\n\n\n2 Conditional Probabilities\nThe tree diagram in figure Figure 1 models the average assignments of ratings \\(C_{ij}\\) over subject \\(i\\) and rater \\(j\\), and can be read by multiplying the conditional probabilities on the edges from the top down to find the probability of a given classification being Class 1$.\nIf a subject is not classified accurately, the classification for that rater is assumed to be made at random, with probability \\(p\\) of choosing Class 1 regardless of the true class. So the conditional probability of a Class 1 classification when the subject really is Class 1 is \\(\\text{Pr}(\\text{Assigned Class 1} | \\text{True Class 1}) = a + \\bar{a}p\\). Similarly, \\(\\text{Pr}(\\text{Assigned Class 0} | \\text{True Class 0}) = a + \\bar{a}p\\). This model assumes that guess rates for the two classes are the same independent of the true classification. More complex models are introduced later.\nThe binary (Bernoulli) probability that for a given subject with a given true classification, a rater will assign a Class 1 rating is shown in the table below. This comes from reading the tree diagram from the top down, multiplying the branches.\n\nConditional probabilities of a single rater assigning a Class 1 rating to a subject.\n\n\n\nTrue C1\nTrue C0\n\n\n\n\nClassified C1\n\\(a + \\bar{a}p\\)\n\\(\\bar{a}p\\)\n\n\n\nWe can use these probabilities to find the expected number of ratings of Class 1.\n\n\n3 Binomial Mixtures\nThe rating process posed in the t-a-p model is illustrated with the tree diagram and table of example ratings above. But both of those entail the use of the hidden true classification value for each subject. There are cases where that can be known, but in general it is not. What we usually have to work with is a table of ratings, from which we must infer the hidden variables like rater accuracy. The collection the ratings under the t-a-p assumptions fall into a well-studied probability distribution called a binomial mixture.\n\n\nShow the code\n# set the parameters\nN_r = 5    # number of raters for each subject \nN_s = 100  # number of subjects (not used here)\n\nt = .3 # fraction of subjects that are in fact class 1\na = .7 # probability of a rater rating a subject accurately\np = .2 # probability of a rater rating a subject as class 1 when rating inaccurately\n\n# find the conditional probabilities for each class\n\n  # probabilities of a class 1 rating for a class 0 subject\n  c0_probs = dbinom(0:N_r, N_r, prob = (1-a)*p)\n  \n  # probabilities of a class 1 rating for a class 1 subject\n  c1_probs = dbinom(0:N_r, N_r, prob = a + (1-a)*p)\n\n# create the mixture with t as the mixing parameter\nmixture = c0_probs * (1-t) + c1_probs * t\n\n# Plot the conditional probabilities\nplot(0:N_r, c0_probs, type=\"b\", col=\"blue\", pch=16, ylim=c(0, max(c(c0_probs, c1_probs, mixture))),\n     ylab=\"Probability\", xlab=\"Number of class 1 ratings for a given subject\", main=\"Binomial Mixture Plot\")\n\n# Add the second component (c1_probs)\nlines(0:N_r, c1_probs, type=\"b\", col=\"red\", pch=16)\n\n# Add the mixture as a black dashed line\nlines(0:N_r, mixture, type=\"b\", col=\"black\", lty=2, pch=16)\n\n# Add a legend\nlegend(\"topright\", legend=c(\"Class 0 Probabilities\", \"Class 1 Probabilities\", \"Mixture\"),\n       col=c(\"blue\", \"red\", \"black\"), lty=c(1, 1, 2), pch=16)\n\n\n\n\n\n\n\n\nFigure 2: Binomial mixture showing the probability of the number of class 1 ratings for a given subject. In blue is the distribution for true-class 0 subjects, in red is the distribution for true-class 1 subjects, and the black line is the mixture based on relative proportions.\n\n\n\n\n\nFigure 2 shows an example of how probabilities combine to create the mixture. Given the three t-a-p parameters plus the number of raters per subject, we apply the binomial probability density function using the probabilities found at the end of the previous section. The mixture is created by weighting these two distributions by their frequency in the sample space. In this case, the parameters are \\(t = .3\\), \\(a = .7\\), and \\(p = .2\\).\nFor true Class 1 cases, the probability of a rater assigning a Class 1 rating is \\(ta + t\\bar{a}p\\), and for true Class 0 cases it is \\(\\bar{t}\\bar{a}p\\) (see the table at the end of the last section). Those are probabilities for a single rating. If we have \\(R\\) raters, then anywhere between zero and \\(R\\) of them could assign a Class 1 rating to a given subject. The binomial distribution gives the probability for each of those possible count outcomes. For true Class 1 subjects the probability of \\(k\\) raters assigning a Class 1 rating is\n\\[\n\\begin{aligned}\nPr(k | \\text{True Class 1}) &= \\binom{R}{k} (a + \\bar{a}p)^k (1 - a - \\bar{a}p)^{R - k} \\\\\n            &= \\binom{R}{k} (a + \\bar{a}p)^k (\\bar{a} - \\bar{a}p)^{R - k} \\\\\n            &= \\binom{R}{k} (a + \\bar{a}p)^k (\\bar{a} \\bar{p})^{R - k}\n\\end{aligned}\n\\]\nThat distribution is represented by the red line in Figure 2. Notice that the most outcome is that four of the five raters assign a Class 1 rating. The reason that’s not five is that the parameter \\(p = .2\\) means that for inaccurate ratings, the “guess” is much more likely to assign Class 0 than Class 1. The effect is to deflate the number of Class 1 ratings for true Class 1 subjects.\nFor true Class 0 cases (the blue line in the plot), it is\n\\[ Pr(k | \\text{True Class 0}) = \\binom{R}{k} (\\bar{a}p)^k (1 - \\bar{a}p)^{R - k} \\]\nThese are the two probability distributions are shown in Figure 2, with the given parameters applied. The code is included so that you can try variations on your own.\nBut there are not necessarily the same number of true Class 1 and Class 0 cases. The fraction of Class 1 cases is assumed to be \\(t\\). The mixture of the two is\n\\[ Pr(k) = t \\binom{R}{k} (a + \\bar{a}p)^k (\\bar{a}\\bar{p})^{R - k} + \\bar{t} \\binom{R}{k} (\\bar{a}p)^k (1 - \\bar{a}p)^{R - k}.  \\tag{1}\\]\nThis is the mixture distribution that is assumed to represent the count data of Class 1 ratings per subject. To proceed with an investigation of the t-a-p model, we first count up the number of Class 1 ratings for each subject. If there are the same number of raters for each subject, these counts will form a histogram that corresponds to the black dashed line in the figure, for some set of t-a-p parameters. The job then is to find out what those parameters are.\nFor a real data set, the red and blue plots in Figure 2 are assumed to exist, but are not directly accessible to us. Instead we can see something like the mixture (black dashed line). But even that isn’t exact, because it is subject to sampling error: the histogram of counts won’t exactly correspond to the ideal probabilities. The larger the number of subjects rated, the closer the empirical proportions will, in theory, converge to the ideal mixture distribution.\nFor general information on binomial mixtures of discrete data see Agresti (2003) chapter 14.\n\n\n4 Simulation\nThe interactive app provided with the tapModel R package allows you to specify t-a-p parameters and generate a data set from them. Once the package is installed, you can run the app with tapModel::launchApp(). Navigate to the Simulate Data tab.\n\n\n\n\n\n\nFigure 3: Rating distributions with t = .3, a = .7, p = .2. Plot A has 20 subjects and Plot B has 1000. The blue bars are the histograms of the simulated data and the orange dashed line is the true mixture probability distribution.\n\n\n\nFigure 3 shows two count histograms with identical parameters except for the number of subjects (20 versus 1000). The histograms are the result of applying the t-a-p binomial mixture distribution with five raters on each subject and parameter set \\((t = .3, a = .7, p = .2)\\). Notice that the smaller sample size on the left (plot A) doesn’t match the distribution line as well as the one on the right. This is the effect of random sampling. The smaller the sample, the more likely it is that the empirical counts don’t look like the distribution.\nThis leaves us with two problems: how do we estimate the parameters, and how much should we trust the results? These are classical problems from statistics.\nThe accuracy rate \\(a\\) will affect the subject distributions. If \\(a = 0\\) the ratings will be distributed as \\(\\text{Bernoulli}(p)\\), independently of the subjects being rated. If \\(a=1\\), then all raters agree on the true value for each subject. Therefore the way we can reconstruct \\(a\\) from data is through the distribution of the within-subject ratings. The method used here can be seen as a latent class model with binomial mixture distributions. For a nice discussion of these ideas in practice see Grilli et al. (2015), which helpfully notes that binomial mixtures are statistically identifiable if the number of cases exceeds a low threshold McLachlan & Peel (2000).\nWe would like to know the true proportion \\(t\\) of the subjects belonging to true Class 1 regardless of how they were rated, rater accuracy \\(a\\), and the proportion \\(p\\) of inaccurate assignments that are Class 1. That goal describes the general model illustrated in the following section.\n\n\n5 Fitting the Model\nThe first question about the model illustrated in figure Figure 1 is whether it is computationally useful. Using known parameter values for \\(t, a, p\\) to generate simulated ratings, can we then recover the parameters from the data? The answer is yes, with some provisos. Given a data set \\(c_{ij}\\), we can fit a general model to estimate the three parameters \\(t\\), \\(a\\), and \\(p\\) using maximum likelihood to fit a binomial mixture model. The log likelihood function for the binomial mixture described by figure Figure 1 with \\(N\\) subjects and \\(R_i\\) raters for subject \\(i\\) is\n\\[\n\\begin{aligned}\n\\ell(t,a,p;R_1, \\dots, R_N, k_1, \\dots,k_N) &= \\sum_{i = 1}^N \\log \\left( t\\binom{R_i}{k_i}(a + \\bar{a}p)^{k_i}(\\bar{a}\\bar{p})^{R_i-k_i} +  \\bar{t}\\binom{R_i}{k_i}(\\bar{a}p)^{k_i}(1-\\bar{a}p)^{R_i-k_i} \\right)  \\\\\n&= \\sum_{i = 1}^N \\log \\left( t\\,\\text{binom}(R_i,k_i,a + \\bar{a}p) +  \\bar{t}\\,\\text{binom}(R_i,k_i,\\bar{a}p) \\right)  \\\\\n&= \\sum_{u = 1}^{\\text{unique}(R_i,k_i)}n_u \\left[ \\log \\left( t\\,\\text{binom}(R_u,k_u,a + \\bar{a}p) +  \\bar{t}\\,\\text{binom}(R_u,k_u,\\bar{a}p) \\right)  \\right]\\\\\n\\end{aligned}\n\\tag{2}\\]\nwhere \\(k_i=\\sum_{j}C_{ij}\\) is the number of Class 1 ratings for subject \\(i\\). The sum over the logs is justified by the assumption that ratings are independent (i.e. multiplying probabilities). The \\(t\\) and \\(\\bar{t}\\) terms at the top of Equation 2 are the mixing proportions for the two classes.\nThe second equation in Equation 2 just rewrites the log-likelihood function more compactly to see the binomial mixture structure. The last equation is a more efficient way to calculate the likelihood, based on the observation that there are a limited number of pairs \\((R_i,k_i)\\) that can occur in the data. For example, if there are three raters for each subject, then the possible pairs are \\((4,0), (4,1), \\dots, (4,4)\\), five unique combinations regardless of the number of subjects \\(N\\). The likelihood is a sum over the unique pairs, multiplying each log-likelihood for that pair by the number of times it occurs in the data, denoted here by \\(n_u\\). In other words, the rating combination \\((R_u, k_u)\\) occurs \\(n_u\\) times. Using that formulation speeds up estimation algorithms because it reduces the number of calculations needed to fit the model.\nIt is straightforward to implement the function in the Bayesian programming language Stan (Carpenter et al., 2017), using uniform \\((0,1)\\) priors for the three parameters (see the discussion section at the end of that paper to access the source code).\nTo test the computational feasibility of this method, ratings were simulated using a range of values of \\(t\\), \\(a\\), and \\(p\\). The 729 trials each simulated 300 subjects with five raters each, using all combinations of values ranging from .1 to .9 in increments of .1 for each of \\(t\\), \\(a\\), and \\(p\\). The Stan engine uses a Markov chain Monte Carlo (MCMC) algorithm to gather representative samples from the joint probability density of the three parameters. Each run used 1,000 iterations (800 after warm-up) with four chains each.\n\n\n\n\n\n\n\n\nFigure 4: Box and whisker plots show parameter estimates from simulations of rater data \\(t\\)-\\(a\\)-\\(p\\) values ranging from .1 to .9. The diagonal line marks perfect estimates.\n\n\n\n\n\nThe accuracy measure \\(a\\) and the Class 1 “guess rate” \\(p\\) are stable across scenarios in figure Figure 4, but the estimated true fraction of Class 1 cases \\(t\\) is sensitive to values of \\(a\\) near zero. To see this, substitute \\(a = 0\\) into the likelihood function to get\n\\[\n\\begin{aligned}\n\\ell(t,p;a = 0, k_1, \\dots,k_N) &= \\sum_{i = 1}^N t\\log \\left( \\binom{R}{k_i}p^{k_i}(\\bar{p})^{R-k_i} \\right)+ \\bar{t}\\log \\left(\\binom{R}{k_i}p^{k_i}(\\bar{p})^{R-k_i} \\right) \\\\\n&= \\sum_{i = 1}^N \\log \\left( \\binom{R}{k_i}p^{k_i}(1-p)^{R-k_i}  \\right).\n\\end{aligned}\n\\]\nSince the two terms at the top are the same, and we sum \\(t + \\bar{t} = 1\\), the \\(t\\) drops out of the formula. So \\(t\\) is under-determined when \\(a = 0\\), and we should expect poor behavior as \\(a\\) nears zero. This is intuitive: if the raters are only guessing, they should give us no information about the true Class 1 rate. If the data in figure Figure 4 are filtered to \\(a &gt; .2\\) the estimates of \\(t\\) greatly improve. Aside from extreme values of \\(a\\) affecting the estimation of \\(t\\), a visual inspection of the scatter plots of the parameter estimates shows no correlations.\nThe estimates in Figure 4 are taken from averages of the posterior parameter distributions, which is convenient, but sometimes hides the uncertainty in the estimates because of the limited range of possible values on [0,1]. When making inferences from a real data set, it’s useful to look at the full posterior distributions of the parameters to see if they are multi-modal or have other complications.\n\n\n6 Exact Formulas for Accuracy\nAs we’ll see in Chapter 3, the Fliess Kappa illustrates a “closed form” for calculating accuracy in the case when \\(t = p\\). We can find exact formulas for other special cases, which is useful for theoretical purposes. Consider the two-rater case on \\(N_s\\) subjects, where the number of cases where both raters assigned Class 0 is \\(n_0\\), the mixed-rating case is \\(n_1\\), and the count of cases with both Class 1 is \\(N_2\\). The log likelihood function is\n\\[\n\\begin{aligned}\n\\ell(t,a,p)\n&= n_0\\log [t(\\bar{a}\\bar{p})^2+ \\bar{t}(1-\\bar{a}p)^2]\\\\\n&+ n_1\\log 2[t\\bar{a}\\bar{p}(a+\\bar{a}p)+ \\bar{t}\\bar{a}p(1-\\bar{a}p)] \\\\\n&+ n_2\\log [t(a+\\bar{a}p)^2+ \\bar{t}(\\bar{a}p)^2].\n\\end{aligned}\n\\] If we further assume that \\(t = p = 1/2\\), after some simplification we arrive at\n\\[\n\\begin{aligned}\n\\ell(t = 1/2,a,p = 1/2)\n&= n_0 (\\log (1+a^2) - 2)\\\\\n&+ n_1\\log(1 - a^2) - 1 \\\\\n&+ n_2 (\\log (1 + a^2) - 2),\n\\end{aligned}\n\\] where we assume that the base for the log is \\(1/2\\) (motivation for that choice is found in Chapter 5). When the log likelihood is maximized, the derivative is assumed to be zero, so we differentiate with respect to \\(a\\) and obtain\n\\[\n\\begin{aligned}\n\\frac{\\ell(a)}{\\partial a}\n&=  2a \\left( \\frac{n_0 + n_2}{1+a^2} -\\frac{n_1}{1-a^2} \\right)\n\\end{aligned}.\n\\] Clearly, \\(a=0\\) is one solution. The other is\n\\[\n\\begin{aligned}\na &= \\sqrt{\\frac{n_0 - n_1+ n_2 }{n_0 + n_1 + n_2}} \\\\\n&=  \\sqrt{\\eta_0 - \\eta_1 + \\eta_2} \\, ,\n\\end{aligned}\n\\tag{3}\\]\nwhere \\(\\eta_i = n_i/N_r\\) is the proportion of cases with \\(i\\) ratings of Class 1. If there are no mismatched ratings, then \\(\\eta_1=0\\), and \\(a = 1\\). If \\(\\eta_1 = 1/2\\), then the ratings look like coin flips, and \\(a = 0\\). The form of Equation 3 represents \\(a^2\\) as a linear combination of match rates and mismatch rates. In fact, for unbiased raters (\\(t=p\\)) we can derive the coefficients \\(\\beta_k\\) so that\n\\[\n\\begin{aligned}\na^2 &= \\sum_{k=0}^R \\beta_k\\ \\eta_k  \\\\\n\\beta_k &= \\frac{k^2}{R(R-1)t\\bar{t}}+ 1 - \\frac{R}{(R-1)\\bar{t}}. \\\\\n\\end{aligned}\n\\tag{4}\\]\nThe derivation of the formula for \\(\\beta_k\\) is found in the Appendix. The linear models for \\(a^2\\) are useful as theoretical tools, e.g. sensitivity analysis, but shouldn’t be used for direct estimation of accuracy, since the \\(t=p\\) coefficient has to be specified, and the computation is sensitive to violations of the unbiased rater assumption. The supporting R package contains a function tapModel::exact_accuracy_coefs(N_r, tp) that computes the \\(\\beta_k\\) coefficients.\n\n\n7 Identifiability\nBinomial mixtures are not always identifiable, meaning that the data don’t contain enough information to uniquely determine the parameters. If we fit two binomial distributions to the data with complete flexibility, each will have a “success rate” for the underlying Bernoulli trial, which determines the distribution’s mean and variance. There is also a weight parameter that defines the mixture; that determines the heights of the respective histograms of the two binomials. A simple data set would comprise the counts of Class 1 ratings for each subject. If the raters have reasonably high accuracy, a histogram of ratings will have a peak on the right, where a large fraction of raters of true Class 1 ratings agree. The average might be that 75 of 100 raters, on average agree that it’s Class 1. For true Class 0 cases, the raters might have an average of 25 of 100 ratings of Class 1 (erroneous ratings). The chapter on the interactive application shows how to simulate such data. It looks like this:\n\n\n\nSample ratings with 100 raters and 1000 subject, with high rating accuracy. It shows a clear separation of the two binomial distributions.\n\n\nIf we ask a generic optimization algorithm for the statistics to describe the two binomials, we can get two valid answers, e.g. distribution means of \\(\\mu_0 = 25\\) and \\(\\mu_1= 75\\) or \\(\\mu_0 = 75\\) and \\(\\mu_1 = 25\\). This is called “label-swapping,” because the solution doesn’t care which of the classes is which. One ad hoc solution is to include a restriction that the success rate for one of the distributions is greater than the other, but this is computationally awkward. The t-a-p model avoids this problem because the accuracy \\(a\\) is the non-negative difference between the means of the two binomials. In the example above, \\(a = .5 = (75-25)/100\\). Since \\(a\\) is bounded by 0 and 1, the model is generally identifiable.\nNon-identifiable cases do occur when one or more of the t-a-p parameters is zero or one. If \\(a = 1\\), it doesn’t matter what \\(p\\) is, for example. This can also cause problems when a parameter is close to one or zero. For an example of that, see Chapter 4: The Kappa Paradox. These issues can be investigated using Bayesian MCMC estimation, which provides a full posterior distribution for the parameters. The distribution may be multi-modal if there are two competing solutions. This gives us a way to detect degenerate solutions and look for a different model if desired. These degenerate cases aside, the tap parameterization of the binomial mixture has an advantage over generic optimization.\nWe can add more parameters to the basic t-a-p model. For example, we might split accuracy into Class 1 accuracy and Class 0 accuracy, as is illustrated in Chapter 4. These additions can lead to multiple solutions, which can often be detected from the MCMC distributions of parameters. This topic needs more development.\n\n\n8 The Dunning-Kruger Horizon\nThe Dunning–Kruger effect (Kruger & Dunning, 1999) is the tendency of people with low ability in some domain to overestimate their own ability (ignorance creates meta-ignorance). It turns out to be a useful metaphor for what happens when we estimate rater accuracy, because low rater accuracy degrades estimates of accuracy. We can visualize this effect using the linear model derived above for for exactly calculating accuracy (\\(a\\)) when \\(p=t=1/2\\) for two raters. If we suppose that thirty subjects are being rated (\\(N_s=30\\)), the t-a-p model assumes a binomial mixture distribution for the counts of Class 1 ratings across all subjects. In particular, if accuracy is zero (\\(a=0\\)), the count \\(n_1\\) of the number of subjects that have split ratings (one Class 1 and one Class 0) will be distributed as a simple binomial with mean \\(15 = 30(.5)\\). With zero accuracy, the raters are essentially flipping coins. The question is, given this coin-flipping distribution for the results of the ratings, what are the possible accuracy ratings that reflect those random samples? Since we have an exact formula for the accuracy, we can draw a picture.\n\n\nShow the code\nN_s = 30\n\ndf &lt;- tibble( n1 = 0:30,\n              `Pr[n1]` = dbinom(n1, N_s, .5),\n              a       = if_else(n1*2 &gt;= N_s, 0,  sqrt((N_s - 2*n1)/N_s)))\n\ndf |&gt; \n  gather(statistic, value, -n1) |&gt; \n  ggplot(aes(x = n1, y = value, group = statistic, color = statistic)) +\n  geom_rect(xmin = 11, xmax = 19, ymin = 0, ymax = Inf, \n            fill = \"#CCCCCC22\", color = \"#CCCCCC22\") +\n  geom_point() +\n  geom_line() +\n  theme_bw() +\n  xlab(\"Number of Class 1 ratings (n1)\") +\n  ylab(\"\")\n\n\n\n\n\n\n\n\nFigure 5: Illustrating the range of accuracy estimates from random (a = 0, p = .5) ratings of 30 subjects with two raters each. The key statistic is how many mixed ratings n1 (one of each class) are generated, and the blue line shows the probability of each n1 occuring. The shaded region captures 90% of the distribution. The red line shows the accuracy estimate resulting from the sample.\n\n\n\n\n\nThe plot in Figure 5 shows how bad our estimate of accuracy can be for random ratings. Half of the time the count of mixed ratings \\(n_1\\) will be greater than the mean of 15, and we’ll get the correct estimate of \\(a = 0\\). The other half of the time, when the number of mismatched ratings is less than 15, the estimate for accuracy quickly increases. We can see from the plot that there’s a significant chance that the estimated \\(a \\ge .25\\). At the left edge of the shaded area, where \\(n_1 = 11\\), there is a 5% chance of this outcome, and the resulting accuracy estimate is a little more than .5.\nThe t-a-p model is asymetrical with respect to accuracy. If the ratings are 100% accurate, we’ll get the correct answer of \\(a = 1\\) every time. But when the accuracy is zero, we’ll get a significant overestimate of \\(a\\) almost half the time. This effect produces a “zone of ignorance” in the following sense. If we can’t a priori rule out a very low accuracy, then as a precautionary measure we should entertain the idea that the ratings are purely random. Maybe something went wrong with the raters or coding or data transmission. Maybe the subject IDs got mixed up. If it’s possible that the ratings could be random, then we must condider how large an estimate of accuracy can be in that condition. That depends on the sample size and \\(p\\). Assuming \\(p=.5\\) is not universally applicable, but it’s a reasonable reference point.\nIf we choose the 5% mark as the cutoff value, as in Figure 5, then given the number of subjects and average number of ratings, we can simulate the largest (worst) \\(a\\) that can occur in those conditions at that threshold. I’ll call the resulting value of \\(a\\) the “DK horizon” as a nod to Dunning and Kruger. The analogy isn’t perfect, but the idea is that if accuracy (like expertise) is low enough, a reversal happens, where our estimates of the accuracy increase instead of decrease.\nFor the example here, the DK horizon is \\(a = .5\\), meaning with 30 subjects and 2 raters each, any result that has \\(a \\le .5\\) is highly suspect; we could be looking at the result of coin-flipping raters. The DK horizon is a veil of ignorance that prevents us from interpreting the t-a-p model coefficients when accuracy is too low. The horizon is affected by sample size, so we can do something like a power analysis to estimate where the DK horizon is.\n\n\nShow the code\nsampled_a &lt;- read_csv(\"data/a_sim_exact.csv\")\n\nsampled_a |&gt; \n  ggplot(aes(x = a, y = a_sim, group = a)) +\n  geom_boxplot() +\n  geom_abline() +\n  geom_smooth(aes(x = a, y = a_sim, group = 1), se = FALSE, color = \"steelblue\", linewidth = .9) +\n  facet_grid(N_s ~ N_r)\n\n\n\n\n\n\n\n\nFigure 6: Box plots showing the maximum likelihood estimate for accuracy (a) from samples drawn from a tap model with parameters t = p = 1/2 and a as shown on the horizontal axis. The 2, 5, 10, and 25 column headers are the number of raters for each subject, and the row headers along the right side are the number of subjects rated by each rater. The blue line is the average accuracy for the sampled ratings.\n\n\n\n\n\nAs is usually the case, larger sample sizes reduce sampling error. With 300 subjects and 25 raters each (bottom right), there’s hardly any sampling error. But for small sample sizes, the sampling error noticiably increases as accuracy approaches zero. For the 2-rater, 3-subject case in the top left of Figure 6, when \\(a &lt; .5\\), the estimation error quickly becomes so bad as to make the estimation worthless. Intuitively, if we got an estimate of \\(a = .3\\), say, the best we can say is that probably the accuracy is somewhere between zero and one half. For the 5-rater, 30-subject case, this “zone of ignorance” is not as bad, with the DK horizon at about \\(a =.25\\). This is useful knowledge when interpreting parameter estimates from a small sample of ratings.\n\n\n9 Overdispersion\nThe basic t-a-p model assumes fixed averages of the three parameters over raters and subjects. The most sensible of these assumptions is that there is a single value for \\(t\\) that represents the fraction of Class 1 cases. That leaves two parameters that are certainly oversimplified in the tap model, so that counts of Class 1 ratings per subject are likely to have more variance than a binomial model would. This is due to anticipated variance in rater ability and the difficulty in rating subjects, resulting in overdispersion. A general approach to this problem is to allow each rater to have a different accuracy rate \\(a_j\\) and each subject to have a different guessing rate \\(p_i\\). This is a hierarchical model, which is described in Chapter 5: Hierarchical Models. Other approaches include using a beta-binomial prior for parameters, Williams (1975). Also see Ascari & Migliorati (2021).\n\n\n10 Other Properties\nAs is shown in Appendix A, rater accuracy \\(a\\) is proportional to the correlation between the true and assigned classifications. If \\(C\\) is the true classification and \\(T\\) is the assigned classification, then\n\\[ \\text{Cor}(T, C) = a\\frac{\\sigma_T}{\\sigma_C} \\]\nRelated properties can be found in the appendix.\n\n\n\n\n\nReferences\n\nAgresti, A. (2003). Categorical data analysis (Vol. 482). John Wiley & Sons.\n\n\nAscari, R., & Migliorati, S. (2021). A new regression model for overdispersed binomial data accounting for outliers and an excess of zeros. Statistics in Medicine, 40(17), 3895–3914.\n\n\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., & Riddell, A. (2017). Stan: A probabilistic programming language. Journal of Statistical Software, 76(1).\n\n\nGrilli, L., Rampichini, C., & Varriale, R. (2015). Binomial mixture modeling of university credits. Communications in Statistics - Theory and Methods, 44(22), 4866–4879. https://doi.org/10.1080/03610926.2013.804565\n\n\nKruger, J., & Dunning, D. (1999). Unskilled and unaware of it: How difficulties in recognizing one’s own incompetence lead to inflated self-assessments. Journal of Personality and Social Psychology, 77(6), 1121.\n\n\nMcLachlan, G., & Peel, D. (2000). Wiley series in probability and statistics. Finite Mixture Models, 420–427.\n\n\nWilliams, D. (1975). 394: The analysis of binary responses from toxicological experiments involving reproduction and teratogenicity. Biometrics, 949–952."
  },
  {
    "objectID": "tapmodel.html#conditional-probabilities",
    "href": "tapmodel.html#conditional-probabilities",
    "title": "Chapter 2: The t-a-p Model",
    "section": "1.1 Conditional Probabilities",
    "text": "1.1 Conditional Probabilities\nThe tree diagram in figure @fig-tree models the assignments of ratings \\(c_{ij}\\) over subject \\(i\\) and rater \\(j\\), and can be read by multiplying the conditional probabilities on the edges from the top down to find the probability of a given classification being \\(c_{ij} = \\widehat{C_1}\\).\nIf a subject is not classified accurately, the classification for that rater is assumed to be made at random, with probability \\(p\\) of choosing \\(\\widehat{C_1}\\) regardless of the true class. So the conditional probability of a \\(\\widehat{C_1}\\) classification when the subject really is \\(C_1\\) is \\(\\text{Pr}(\\widehat{C_1} | C_1) = a + a'p\\). Similarly, \\(\\text{Pr}(\\widehat{C_1} | C_0) = a'p\\). This model assumes that guess rates for the two classes are the same independent of the true classification. More complex models are introduced later."
  },
  {
    "objectID": "tapmodel.html#binomial-mixtures",
    "href": "tapmodel.html#binomial-mixtures",
    "title": "Chapter 2: The t-a-p Model",
    "section": "1.2 Binomial Mixtures",
    "text": "1.2 Binomial Mixtures\nThe rating process posed in the t-a-p model is illustrated with the tree diagram and table of example ratings above. But both of those entail the use of the hidden true classification value for each subject. There are cases where that can be known, but in general it is not. What we usually have to work with is a table of ratings, from which we must infer the hidden variables like rater accuracy. The collection the ratings under the t-a-p assumptions fall into a well-studied probability distribution called a binomial mixture.\n\n\nCode\n# set the parameters\nN_r = 5    # number of raters for each subject \nN_s = 100  # number of subjects (not used here)\n\nt = .5 # fraction of subjects that are in fact class 1\na = .7 # probability of a rater rating a subject accurately\np = .5 # probability of a rater rating a subject as class 1 when rating inaccurately\n\n# find the conditional probabilities for each class\n\n  # probabilities of a class 1 rating for a class 0 subject\n  c0_probs = dbinom(0:N_r, N_r, prob = (1-a)*p)\n  \n  # probabilities of a class 1 rating for a class 1 subject\n  c1_probs = dbinom(0:N_r, N_r, prob = a + (1-a)*p)\n\n# create the mixture with t as the mixing parameter\nmixture = c0_probs * (1-t) + c1_probs * t\n\n# Plot the conditional probabilities\nplot(0:N_r, c0_probs, type=\"b\", col=\"blue\", pch=16, ylim=c(0, max(c(c0_probs, c1_probs, mixture))),\n     ylab=\"Probability\", xlab=\"Number of class 1 ratings for a given subject\", main=\"Binomial Mixture Plot\")\n\n# Add the second component (c1_probs)\nlines(0:N_r, c1_probs, type=\"b\", col=\"red\", pch=16)\n\n# Add the mixture as a black dashed line\nlines(0:N_r, mixture, type=\"b\", col=\"black\", lty=2, pch=16)\n\n# Add a legend\nlegend(\"topright\", legend=c(\"Class 0 Probabilities\", \"Class 1 Probabilities\", \"Mixture\"),\n       col=c(\"blue\", \"red\", \"black\"), lty=c(1, 1, 2), pch=16)\n\n\n\n\n\n\n\n\nFigure 2: Binomial mixture showing the probability of the number of class 1 ratings for a given subject. In blue is the distribution for true-class 0 subjects, in red is the distribution for true-class 1 subjects, and the black line is the mixture based on relative proportions.\n\n\n\n\n\nFigure 2 shows an example of how probabilities combine to create the mixture. Given the three t-a-p parameters plus the number of raters per subject, we apply the binomial probability density function using the probabilities found at the end of the previous section. The mixture is created by weighting these two distributions by their frequency in the sample space.\nFor a real data set, the red and blue plots in Figure 2 are assumed to exist, but are not directly accessible to us. Instead we can see something like the mixture (black dashed line). Even that isn’t exact, because it is subject to sampling error. The larger the number of subjects rated, the closer the empirical proportions will, in theory, converge to the ideal mixture distribution. Our task is to disentangle the t-a-p parameters from that rating distribution so that we find the parameter set that best matches the data.\nThe interactive app provided with the kappaZoo R package allows you to specify the t-a-p parameters and generated data from them, and we’ll use that here to visualize what a binomial mixture looks like.\n\n\n\nRating distribution with t = .5, a = .7, and p = .5\n\n\nThe accuracy rate \\(a\\) will affect the subject distributions. If \\(a = 0\\) the ratings will be distributed as \\(\\text{Bernoulli}(p)\\), independently of the subjects being rated. If \\(a=1\\), then all raters agree on the true value for each subject. Therefore the way we can reconstruct \\(a\\) from data is through the distribution of the within-subject ratings. The method used here can be seen as a latent class model with binomial mixture distributions. For a nice discussion of these ideas in practice see , which helpfully notes that binomial mixtures are statistically identifiable if the number of cases exceeds a low threshold . More generally, see chapter 14 on mixture models for discrete data.\nWe would like to know the true proportion \\(t\\) of the subjects belonging to \\(C_1\\) regardless of how they were rated, rater accuracy \\(a\\), and the proportion \\(p\\) of inaccurate assignments that are \\(\\widehat{C_1}\\). That goal describes the general model illustrated in the following section. A hierarchical version is given subsequently.\nAs is shown in Appendix A, rater accuracy \\(a\\) is proportional to the correlation between the true and assigned classifications. If \\(C\\) is the true classification and \\(T\\) is the assigned classification, then\n\\[\n\\text{Cor}(T, C) = a\\frac{\\sigma_T}{\\sigma_C}\n\\]\nThe t-app model focuses on raters more than subjects. For example, some rater models include a difficulty parameter for each subject, assuming that some are harder to classify than others. The t-a-p model does not include this, per se, but a random-effects version of the model is introduced later that allows for subject-specific coefficients \\(p_i\\), which adjusts the guessing rate per subject when a classification is made inaccurately. See @paun2018comparing for a discussion of difficulty parameters in machine-learning models similar to tap.\nnote: see https://www.jarad.me/courses/stat544/slides/Ch05/Ch05a.pdf\nand https://cran.r-project.org/web/packages/mixtools/vignettes/mixtools.pdf\nhttps://joss.theoj.org/papers/10.21105/joss.01505\nI should address the label-switching problem. I think that’s taken care of by way the classifications are pre-specified, AND accuracy is generic."
  },
  {
    "objectID": "tapmodel.html#overdispersion",
    "href": "tapmodel.html#overdispersion",
    "title": "Chapter 2: The t-a-p Model",
    "section": "1.3 Overdispersion",
    "text": "1.3 Overdispersion\nThe tap model assumes fixed averages over raters and subjects for the three parameters. The most sensible of these assumptions is that there is a single value for \\(t\\) that represents the fraction of Class 1 cases, although this idea has been challenged by @???. That leaves two parameters that are certainly oversimplified in the tap model, so that counts of Class 1 ratings per subject are likely to have more variance than a binomial model would. This is due to anticipated variance in rater ability and the difficulty in rating subjects, resulting in overdispersion. A general approach to this problem is to allow each rater to have a different accuracy rate \\(a_j\\) and each subject to have a different guessing rate \\(p_i\\). This is the hierarchical model, which is described later.\nOriginal suggestion to use beta-binomial @williams1975394, modern look at it in @ascari2021new."
  },
  {
    "objectID": "tapmodel.html#fitting-the-model",
    "href": "tapmodel.html#fitting-the-model",
    "title": "Chapter 2: The t-a-p Model",
    "section": "1.4 Fitting the Model",
    "text": "1.4 Fitting the Model\nThe first question about the model illustrated in figure \\(\\ref{fig:tree}\\) is whether it is computationally useful. Using known parameter values for \\(p, a, t\\) to generate simulated ratings, can we then recover the parameters from the data? The answer is yes, with some provisos. Given a data set \\(c_{ij}\\), we can fit a general model to estimate the three parameters \\(t\\), \\(a\\), and \\(p\\) using maximum likelihood to fit a binomial mixture model. The log likelihood function for the binomial mixture described by figure \\(\\ref{fig:tree}\\) with \\(N\\) subjects and \\(R&gt;1\\) raters is\n\\[\n\\ell(t,a,p;k_1, \\dots,k_N) = \\sum_{i = 1}^N \\log \\left( t\\binom{R}{k_i}(a + a'p)^{k_i}(a'p')^{R-k_i} + t'\\binom{R}{k_i}(a'p)^{k_i}(1-a'p)^{R-k_i} \\right)  \n\\]\nwhere \\(k_i=\\sum_{j}C_{ij}\\) the number of Class 1 ratings for subject \\(i\\). The sum over the logs is justified by the assumption that ratings are independent. It is straightforward to implement the function in the Bayesian programming language Stan , using uniform \\((0,1)\\) priors for the three parameters (see the Discussion section to access the source code).\nTo test the computational feasibility of this method, ratings were simulated using a range of values of \\(t\\), \\(a\\), and \\(p\\). The 729 trials each simulated 300 subjects with five raters each, using all combinations of values ranging from .1 to .9 in increments of .1 for each of \\(t\\), \\(a\\), and \\(p\\). The Stan engine uses a Markov chain Monte Carlo (MCMC) algorithm to gather representative samples from the joint probability density of the three parameters. Each run used 1,000 iterations (800 after warm-up) with four chains each.\n[need to cite @gelman2020bayesian and use those methods]\n\n\nCode\nregenerate = FALSE\n\nif(regenerate == TRUE){\n  param_grid &lt;- expand.grid(t = seq(.1,.9,.1),a = seq(.1,.9,.1),p = seq(.1,.9,.1))\n  \n  n_sims &lt;- nrow(param_grid)\n  N &lt;- 300 # number of subjects\n  R &lt;- 5   # number of raters\n  \n  out &lt;- data.frame()  \n  fixed_model_spec &lt;- stan_model(\"code/fixed_effects.stan\")\n  \n  for(i in 1:n_sims){\n    ratings &lt;- generate_ratings(N, R, param_grid$p[i], param_grid$a[i], param_grid$t[i])  \n    \n    # get the number of Class 1 ratings per subject in a vector\n    counts &lt;- ratings %&gt;% \n      group_by(SubjectID) %&gt;% \n      summarize(k = sum(RatedClass)) %&gt;% \n      select(k) %&gt;% \n      pull()\n    \n    kappa &lt;- sqrt(fleiss(counts,R))\n      \n    fixed_model &lt;- sampling(object = fixed_model_spec, \n                              data = list(N = N, R = R, S = 1, count = counts), \n                              iter = 1000,\n                              warmup = 200,\n                              thin = 1)\n      \n      stats &lt;- rbind( broom::tidy(fixed_model),\n                      data.frame(term = \"root kappa\",estimate = kappa, std.error = NA,\n                                 stringsAsFactors = FALSE)) %&gt;% \n                      mutate(p = param_grid$p[i], a = param_grid$a[i], t = param_grid$t[i])\n    \n      out &lt;- rbind(out,stats)\n    }\n    write_csv(out,\"data/fit_test_sim.csv\")\n} else {\n  out &lt;- read_csv(\"data/fit_test_sim.csv\")\n}\n\n# format for plotting\npdf &lt;- out %&gt;% \n#  filter(a &gt; .2) %&gt;% \n  select(parameter = term, value = estimate, actual_p = p, actual_a = a, actual_t = t) %&gt;% \n  spread(parameter, value) %&gt;% \n  rename(estimated_a = accuracy, estimated_p = p, kappa_a = `root kappa`, estimated_t = t) %&gt;% \n  mutate(Sim = row_number()) %&gt;% \n  gather(parameter, value, -Sim) %&gt;% \n  separate(parameter, into = c(\"type\",\"parameter\"), sep = \"_\") %&gt;% \n  spread(type, value)\n\n  ggplot(pdf, aes(x = actual, y = estimated, group = actual)) +\n    geom_boxplot() +\n    geom_abline(slope = 1, intercept = 0) +\n    facet_grid(. ~ parameter) +\n    theme_bw()\n\n\n\n\n\nBox and whisker plots show parameter estimates from simulations of rater data \\(t\\)-\\(a\\)-\\(p\\) values ranging from .1 to .9. The diagonal line marks perfect estimates.\n\n\n\n\nThe accuracy measure \\(a\\) and the Class 1 “guess rate” \\(p\\) are stable across scenarios in figure \\(\\ref{fig:tap_sim}\\), but the estimated true fraction of Class 1 cases \\(t\\) is sensitive to values of \\(a\\) near zero. To see this, substitute \\(a = 0\\) into the likelihood function to get\n\\[\n\\begin{aligned}\n\\ell(t,p;a = 0, k_1, \\dots,k_N) &= \\sum_{i = 1}^N \\log \\left( t\\binom{R}{k_i}p^{k_i}(p')^{R-k_i} + t'\\binom{R}{k_i}p^{k_i}(p')^{R-k_i} \\right) \\\\\n&= \\sum_{i = 1}^N \\log \\left( \\binom{R}{k_i}p^{k_i}(1-p)^{R-k_i}  \\right)\n\\end{aligned}\n\\]\nshowing that \\(t\\) is under-determined when \\(a = 0\\), and we should expect poor behavior as \\(a\\) nears zero. This is intuitive: if the raters are only guessing, they should give us no information about the true \\(C_1\\) rate. If the data in figure \\(\\ref{fig:tap_sim}\\) are filtered to \\(a &gt; .2\\) the estimates of \\(t\\) greatly improve. Aside from extreme values of \\(a\\) affecting the estimation of \\(t\\), a visual inspection of the scatter plots of the parameter estimates shows no correlations."
  },
  {
    "objectID": "tap.html",
    "href": "tap.html",
    "title": "tap",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "tap.html#quarto",
    "href": "tap.html#quarto",
    "title": "tap",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\nAgresti, A. (2003). Categorical data analysis (Vol. 482). John Wiley & Sons.\n\n\nAickin, M. (1990). Maximum likelihood estimation of agreement in the constant predictive probability model, and its relation to cohen’s kappa. Biometrics, 293–302.\n\n\nAscari, R., & Migliorati, S. (2021). A new regression model for overdispersed binomial data accounting for outliers and an excess of zeros. Statistics in Medicine, 40(17), 3895–3914.\n\n\nBassett, R., & Deride, J. (2019). Maximum a posteriori estimators as a limit of bayes estimators. Mathematical Programming, 174, 129–144.\n\n\nBennett, E. M., Alpert, R., & Goldstein, A. (1954). Communications through limited-response questioning. Public Opinion Quarterly, 18(3), 303–308.\n\n\nBonett, D. G. (2022). Statistical inference for g-indices of agreement. Journal of Educational and Behavioral Statistics, 47(4), 438–458.\n\n\nBrennan, R. L., Measurement in Education, N. C. on, et al. (2006). Educational measurement. Praeger Publishers,.\n\n\nButton, C. M., Snook, B., & Grant, M. J. (2020). Inter-rater agreement, data reliability, and the crisis of confidence in psychological research. Quant Methods Psychol, 16(5), 467–471.\n\n\nByrt, T., Bishop, J., & Carlin, J. B. (1993). Bias, prevalence and kappa. Journal of Clinical Epidemiology, 46(5), 423–429.\n\n\nCarpenter, B. (2008). Multilevel bayesian models of categorical data annotation. Unpublished Manuscript, 17(122), 45–50.\n\n\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., & Riddell, A. (2017). Stan: A probabilistic programming language. Journal of Statistical Software, 76(1).\n\n\nChaturvedi, S., & Shweta, R. (2015). Evaluation of inter-rater agreement and inter-rater reliability for observational data: An overview of concepts and methods. Journal of the Indian Academy of Applied Psychology, 41(3), 20–27.\n\n\nCicchetti, D. V., & Feinstein, A. R. (1990). High agreement but low kappa: II. Resolving the paradoxes. Journal of Clinical Epidemiology, 43(6), 551–558.\n\n\nCohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37–46.\n\n\nCronbach, L. J., Rajaratnam, N., & Gleser, G. C. (1963). Theory of generalizability: A liberalization of reliability theory. British Journal of Statistical Psychology, 16(2), 137–163.\n\n\nDavani, A. M., Dı́az, M., & Prabhakaran, V. (2022). Dealing with disagreements: Looking beyond the majority vote in subjective annotations. Transactions of the Association for Computational Linguistics, 10, 92–110.\n\n\nDawid, A. P., & Skene, A. M. (1979). Maximum likelihood estimation of observer error-rates using the EM algorithm. Journal of the Royal Statistical Society: Series C (Applied Statistics), 28(1), 20–28.\n\n\nDelgado, R., & Tibau, X.-A. (2019). Why cohen’s kappa should be avoided as performance measure in classification. PloS One, 14(9), e0222916.\n\n\nEngelhard, G. (2012). Examining rating quality in writing assessment: Rater agreement, error, and accuracy. Journal of Applied Measurement, 13, 321–335.\n\n\nEngelhard Jr, G. (1996). Evaluating rater accuracy in performance assessments. Journal of Educational Measurement, 33(1), 56–70.\n\n\nEubanks, D. (2017). (Re)visualizing rater agreement:beyond single-parameter measures. Journal of Writing Analytics, 1.\n\n\nEubanks, D. A. (2014). Causal interfaces. Arxiv.org Preprint. http://arxiv.org/abs/1404.4884v1\n\n\nFleiss, J. L. (1971). Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5), 378.\n\n\nFleiss, J. L., Levin, B., & Paik, M. C. (2013). Statistical methods for rates and proportions. john wiley & sons.\n\n\nGelman, A., & Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge university press.\n\n\nGelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., Kennedy, L., Gabry, J., Bürkner, P.-C., & Modrák, M. (2020). Bayesian workflow. arXiv Preprint arXiv:2011.01808.\n\n\nGettier, E. L. (1963). Is justified true belief knowledge? Analysis, 23(6), 121–123.\n\n\nGrilli, L., Rampichini, C., & Varriale, R. (2015). Binomial mixture modeling of university credits. Communications in Statistics - Theory and Methods, 44(22), 4866–4879. https://doi.org/10.1080/03610926.2013.804565\n\n\nGwet, K. L. (2008). Computing inter-rater reliability and its variance in the presence of high agreement. British Journal of Mathematical and Statistical Psychology, 61(1), 29–48.\n\n\nHodgson, R. T. (2008). An examination of judge reliability at a major US wine competition. Journal of Wine Economics, 3(2), 105–113.\n\n\nHolley, J. W., & Guilford, J. P. (1964). A note on the g index of agreement. Educational and Psychological Measurement, 24(4), 749–753.\n\n\nHovy, D., Berg-Kirkpatrick, T., Vaswani, A., & Hovy, E. (2013). Learning whom to trust with MACE. Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1120–1130.\n\n\nKrippendorff, K. (2013). Commentary: A dissenting view on so-called paradoxes of reliability coefficients. Annals of the International Communication Association, 36(1), 481–499.\n\n\nKrippendorff, K. (2018). Content analysis: An introduction to its methodology. Sage publications.\n\n\nKrippendorff, K., & Fleiss, J. L. (1978). Reliability of binary attribute data. Biometrics, 34(1), 142–144.\n\n\nKumar, S., Hooi, B., Makhija, D., Kumar, M., Faloutsos, C., & Subrahmanian, V. (2018). Rev2: Fraudulent user prediction in rating platforms. Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, 333–341.\n\n\nLandis, J. R., & Koch, G. G. (1977). The measurement of observer agreement for categorical data. Biometrics, 159–174.\n\n\nMcLachlan, G., & Peel, D. (2000). Wiley series in probability and statistics. Finite Mixture Models, 420–427.\n\n\nPaun, S., Carpenter, B., Chamberlain, J., Hovy, D., Kruschwitz, U., & Poesio, M. (2018). Comparing bayesian models of annotation. Transactions of the Association for Computational Linguistics, 6, 571–585.\n\n\nRasch, G. (1977). On specific objectivity. An attempt at formalizing the request for generality and validity of scientific statements in symposium on scientific objectivity, vedbaek, mau 14-16, 1976. Danish Year-Book of Philosophy Kobenhavn, 14, 58–94.\n\n\nRasch, G. (1993). Probabilistic models for some intelligence and attainment tests. MESA Press.\n\n\nRoss, V., & LeGrand, R. (2017). Assessing writing constructs: Toward an expanded view of inter-reader reliability. Journal of Writing Analytics, 1.\n\n\nScott, W. A. (1955). Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly, 321–325.\n\n\nShabankhani, B., Charati, J. Y., Shabankhani, K., & Cherati, S. K. (2020). Survey of agreement between raters for nominal data using krippendorff’s alpha. Arch Pharma Pract, 10(S1), 160–164.\n\n\nShrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing rater reliability. Psychological Bulletin, 86(2), 420.\n\n\nTeam, S. D. (2022). Stan user’s guide 2.34. Stan Development Team.\n\n\nVach, W., & Gerke, O. (2023). Gwet’s AC1 is not a substitute for cohen’s kappa–a comparison of basic properties. MethodsX, 102212.\n\n\nWilliams, D. (1975). 394: The analysis of binary responses from toxicological experiments involving reproduction and teratogenicity. Biometrics, 949–952."
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Appendix A: Statistical Details",
    "section": "",
    "text": "1 Closed Form Accuracy Coefficients\nWe’ll show that\n\\[\na^2 = \\sum_{k=0}^R\\beta_k\\eta_k\n\\] for unbiased raters (\\(t=p\\)), so that the Fleiss kappa can be expressed as a weighted sum of the proportions of subject counts of Class 1 ratings. This provides details for the section on exact formulas in Chapter 2. A closed form for the coefficients is derived below.\nThe Fleiss kappa calculates match rates per subject. Suppose we have \\(R\\) raters who assigned \\(k\\) ratings of Class 1 for subject \\(i\\), then the number of matched ratings is the sum of Class 1 matches and Class 0 matches, or \\(M_i = k(k-1)/2 + \\bar{k}(\\bar{k}-1)/2\\), where \\(\\bar{k} = R - k\\). The match rate is this count out of the number of possible matches, which is \\(R(R-1)/2\\). With \\(R\\) raters, \\(k \\epsilon \\{0,1,..,R\\}\\), and over the \\(N\\) subjects there will be some count \\(n_k\\) for the number of the occurances of each possible value of \\(k\\). The Fliess kappa calculation averages over these subject match rates. This average match rate \\(m_o\\) can be simplified with\n\\[\n\\begin{aligned}\nm_o &=  \\frac{2}{NR(R-1)}\\sum_{k=0}^{R} n_k M_k \\\\\n&= \\frac{1}{R(R-1)}\\sum_{k=0}^{R} \\eta_k \\left( k(k-1) + \\bar{k}(\\bar{k}-1) \\right)\\\\\n&= \\frac{1}{R(R-1)}\\sum_{k=0}^{R} \\eta_k \\left( 2k^2 -2Rk+R^2-R\\right)\\\\\n&= \\frac{1}{R-1}\\sum_{k=0}^{R} \\eta_k \\left( R-2k-1\\right) + \\frac{2}{R(R-1)}\\sum_{k=0}^{R} k^2\\eta_k \\\\\n&= \\frac{R - 1 -2 \\sum_{k=0}^{R} k\\eta_k  }{R-1} +  \\frac{2}{R(R-1)}\\sum_{k=0}^{R} k^2\\eta_k \\\\\n&= 1 - \\frac{2}{R-1}\\sum_{k=0}^{R} k\\eta_k + \\frac{2}{R(R-1)}\\sum_{k=0}^{R} k^2\\eta_k \\\\\n\\end{aligned}\n\\]\nwhere \\(\\eta_k = n_k / N\\) is the proportion of subjects with \\(k\\) ratings of Class 1, so that \\(\\sum \\eta_k = 1\\). It will be useful to use the fact that the average rate of Class 1 ratings is\n\\[\nc = \\frac{1}{R}\\sum_{k=0}^{R} k\\eta_k\\,,\n\\] since the sum adds up the number of Class 1 ratings over all subjects divided by the number of subjects, and \\(NR\\) is the total number of ratings. Using that idea, we can express\n\\[\n\\begin{aligned}\nm_o &= 1 - \\frac{2R}{R-1}c + \\frac{2}{R(R-1)}\\sum_{k=0}^{R} k^2\\eta_k/R \\\\\n&:= 1 - 2\\frac{Rc-\\gamma}{R-1} \\, ,\n\\end{aligned}\n\\] where \\(\\gamma = \\sum k^2\\eta_k/R\\) to simplify the notation.\nThe Fleiss kappa is then the difference between the average match rate and the expected match rate under independence, which is the sum of the products of the marginal proportions of Class 1 ratings for each rater.\nThe Fleiss formula is\n\\[\n\\kappa = \\frac{m_o - m_c}{1 - m_c},\n\\]\nwhere \\(m_c = c^2 + \\bar{c}^2 = 2c^2 - 2c + 1\\). Here \\(c\\) is the average Class 1 rate over all ratings,\nSo that we can express the kappa formula as \\[\n\\begin{aligned}\n\\kappa &= \\frac{1 - 2\\frac{Rc-\\gamma}{R-1}-2c^2 + 2c - 1  }{1-2c^2 + 2c - 1} \\\\\n&= \\frac{\\gamma/c-R}{(R-1)(1-c)} + 1\n\\end{aligned}\n\\] Under the unbiased rater assumption we have \\(p=t=c\\), and if we fix \\(t\\) to a constant, with the only unknown being \\(a\\), the formula reduces to\n\\[\n\\begin{aligned}\n\\kappa_t = a^2 &=\\frac{\\frac{1}{Rt} \\sum_{k=0}^{R} k^2\\eta_k-R}{(R-1)\\bar{t}} + 1 \\\\\n         &= \\sum_{k=0}^{R} \\left( \\frac{k^2}{R(R-1)t\\bar{t}}   \\right)\\eta_k + 1 - \\frac{R}{(R-1)\\bar{t}} \\\\\n         &:= \\sum_{k=0}^{R} \\beta_k\\eta_k  \\,, \\\\\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\beta_k &= \\frac{k^2}{R(R-1)t\\bar{t}}+ 1 - \\frac{R}{(R-1)\\bar{t}}. \\\\\n\\end{aligned}\n\\] It seems odd algebraically to bring the constants inside the sum as if they were multiplied by the \\(\\eta_k\\) proportions, but this works because \\(\\sum \\eta_k = 1\\).\nThe coefficients aren’t unique if the distribution of \\(\\eta_k\\) is symmetrical, as is the case we worked out by hand in Chapter 2, with \\(R=2\\) and \\(t = p= .5\\). In case of symmetrical proportions of Class 1 ratings, we could use\n\\[\n\\begin{aligned}\n\\dot\\beta_k &= \\frac{1}{2}(\\beta_k+\\beta_{R-k}). \\\\\n\\end{aligned}\n\\] In the special case when \\(t = p = 1/2\\) and \\(R = 2\\), we have \\(\\beta_0 = -3\\), \\(\\beta_1 = -1, \\beta_2 = 5\\), so \\(\\dot \\beta_0 = \\beta_2 = 1\\) and \\(\\dot \\beta_1 = -1\\) as we found with the direct derivation. In general we can’t use this method, because \\(\\eta_k\\) isn’t usually symmetrical.\n\n\nShow the code\nR &lt;- 30\n\nbetas &lt;- data.frame(\n  k = 0:R,\n  beta1 = tapModel::exact_accuracy_coefs(R, .1),\n  beta2 = tapModel::exact_accuracy_coefs(R, .3),\n  beta3  =tapModel::exact_accuracy_coefs(R, .5)) |&gt; \ngather(beta, val, -k) |&gt; \n  mutate(tp = case_when( beta == \"beta1\" ~ \".1\",\n                           beta == \"beta2\" ~ \".3\",\n                           TRUE ~ \".5\"))\n\nbetas |&gt; \n  ggplot(aes(x = k, y = val, group = tp, color = tp)) +\n  geom_point() +\n  geom_line() +\n  theme_bw() +\n  ylab(\"beta_k\") +\n  xlab(\"Number of Class 1 ratings, k\")\n\n\n\n\n\n\n\n\nFigure 1: A sample of weights for exact formulation of accuracy squared, with 30 raters and a selection of values for t=p.\n\n\n\n\n\n\n\nShow the code\nparam_grid &lt;- expand.grid(t = seq(.05, .95, .05),\n                          a = seq(.05, .95, .05)) |&gt; \n  mutate( a = a + t/30,\n          a_calc = NA)\n\nfor(i in 1:nrow(param_grid)){\n  t &lt;- param_grid$t[i]\n  a &lt;- param_grid$a[i]\n  props &lt;- tapModel:::exact_count_probabilites(10,t,a,t)\n  tap_coefs &lt;- exact_accuracy_coefs(10,t)\n  param_grid$a_calc[i] &lt;- sqrt(sum(props*tap_coefs))\n}\n\nparam_grid |&gt; \n  ggplot(aes(x = a, y = a_calc, color = t)) +\n  geom_point() +\n  geom_abline() +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 2: Validity check, calculating accuracy from the linear representation for a range of values of t = p.\n\n\n\n\n\nThe linear expression works precisely across the range of parameters in the grid illustrated in Figure 2. However, the calculation is quite sensitive to violations of the unbiased rater assumption.\n\n\nShow the code\nparam_grid &lt;- expand.grid(t = seq(.05, .95, .05),\n                          a = seq(.05, .95, .05)) |&gt; \n  mutate( a = a + t/30,\n          a_calc = NA,\n          a_kappa = NA)\n\nfor(i in 1:nrow(param_grid)){\n  t &lt;- param_grid$t[i]\n  a &lt;- param_grid$a[i]\n  props &lt;- tapModel:::exact_count_probabilites(10,t-.02,a,t+.02)\n  tap_coefs &lt;- exact_accuracy_coefs(10,t)\n  param_grid$a_calc[i] &lt;- sqrt(sum(props*tap_coefs))\n  counts &lt;- data.frame(N_r = 10, N_c = 0:10, n = props)\n  param_grid$a_kappa[i] &lt;- fleiss_kappa(counts)$a\n}\n\nparam_grid |&gt; \n  ggplot(aes(x = a, y = a_calc, color = t)) +\n  geom_point() +\n  geom_abline() +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 3: Calculations of accuracy using the linear combination with a minor violation of the unbiased rater assumption.\n\n\n\n\n\nThe results in Figure 3 come from calculating the coefficients with a value of \\(t\\), then replacing \\(t\\) and \\(p\\) in the simulated data with \\(\\hat{t} = t - .02\\) and \\(\\hat{p} = t + .02\\). The resulting estimates of accuracy can become quite poor. This is not true of the Fleiss kappa, which has more robust approximation when the unbiased rater assumption is violated.\n\n\nShow the code\nparam_grid |&gt; \n  ggplot(aes(x = a, y = a_kappa, color = t)) +\n  geom_point() +\n  geom_abline() +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 4: Accuracy approximations by Fleiss kappa when the unbiased rater assumption is slightly violated.\n\n\n\n\n\nThe relatively low approximation error in Figure 4 shows a direct comparison to the linear combination approximation.\n\n\n2 Correlation of Ratings between Raters\nGiven two distinct raters \\(i\\) and \\(j\\) with common accuracy \\(a\\) and guess probability \\(p\\), what’s the correlation between their ratings? Let \\(c = E[C_i] = E[C_j] = ta + p\\bar{a}\\). Capital letters denote random binary variables, so that \\(A_i\\) is one if the first rater made an accurate assessment and zero if not. \\(T\\) is the true value of a common subject being rated. The covariance between the two raters’ ratings is\n\\[\n\\begin{aligned}\n\\textrm{Cor}(C_i, C_j) &= \\frac{\\textrm{Cov}(C_i, C_j)}{\\sqrt{\\textrm{Var}(C_i)\\textrm{Var}(C_j)}} \\\\\n&= \\frac{E[(TA_i + \\bar{A_i}P_i)(TA_j + \\bar{A_j}P_j)] - c^2}{\\textrm{Var}(C)} \\\\\n&= \\frac{ta^2 + 2ta\\bar{a}p + \\bar{a}^2p^2  - (ta + p\\bar{a})^2}{(ta + p\\bar{a})\\overline{(ta + p\\bar{a})  }} & (\\textrm{since }T^2 = T)\\\\\n&= \\frac{ta^2 + 2ta\\bar{a}p - t^2a^2 - 2tap\\bar{a}}{(ta + p\\bar{a})\\overline{(ta + p\\bar{a})  }} \\\\\n&= \\frac{a^2t\\bar{t}}{c\\bar{c }} \\\\\n\\end{aligned}\n\\]\nRater accuracy can be obtained via\n\\[\na^2 =\\frac{c\\overline{c }}{t\\bar{t}} \\text{Cor}(C_a, C_b) = \\frac{c\\overline{c }}{t\\bar{t}}\\kappa_{fleiss}\n\\tag{1}\\]\nThe correlation between two raters’ ratings of the same subject is the intraclass correlation coefficient (ICC) for a two-way random effects model Shrout & Fleiss (1979), which has been shown to be equivalent to the Fleiss kappa as described in Fleiss et al. (2013), p. 611-12. Under the \\(t = p\\) unbiased rater assumption, \\(c = ta + \\bar{a}p = p\\), so that the Fliess kappa is (again) shown to be \\(a^2\\) under that condition. The relation Equation 1 suggests that the Fliess kappa could be adjusted for cases when \\(t \\ne p\\) by making assumptions about those two parameters. For example, maybe the true rate is known from other information. The overall rate of Class 1 ratings \\(c\\) can be estimated directly from the data, but estimating \\(t\\) requires either prior knowledge of the context or using the full t-a-p estimation process, in which case there’s no need to compute the Fliess kappa.\n\n\n3 Correlation Between Ratings and True Values\nIt is of interest to find the correlation between \\(T_i\\) the truth value of subject \\(i\\) and the resulting classification \\(C_i\\). Note that both of the random variables \\(T_i\\) and \\(C_i\\) take only values of zero or one, so squaring them doesn’t change their values. This fact simplifies computations, for example \\(E[C_i^2] = E[C_i] = ta + p\\bar{a}\\). The variance of \\(C\\) is therefore \\[\n\\begin{aligned}\n\\textrm{Var}(C) &= E[C^2] - E^2[C] \\\\\n       &= c - c^2 \\\\\n       &= c\\bar{c} \\\\\n       &= (ta + p\\bar{a})\\overline{(ta + p\\bar{a})}. \\\\\n\\end{aligned}\n\\] Similarly, \\(Var(T) = t\\bar{t}\\). The correlation between true values and ratings is then\n\\[\n\\begin{aligned}\n\\text{Cor}(T, C) &= \\frac{\\text{Cov}(T, C)}{\\sqrt{\\text{Var}(T)\\text{Var}(C)}} \\\\\n&= \\frac{E[T(Ta + p\\bar{a}) ] - t(ta + p\\bar{a})}{\\sqrt{t\\bar{t} c\\bar{c}}  } \\\\\n&= \\frac{t(a + p\\bar{a})  - t(ta + p\\bar{a})}{\\sqrt{t\\bar{t} c\\bar{c}}  } \\\\\n&= a\\frac{\\sqrt{t\\bar{t}}}{\\sqrt{c\\bar{c}}} \\\\\n&= a\\frac{\\sigma_T}{\\sigma_C}.\n\\end{aligned}\n\\tag{2}\\]\nWhere \\(\\sigma\\) is the standard deviation (square root of variance). The relationship in Equation 2 can also be seen as \\(a = \\text{Cor}(T, C) \\frac{\\sigma_C}{\\sigma_T}\\), which means \\(a\\) can be interpreted as the slope of the regression line \\(C = \\beta_0 + \\beta_1T + \\varepsilon\\), i.e. \\(a = \\beta_1\\). In the unbiased rater case \\(p = t\\), \\(\\sigma_C = \\sigma_T\\) and so \\(\\text{Cor}(T, C) = a\\). It can also be shown that for a \\(t\\)-\\(a_1,a_0\\)-\\(p\\) model, the \\(t=p\\) assumption leads to \\(a = \\sqrt{a_1a_0}.\\) See Eubanks (2014).\nThe two correlations derived here are related by \\(\\text{Cor}^2(T, C) = \\text{Cor}(C_i, C_j)\\).\n\n\n4 Alternate Derivation of Fleiss Kappa Relationship\nThis appendix gives an alternative derivation for the Fleiss kappa’s relationship to rater accuracy under the unbiased rater assumption.\nThe Fleiss kappa Fleiss (1971) is a particular case of Krippendorf’s alpha Krippendorff & Fleiss (1978) and a multi-rater extension of Scott’s pi Scott (1955). The statistic compares the overall distribution of ratings (ignoring subjects) to the average over within-subject distributions. These distributions are used to compute the number of observed matches (i.e. agreements) \\(m_o\\) over subjects \\(i = 1 \\dots N\\). For a two-category classification with a fixed number of raters \\(R&gt;1\\) per subject the number of matched ratings for a given subject \\(i\\) is\n\\[\n\\begin{aligned}\nm_o &= \\frac{ {\\binom{k_i}{2}} + {\\binom{R - k_i}{2}}}{\\binom{R}{2}} \\\\\n&= \\frac{k_i(k_i-1)+ (R-k_i)(R - k_i-1)}{R(R-1)} \\\\\n&= \\frac{2k_i^2 - 2k_iR + R^2-R}{R(R-1)}\n\\end{aligned}\n\\]\nwhere \\(k_i\\) is the count of Class 1 ratings for the \\(i\\)th subject. The match rates are averaged over the subjects to get \\(\\text{E}[m_o]\\) and then a chance correction is applied with\n\\[\n\\kappa = \\frac{\\text{E}[m_i] - \\text{E}[m_c]}{1-\\text{E}[m_c]},\n\\]\nwhere \\(\\text{E}[m_c]\\) is the expected number of matches due to chance. Recall that different agreement statistics make different assumptions about this chance. Using the t-a-p model, and assuming \\(t = p\\), the true rate of Class 1 \\(t\\) is assumed to be \\(\\text{E}[c_{ij}]\\), so \\(\\text{E}[m_c] = t^2 + (1-t)^2\\), the asymptotic expected match rate for independent Bernoulli trials with success probability \\(t\\).\nBy replacing \\(p\\) with \\(t\\) in the t-a-p model’s mixture distribution for the number \\(k\\) of Class 1 ratings a subject is assigned we obtain\n\\[ Pr(k) = t \\binom{R}{k} (a + \\bar{a}t)^k (\\bar{a}\\bar{t})^{R - k} + \\bar{t} \\binom{R}{k} (\\bar{a}t)^k (1 - \\bar{a}t)^{R - k} \\] so it suffices for large \\(N\\) to write the expected match rate as\n\\[\n\\begin{aligned}\n      \\text{E}[m(a)] &= \\sum_{k=0}^R \\frac{2k^2 - 2kR + R^2-R}{R(R-1)}\\text{Pr}(k;a,t) \\\\\n      &= \\sum_{k=0}^R \\frac{2k^2 - 2kR + R^2-R}{R(R-1)} \\left[ t\\binom{R}{k}(a + \\bar{a}t)^{k}(\\bar{a}\\bar{t})^{R-k_i} + \\bar{t}\\binom{R}{k}(\\bar{a}t)^{k}(1-\\bar{a}t)^{R-k} \\right] \\\\\n      &= \\frac{2}{R(R-1)} \\sum_{k=0}^R k^2 \\left[ t \\text{ Binom}(R,k,a+\\bar{a}t) + \\bar{t} \\text{ Binom}(R,k,\\bar{a}t) \\right] \\\\\n      &- \\frac{2R}{R(R-1)} \\sum_{k=0}^R k \\left[ t \\text{ Binom}(R,k,a+\\bar{a}t) + \\bar{t} \\text{ Binom}(R,k,\\bar{a}t) \\right] \\\\\n      &+ \\frac{R(R-1)}{R(R-1)} \\sum_{k=0}^R \\left[ t \\text{ Binom}(R,k,a+\\bar{a}t) + \\bar{t} \\text{ Binom}(R,k,\\bar{a}t) \\right] \\\\\n      &= \\frac{2}{R(R-1)} \\left[ tR(a+\\bar{a}t)\\bar{a}\\bar{t}+tR^2(a+\\bar{a}t)^2 + \\bar{t}R(\\bar{a}t)(1-\\bar{a}t)+\\bar{t}R^2(\\bar{a}t)^2\\right] \\\\\n      &- \\frac{2}{R-1} \\left[ tR(a+\\bar{a}t) +  \\bar{t}R(\\bar{a}t)\\right] +1 \\\\\n      &= 2a^2(t-t^2) + 2t^2 -2t + 1,\n\\end{aligned}\n\\]\nusing the moment identities to gather the sums. Here, \\(t\\) and \\(R\\) are fixed, and \\(m(a)\\) is the average match rate over cases, which depends on unknown \\(a\\) and fixed \\(t = \\text{E}[c_{ij}]\\). Now we can compute the Fleiss kappa with\n\\[\n\\begin{aligned}\n\\kappa_{fleiss} &= \\frac{\\text{E}[m_i] - \\text{E}[m_*]}{1-\\text{E}[m_*]} \\\\\n            &= \\frac{2a^2(t-t^2) + 2t^2 -2t + 1 - (t^2+(1-t)^2)}{1-(t^2+(1-t)^2)} \\\\\n            &= a^2.\n\\end{aligned}\n\\]\nSo kappa is the square of accuracy under the unbiased rater assumption, with constant rater accuracy and fixed number of raters. The relationship does not depend on the true distribution \\(t\\) of Class 1 cases.\n\n\n\n\n\nReferences\n\nEubanks, D. A. (2014). Causal interfaces. Arxiv.org Preprint. http://arxiv.org/abs/1404.4884v1\n\n\nFleiss, J. L. (1971). Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5), 378.\n\n\nFleiss, J. L., Levin, B., & Paik, M. C. (2013). Statistical methods for rates and proportions. john wiley & sons.\n\n\nKrippendorff, K., & Fleiss, J. L. (1978). Reliability of binary attribute data. Biometrics, 34(1), 142–144.\n\n\nScott, W. A. (1955). Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly, 321–325.\n\n\nShrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing rater reliability. Psychological Bulletin, 86(2), 420."
  },
  {
    "objectID": "correlation.html#correlation-between-ratings-and-true-values",
    "href": "correlation.html#correlation-between-ratings-and-true-values",
    "title": "Appendix A: Statistical Details",
    "section": "",
    "text": "It is of interest to find the correlation between \\(T_i\\) the truth value of subject \\(i\\) and the resulting classification \\(C_i\\). Note that both of the random variables \\(T_i\\) and \\(C_i\\) take only values of zero or one, so squaring them doesn’t change their values. This fact simplifies computations, for example \\(E[C_i^2] = E[C_i] = ta + p\\bar{a}\\). The variance of \\(C\\) is therefore \\[\n\\begin{aligned}\n\\textrm{Var}(C) &= E[C^2] - E^2[C] \\\\\n       &= c - c^2 \\\\\n       &= c\\bar{c} \\\\\n       &= (ta + p\\bar{a})\\overline{(ta + p\\bar{a})}. \\\\\n\\end{aligned}\n\\] Similarly, \\(Var(T) = t\\bar{t}\\). The correlation between true values and ratings is then\n\\[\n\\begin{aligned}\n\\text{Cor}(T, C) &= \\frac{\\text{Cov}(T, C)}{\\sqrt{\\text{Var}(T)\\text{Var}(C)}} \\\\\n&= \\frac{E[T(Ta + p\\bar{a}) ] - t(ta + p\\bar{a})}{\\sqrt{t\\bar{t} c\\bar{c}}  } \\\\\n&= \\frac{t(a + p\\bar{a})  - t(ta + p\\bar{a})}{\\sqrt{t\\bar{t} c\\bar{c}}  } \\\\\n&= a\\frac{\\sqrt{t\\bar{t}}}{\\sqrt{c\\bar{c}}} \\\\\n&= a\\frac{\\sigma_T}{\\sigma_C}.\n\\end{aligned}\n\\] Where \\(\\sigma\\) is the standard deviation (square root of variance). The relationship in ?@eq-cor-tc can also be seen as \\(a = \\text{Cor}(T, C) \\frac{\\sigma_C}{\\sigma_T}\\), which means \\(a\\) can be interpreted as the slope of the regression line \\(C = \\beta_0 + \\beta_1T + \\varepsilon\\), i.e. \\(a = \\beta_1\\). In the proficient rater case \\(p = t\\), \\(\\sigma_C = \\sigma_T\\) and so \\(\\text{Cor}(T, C) = a\\). It can also be shown that for a \\(t\\)-\\(a_1,a_0\\)-\\(p\\) model, the \\(t=p\\) assumption leads to \\(a = \\sqrt{a_1a_0}.\\) See @eubankscause.\nThe two correlations derived here are related by \\(\\text{Cor}^2(T, C) = \\text{Cor}(C_i, C_j)\\)."
  },
  {
    "objectID": "causality.html",
    "href": "causality.html",
    "title": "Cauality",
    "section": "",
    "text": "As a bonus, the confusion matrix can be split into the sum of two matrices to separate the causal part of the classification process, where true classes are identified accurately, from the random classifications, which will sometimes reach the right class assignments, but for the wrong reasons (without justification).\n\\[ a\\begin{bmatrix} t & 0 \\\\ 0 & \\bar{t} \\end{bmatrix} +  \\bar{a} \\begin{bmatrix} tp & \\bar{t}p \\\\ t\\bar{p} & \\bar{t}\\bar{p} \\end{bmatrix} \\]\nin the usual confusion matrix, we know that the off-diagonal cells are inaccurate, but we don’t know what fraction of the diagonal elements are. If the \\(t-a-p\\) model fits the data, we can estimate\nExperiments assume that each experimental unit is identical, and we’re causing an effect rather than classifying a case. However, if we think of the treatment as a classification algorithm, then each subject is unique and we don’t have the opportunity to treat multiple times, so no within-subject comparisons. If a treatment can be repeated, then we can test this way, and get at t and a.\nSection on link to causality.\nsensivity = TPR, specificity = TNR"
  },
  {
    "objectID": "hierarchical.html",
    "href": "hierarchical.html",
    "title": "Chapter 5: Hierarchical Models",
    "section": "",
    "text": "In Chapter 4: The Kappa Paradox we saw how adding parameters can increase the explanatory power of t-a-p models. This is an old idea. The Cohen kappa (Cohen, 1960) dates from the 1960s, and for large data sets can be seen as a variation of the Fleiss kappa, but where each rater has a parameter for random assigments. In the original paper, the formula is derived for two rates, but it can be expanded to any number of them. The application of that idea to the three parameter t-a-p model is to create a \\(p_j\\) parameter for each rater \\(j\\). However, this involves complications, and is treated toward the end of this chapter.\nThe expansion of model parameters maximally includes coefficients for each rater and subject. For example, we could assign a truth parameter \\(t_i\\) to each subject \\(i\\), and accuracy and random assignment parameters \\(a_j\\) and \\(p_j\\), respectively, to each rater \\(j\\). The latter requires that we have IDs for the raters. Generally, regression models like this are called hierarchical models or random effects models or fixed effects models, depending on the research tradition (As noted in Agresti (2003), p. 523 and Gelman & Hill (2006), p. 245, fixed versus random effects are not good descriptions for Bayesian models).\nThe three parameter t-a-p model is already hierarchical in the sense that the ratings for each subject are clustered for analysis. That’s how we create a distribution of agreement to compare with a binomial mixture. The hierarchical model we’re discussing in this chapter is a further extension of that idea, where we add parameters to the model, generally for one of two reasons. One reason is to increase model fit by accounting for variation in the data that is not explained by the three average parameters. The other reason is to obtain estimates for individual subjects or raters.\nIndividual rater parameters and subject truth values are of interest in many contexts. In educational psychology, we might want to know how well a teacher is able to assess student learning, and how well the students demonstrate knowledge or skill. In medical research, we might want to know how well a doctor is able to diagnose a disease, and how well the patients are responding to treatment. In machine learning, we may be concerned with the quality of training data drawn from human classifications. The ratings of consumer products on sites like Amazon or Yelp can be polluted with unfaithful reviews, so some means of detection is valuable. Likewise, individual subject truth parameters are valuable if we want to make decisions with the rating data, for example in a medical diagnosis, a pass/fail assessment of student learning, or using product ratings to make a selection."
  },
  {
    "objectID": "hierarchical.html#threshold-functions",
    "href": "hierarchical.html#threshold-functions",
    "title": "Chapter 4: Hierarchical Models",
    "section": "",
    "text": "One way to retreat slightly from discrete truth values is to borrow an idea from the latent scale approach used in log-odds methods, and parameterize \\(t_i\\) as \\(t_i = \\tau(s_i)\\), where \\(s_i \\epsilon \\mathbb{R}\\) is a latent variable representing characteristics of the subject being observed and categorized, and \\(\\tau\\) is a thresholding function that very nearly maps \\(x\\) to \\(\\{0,1\\}\\) in a continous manner. A reasonable choice for \\(\\tau\\) is a sigmoid like the logistic function, \\(\\tau(s) = \\frac{1}{1+e^{-s}}\\), which has the property that \\(\\tau(s) \\rightarrow 0\\) as \\(s \\rightarrow -\\infty\\) and \\(\\tau(s) \\rightarrow 1\\) as \\(s \\rightarrow \\infty\\). One advantage of this approach is that we can build full regression models for the parameters, e.g. to incorporate subject-specific or rater-specific information as explanatory variables. In simulations, this approach can work, but the sigmoid operation significantly slows down convergence of the MCMC sampler.\nA second approach, which deserves more analysis, is to penalize the likelihood function with a term proportional to \\(\\sum{t_i(1-t_i)}\\). This is a crude way to encourage the sampler to find values of \\(t_i\\) that are close to 0 or 1, but not exactly 0 or 1. This also slows down computation significantly.\nThe third, most practical solution for simple indexing, where explanatory variables are not needed, is to first estimate the three-parameter \\(t\\)-\\(a\\)-\\(p\\) model and then use those estimates to recenter the indexed parameters of interest. In simulations, this worked very will in a fully-indexed \\(t_i\\)-\\(a_j\\)-\\(p_j\\) model, where \\(i\\) indexes subjects being classified and \\(j\\) indexes raters. But it is not clear that it will work in all cases.\nThe likelihood function for \\(t_i\\)-\\(a_j\\)-\\(p_j\\) can be expressed as the product of independent ratings, as\n\\[\n\\begin{aligned}\n\\text{L}(a_j,t_i,p_j;c_{ij}) & = \\prod_{ij} \\left[ c_{ij}(\\tau(s_i)a_j+\\bar{a}_jp_j) + \\bar{c_{ij}}(1 -\\tau(s_i) a_j -\\bar{a}_jp_j) \\right]\\\\\n\\end{aligned}\n\\]\nwhere \\(t_i \\epsilon(0,1)\\) is the estimated probability that the \\(i\\)th case is Class 1, \\(a_j\\) is the estimated accuracy of the \\(j\\)th rater, and constant \\(c\\) is the known fraction of \\(\\overline{C}\\) ratings in the whole data set. The prior distributions of the parameters \\((\\t_i = \\tau(s_i),a_j,p_j)\\) are assumed to be uniform on \\((0,1)\\).\nTo interpret induced parameters in the context of the model in figure \\(\\ref{fig:tree}\\), a slight modification to the likelihood function is useful. The generative model in figure \\(\\ref{fig:tree}\\) is conditional on the true class being binary, not a probability between zero and one. This departure from the model is necessary computationally, so that the truth probability of each subject can smoothly evolve to maximize likelihood, but in practice, we’d like the values of \\(t_i\\) to be close to zero or one when computing likelihoods. This is accomplished using a soft threshold on \\(t_i\\) with a sigmoid \\(\\text{sig}_d(t) := 1/(1 + e^{-d(t-.5)})\\), where \\(d\\) is a discrimination parameter that adjusts steepness. The final likelihood function is\n\\[\n\\text{L}(a_j,t_i;c_{ij},P) = (-1)^{c_{ij}'}|a_j(\\text{sig}_d(t_i) -P)| + c_{ij}P + c_{ij}'P'. \\tag{3} \\label{eq:re_model}\n\\]\nInference of the \\(N + R\\) parameters maximizes likelihood using \\(\\eqref{eq:re_model}\\) assuming that each rating is independent of the others so that total probability is the product of the individual rating probabilities.\nTo test the model \\(N=1000\\) subjects with \\(R=5\\) raters each were simulated with a Class 1 rate of \\(p=.20\\), and with rater accuracies of .1, .3, .5, .7, and .9. The sigmoid discrimination parameter was set to \\(d=30\\). The model was specified in the Stan programming language, with four chains of 1000 iterations each, the first 200 being discarded as warm-up samples to control auto-correlation.\n\n\nCode\n# see code/plain model detail sim.R for details\n\nsim_summary_a &lt;- read_rds(\"data/sim_summary.rds\")\n\nsim_summary_a |&gt; \n  ggplot(aes(x = as.factor(n_raters), y = mean_error,\n             ymin = mean_error - SE*2, \n             ymax = mean_error + SE*2,\n             group = `Number of subjects`,\n             linetype = `Number of subjects`)) +\n  geom_point(size = 2, position = position_dodge(.3)) +\n  geom_errorbar(position = position_dodge(.3),\n                width = .1) +\n  theme_bw() +\n  xlab(\"Number of Raters\") +\n  ylab(\"Error in Accuracy\")\n\n\n\n\n\n\n\n\nFigure 1: Estimates of individual rater accuracy with uniformly distributed t-a-p parameters indexed by rater and subject, with bars encompassing two standard deviations from the mean. Each data point represents 200 simulations, using the three parameter t-a-p model to adjust individual accuracy estimates.\n\n\n\n\n\n\n\nCode\n# see code/plain model detail sim.R for details\n\nsim_summary_t &lt;- read_rds(\"data/sim_summary_t.rds\")\n\nsim_summary_t |&gt; \n  ggplot(aes(x = as.factor(n_raters), y = mean_error,\n             ymin = mean_error - SE*2, \n             ymax = mean_error + SE*2,\n             group = `Number of subjects`,\n             linetype = `Number of subjects`)) +\n  geom_point(size = 2, position = position_dodge(.3)) +\n  geom_errorbar(position = position_dodge(.3),\n                width = .3) +\n  theme_bw() +\n  xlab(\"Number of Raters\") +\n  ylab(\"Error in Classification\")\n\n\n\n\n\n\n\n\nFigure 2: Estimates of individual subject classification accuracy with uniformly distributed t-a-p parameters indexed by rater and subject, with bars encompassing two standard deviations from the mean. Each data point represents 200 simulations, using the three parameter t-a-p model to adjust individual accuracy estimates.\n\n\n\n\n\nFor a simulated data set of 5000 ratings analyzed in figure \\(\\ref{fig:sim_truth}\\), the cases are separated quite well even with only five ratings each. The estimated class probabilities \\(t_i\\) on the horizontal axis take into account estimated rater accuracy, which is imputed for each rater simultaneously. The distributions of posterior samples from the Stan output shows that the parameter estimates recapture individual rater accuracy from the simulated ratings. This does not happen without the soft threshold function to nudge the \\(t_i\\)s toward zero or one. For smaller data sets, the estimates become noisier. The estimates in figure \\(\\ref{fig:sim_truth2}\\) have only 100 subjects with three raters each, with randomly chosen accuracies. It shows much less certainty about the estimates.\nIf the model fits the data, the examples show it is possible to recover latent true class probabilities and individual rater accuracies, but a fairly large number of ratings are required to obtain small error bounds.\nIn addition to general models like \\(t\\)-\\(a\\)-\\(p\\), and the hierarchical model illustrated in this section, mixed effects models are straightforward to construct. For example, a \\(t\\)-\\(a_i\\)-\\(p\\) model can take individual rater accuracies into account when estimating \\(t\\) and \\(p\\). This will shrink the accuracy parameters toward their mean, so they are no longer usable as estimates for individual raters."
  },
  {
    "objectID": "package.html",
    "href": "package.html",
    "title": "The TapModel Package",
    "section": "",
    "text": "1 Introduction\nThis is a user guide to the tapModel package, which you can find on github. The guide will show you how to format a data set of ratings and fit a t-a-p model to the data.\nThere are data sets included in the package to use as examples. We’ll use the wine data set. One the tapModel library is loaded, you can access the data set with data(wine).\n\n\nCode\nlibrary(tidyverse)\nlibrary(tapModel)\nlibrary(knitr)\n\n#' Load the included wine data\ndata(wine)\n\n#' It has rating in columns, one for each wine judge. Each row is a wine.\n#' We can convert to a standard long format with the `format_data` function.\nratings &lt;- tapModel::format_data(wine, data_type = \"raters\")\n\n#' To compute the t-a-p model, we need the counts for each class. This is a \n#' dataframe with one row per subjec, with two columns:\n#'        N_r, the number of ratings for that subject\n#'        N_c, the number of ratings in class 1 for that subject\n#'        \n#' If we define Class 1 to be \"acceptable\" wine, with ratings {2,3,4} vs \n#' Class 0 = rating 1, we can find the counts with\ncounts &lt;- tapModel::count_ratings(ratings, c(2,3,4))\n\n#' Compute the solution\ntap &lt;- tapModel::iterative_optim(counts)\n\n#' Show the results\nkable(tap, digits = 2)\n\n\n\n\n\nt\na\np\nll\ndegenerate\n\n\n\n\n0.73\n0.54\n0.78\n-243.36\nFALSE\n\n\n\n\n\nYou can also format the data using tidyverse functions without the helpers.\n\n\nCode\ninclass &lt;- 2:4 # set the range of acceptable ratings\n\n# this will work for the data format where each rater has a column\ncounts &lt;- wine %&gt;%\n  mutate(N_r = rowSums(across(everything(), ~ !is.na(.))),   \n         N_c = rowSums(across(everything(), ~ . %in% inclass))) |&gt; \n  select(N_r, N_c)\n          \ncounts |&gt; \n  head(5) |&gt; \n  kable()\n\n\n\n\n\nN_r\nN_c\n\n\n\n\n4\n5\n\n\n4\n5\n\n\n4\n5\n\n\n4\n4\n\n\n4\n3\n\n\n\n\n\n\n\n2 Ordinal Analysis\nIn addition to a binary comparison, where we split the range of possible ratings into two sets, we can also use the ordinal analysis. This is where we put the ratings in a sensible order, like “strongly disagree” to “strongly agree” for a common survey response, then consider each cut-point between these values. In the case of the wine ratings, we have:\n\nWine rating scale\n\n\nRating Value\nMeaning\n\n\n\n\n1\nNo medal\n\n\n2\nBronze medal\n\n\n3\nSilver medal\n\n\n4\nGold medal\n\n\n\nThere are three natural cut-points:\n\n1|2 divides the 1 ratings from higher ones\n2|3 divides the {1,2} ratings from the {3,4} ratings\n3|4 divides the {1,2,3} ratings from the 4 ratings.\n\nEach cut point defines a binary classification system, which we can estimate the t-a-p parameters for.\nTo use the built-in function for iterating over the cut-points, we need to prepare the data to be in a long format with only two columns SubjectID__ and rating.\n\n\nCode\n#' this will work for the data format where each rater has a column\nratings &lt;- wine %&gt;%\n  mutate(SubjectID__ = row_number()) |&gt; \n  gather(judge, rating, -SubjectID__) |&gt; \n  select(SubjectID__, rating)\n\nratings |&gt; \n  head(5) |&gt; \n  kable()\n\n\n\n\n\nSubjectID__\nrating\n\n\n\n\n1\n3\n\n\n2\n3\n\n\n3\n3\n\n\n4\n3\n\n\n5\n4\n\n\n\n\n\nNow we can apply the function to find estimates for the parameters at each cut-point as well as the Fleiss kappa statistics for each.\n\n\nCode\nparams &lt;- tapModel::get_ordinal_tap(ratings)\n\nkable(params, digits = 2)\n\n\n\n\n\nt\na\np\nll\ndegenerate\nCutPoint\ntype\n\n\n\n\n0.27\n0.54\n0.22\n-243.36\nFALSE\n1|2\nt-a-p\n\n\n0.24\n0.57\n0.24\n-243.47\nFALSE\n1|2\nFleiss\n\n\n0.54\n0.44\n0.59\n-286.12\nFALSE\n2|3\nt-a-p\n\n\n0.57\n0.44\n0.57\n-286.14\nFALSE\n2|3\nFleiss\n\n\n0.65\n0.25\n0.95\n-169.94\nFALSE\n3|4\nt-a-p\n\n\n0.88\n0.35\n0.88\n-170.82\nFALSE\n3|4\nFleiss\n\n\n\n\n\nHere’s how to plot the results.\n\n\nCode\nparams |&gt; \n  select(t,a,p,CutPoint, type) |&gt; \n  gather(param, value, t, a, p) |&gt; \n  ggplot(aes(x = CutPoint, y = value, color = type, group = type)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(. ~ param)\n\n\n\n\n\n\n\n\n\n\n\n3 Bayesian Models\nBayesian estimation using Monte Carlo Markov Chain (MCMC) estimation methods are flexible in allowing for maximum likelihood estimation over any likelihood function you can imagine. This includes the standard t-a–p three-parameter model, but also includes hierarchical models where each subject is assigned a truth value and each rater is assigned an accuracy estimate. The tapModel package provides some limited interface to the most common of these models using library(cmdstanr). Installing that package entails some setup work. Please see the package documentation to install and test your installation before trying out the code below.\nThe use of Stan for Bayesian estimation is also included in the package, with an easy interface for three different models:\n\nt-a-p.stan, a three-parameter t-a-p model\nt-a0a1-p.stan, a four-parameter model that splits the accuracy parameter into \\(a0\\) and \\(a1\\), so that raters can have different accuracies depending on the true classification, and\nt-a0a1-p0p1.stan, a five-parameter version that expands the four-parameter version to also include \\(p0\\) and \\(p1\\), so the guessing probabilities are also conditional on the true classification.\n\nWe can use the counts data frame from the introduction to illustrate.\n\n\nCode\nlibrary(cmdstanr)\nlibrary(LaplacesDemon) # for the Modes() function\n\n#' Get the .stan source code for the chosen model\nstan_model_source &lt;- system.file(\"stan/t-a-p.stan\", package = \"tapModel\")\n\n#' Compile it. This can take a while, but you only need to do it once.\n#' maybe go make a cup of tea\nstan_model &lt;- cmdstanr::cmdstan_model(stan_model_source)\n\n#' Use the convenience function to run the MCMC\n#' This requires counts to be properly formated. \nmodel_draws &lt;- tapModel::fit_tap_model(counts, stan_model) \n\n#' Summarize the statistics\nest_params &lt;- tapModel::get_tap_stats(model_draws)\n\n#' Simplify the draw data into a data fram and then\n#' plot the draw distributions\nmodel_draws |&gt; \n  tapModel::extract_vars() |&gt; \n  tapModel::plot_draw_densities()\n\n\nThe get_tap_stats function returns a data frame with the mean and interquartile range of the draws, as well as an attempt to identify bimodal densities, which can indicate a problem with model fit or non-unique solutions.\n\n\n\n\n\nposition\nvar\navg\np05\np25\nmedian\np75\np95\nmode1\nmode2\n\n\n\n\n2\na\n0.54\n0.45\n0.50\n0.54\n0.58\n0.64\n0.53\n0.55\n\n\n3\np\n0.77\n0.63\n0.73\n0.78\n0.82\n0.88\n0.80\nNA\n\n\n4\nt\n0.73\n0.62\n0.69\n0.73\n0.78\n0.83\n0.73\nNA\n\n\n\n\n\nThe plot_draw_densities function will plot the draws for each parameter, with the mean labeled and the interquartile range shaded. The average log likelihood is indicated with a dotted line.\n\n\n\nParameter estimates for t-a-p on wine data with mean labeled and interquartile range shaded. Average log likelihood is indicated with dotted line."
  },
  {
    "objectID": "app.html",
    "href": "app.html",
    "title": "Interactive t-a-p Analysis",
    "section": "",
    "text": "The R programming language has been extended to include the creation of interactive apps using a framework called Shiny. You’ll need to install the libray(shiny) to run it, with install.packages(\"shiny\").\nTwo separate interactive applications are available for the t-a-p model. One of them is included in the tapModel R package. Once the package is installed, you can open the app with tapModel::launchApp(). This is a limited version of the app that does not provide for Bayesian analysis. The full version of the app can be installed as a stand-alone R project from github. There are instructions there on installing it. To take advantage of the Bayesian estimation methods, you’ll need to install additional R packages and tools. The Bayesian estimation is done with a Markov chain Monte Carlo (MCMC) process of sampling from a probability distribution. For that, the Stan programming language is used, with the stancmdr package. That link has installation instructions. The library LaplacesDemon is also needed to compute the modes of distributions.\n\n\nThe simplest way to get started is with the tapModel library. It provides these functions:\n\nLoad data from comma-delimited value (CSV) files that are formatted as\n\nOutcome ratings in columns, where multiple traits have rating data, one subject per row, and multiple traits per subject. For example if a jury of reviewers rates a musical performance on style, technique, and musicality, each performer would have multiple rows, each with those three columns.\nRaters in columns, where the same type of jury data can be reformatted to have a Category column (style, technique, musicality), and each rater appears as a single column. In this format each subject only appears on a single row.\nLong format, where the subject ID, optional category, and rating appear as the three columns of the CSV.\n\nSimulate a data set by specifying the t-a-p parameters and sample sizes.\nEstimate t-a-p parameters from a (simulated or real) data set.\nEstimate ordinal t-a-p parameters from a data set. This assumes that the rating scale is sorted alphabetically in the correct order. For example, a numerical survey response scale is usually in the right order, but the labels may not be (“neutral” doesn’t sort in the middle of “strongly agree” and “strongly disagree”). You may need to adjust the rating labels accordingly, e.g. “1 - strongly disagree”, … “5 - strongly agree”.\n\n\n\n\nIf you launch the app from the github source code and have the stancmdr package installed, some additional features become available. These derive from using Bayesian modeling written in the Stan programming language to make parameter estimates from maximum likelihood models built to reflect variations of the t-a-p model. Markov chain Monte Carlo (MCMC) methods are used to explore the model’s probability distribution, which you can then see within the app. This is advantageous because a parameter estimate may have a bimodal distribution when parameters are not cleanly identifiable. In those cases relying on an average for a parameter estimate is a mistake.\nAnother advantage of MCMC’s numerical simulation is that the basic t-a-p model can be extended to include more parameters without being rigidly tied to the binomial mixture model. Finally, the simple methods used for three-parameter estimation will fail when confronted with many parameters. In the interactive application, there is limited ability to add parameters. For complete flexibility, Stan scripts are provided in the chapter on hierarchical models.\nThe following sections describe the functionality of both the package app and the stand-alone version, with the latter’s additional features marked as “advanced.”"
  },
  {
    "objectID": "kappa.html",
    "href": "kappa.html",
    "title": "Chapter 3: Kappa Statistics",
    "section": "",
    "text": "The formula for chance-corrected measure of agreement (generically a “kappa”) compares observed match rates to the expectation of random match rates. The kappas vary in how they estimate the random match rates. For two ratings to match, two raters \\(j,k\\) of the same subject \\(i\\) must agree in their assignment of either Class 1 or Class 0 classifications. In other words, the binary random variables must agree: \\(C_{ij} = C_{ik}\\). A generic formula that includes the most common kappas is\n\\[\n\\kappa = \\frac{m_o - m_c}{1 - m_c},\n\\]where \\(m_o\\) is the observed proportion of agreements and \\(m_c\\) is the expected proportion of agreements under chance. The assumption about \\(m_c\\) is a defining feature of the various kappa statistics. The most general treatment of such statistics is the Krippendorff alpha (Krippendorff, 2018, pp. 221–250)\nThe various kappas differ in the assumption made about the chance correction probability \\(m_c\\). Commonly, the assumption is that \\(m_c = x^2 + \\bar{x}^2\\) for some probability \\(x\\). This simple formulation makes sense when both raters are guessing, but the actual case is more complicated because a match “by chance” could be a case where one rating was accurate and the other was a guess. This distinction isn’t generally made in the derivations of the kappas, although the AC1 paper discusses the issue, and hints at a full three-parameter model. It’s ironic that the confusion about kappas is disagreement about the probability of agreement by chance.\nThe Fleiss kappa (Fleiss, 1971) uses the fraction of Class 1 ratings \\(c\\) to create \\(m_c = c^2 + \\bar{c}^2\\). The S statistic (Bennett et al., 1954), also called the Guilfords’ G or G-index1 (Holley & Guilford, 1964), is a kappa that assumes \\(m_c = 1/2\\) when there are two categories. The AC1 kappa has a different form, assuming that \\(m_c = 2c\\bar{c}\\) (Gwet, 2008). The Cohen kappa is a variation where each rater gets a guessing distribution, so \\(m_c = x_1x_2 + \\bar{x_1}\\bar{x_2}\\) (Cohen, 1960). Because of the extra parameter, discussion of Cohen’s kappa is found in the chapter on hierarchical models rather than being included here.\nConsider two raters classifying an observation. In the t-a-p model we can express the expected value of observed matches \\(m_o\\) as the sum of three kinds of agreement: (1) \\(m_a\\) is when both raters are accurate (and hence agree), (2) \\(m_i\\) when both raters are inaccurate (guessing) and agree, and (3) \\(m_x\\) is the mixed case when one rater is accurate and the other is inaccurate but they agree. The second two of these have expressions that include the guessing rate \\(m_c\\). Following that thinking we have the following expectations for rates:\n\\[\n\\begin{aligned}\nm_a &= a^2 & \\text{(both accurate)}\\\\\nm_r &= p^2 + \\bar{p}^2 & \\text{(random ratings)}\\\\\nm_i &= \\bar{a}^2m_r = a^2m_r - 2am_r + m_r &\\text{(both inaccurate)}\\\\\nm_x &= 2a\\bar{a}(tp + \\bar{t}\\bar{p}) &\\text{(mixed accurate and inaccurate)}\\\\\nm_o &= m_a + m_i + m_x &\\text{(observed match rate)}\\\\\n    &= a^2+a^2m_r + m_r - 2am_r + 2a\\bar{a}(tp + \\bar{t}\\bar{p})\\\\\n\\end{aligned}\n\\tag{1}\\]\nFor \\(m_a\\), both ratings must be accurate, in which case they automatically agree. For \\(m_i\\), both must be inaccurate (probability \\(\\bar{a}^2\\)) and then match randomly (probability \\(m_r\\)). For \\(m_x\\), one rater must be accurate and the other inaccurate, in which case they agree if the accurate rater chooses the category that the inaccurate rater guesses. The various kappa derivations usually ignore these mixed matches in favor of using \\(m_r\\) as the chance match rate, which we called \\(m_c\\) in the kappa formula. This amounts to choosing \\(p\\) since \\(m_r = p^2 + \\bar{p}^2\\).\nThe various match rates in Equation 1 create a vocabulary for understanding some of the kappa statistics. The easiest one to analyze is the S-statistic (it is sometimes called the G-index)."
  },
  {
    "objectID": "kappa.html#introduction",
    "href": "kappa.html#introduction",
    "title": "Chapter 3: Kappa Statistics",
    "section": "",
    "text": "One way to make the model identifiable is to put a constraint on one of the parameters. For example, we can constrain the guessing parameter \\(p\\) to be equal to the truth parameter \\(t\\). This is the approach taken by the Fleiss kappa (Fleiss, 1971), as we’ll see below.\nThe formula for chance-corrected measure of agreement (generically a “kappa”) compares observed match rates to the expectation of random match rates. The kappas vary in how they estimate the random match rates. For two ratings to match, two raters \\(j,k\\) of the same subject \\(i\\) must agree in their assignment of either Class 1 or Class 0 classifications. In other words, the binary random variables must agree: \\(C_{ij} = C_{ik}\\). A generic formula that includes the most common kappas is\n\\[\n\\kappa = \\frac{m_o - m_c}{1 - m_c},\n\\] where \\(m_o\\) is the observed proportion of agreements and \\(m_c\\) is the expected proportion of agreements under chance. The most general treatment of such statistics is the Krippendorff alpha (Krippendorff, 2018, pp. 221–250)\nThe various kappas differ in the assumption made about the chance correction probabiliy \\(m_c\\). Commonly, the assumption is that \\(m_c = x^2 + \\bar{x}^2\\) for some probability \\(x\\). This simple formulation makes sense when both raters are guessing, but the actual case is more complicated because a match “by chance” could be a case where one rating was accurate and the other was a guess. This distinction isn’t generally made in the derivations of the kappas, although the AC1 paper discusses the issue, and hints at a full three-parameter model. It’s ironic that the confusion about kappas is disagreement about the probability of agreement by chance.\nThe Fleiss kappa assumes that \\(p = c\\), the observed fraction of class 1 ratings (\\(\\hat{c} = E[C]\\)), so that \\(m_c = c^2 + \\bar{c}^2\\). The \\(S\\) statistic (also called the \\(G\\)-index) is a kappa formulation that assumes \\(m_c = 1/2\\). The AC1 kappa has a different form, assuming that \\(m_c = 2c\\bar{c}\\). The Cohen kappa is a variation where each rater gets a guessing distribution, so \\(m_c = x_1x_2 + \\bar{x_1}\\bar{x_2}\\).\nConsider two raters classifying an observation. In the \\(t-a-p\\) model we can express the expected value of observed matches \\(m_o\\) as the sum of three kinds of agreement: (1) \\(m_a\\) is when both raters are accurate (and hence agree), (2) \\(m_i\\) when both raters are inaccurate (guessing) and agree, and (3) \\(m_x\\) is the mixed case when one rater is accurate and the other is inaccurate but they agree. The second two of these have expressions that include the guessing rate \\(m_c\\). Following that thinking we have the following expectations for rates:\n\\[\n\\begin{aligned}\nm_a &= a^2 & \\text{(both accurate)}\\\\\nm_r &= p^2 + \\bar{p}^2 & \\text{(random ratings)}\\\\\nm_i &= \\bar{a}^2m_r = a^2m_r - 2am_r + m_r &\\text{(both inaccurate)}\\\\\nm_x &= 2a\\bar{a}(tp + \\bar{t}\\bar{p}) &\\text{(mixed accurate and inaccurate)}\\\\\nm_o &= m_a + m_i + m_x &\\text{(observed match rate)}\\\\\n    &= a^2+a^2m_r + m_r - 2am_r + 2a\\bar{a}(tp + \\bar{t}\\bar{p})\\\\\n\\end{aligned}\n\\tag{1}\\]\nFor \\(m_a\\), both ratings must be accurate, in which case they automatically agree. For \\(m_i\\), both must be inaccurate (\\(\\bar{a}^2\\)) and then match randomly (\\(m_r\\)). For \\(m_x\\), one rater must be accurate and the other inaccurate, in which case they agree if the accurate rater chooses the category that the inaccurate rater guesses.\nFollowing the intuition in the introduction that kappa is the square of what we are interested in, we can attempt to choose \\(m_c\\) so that \\(\\kappa = a^2\\) via \\(m_0 - m_c = a^2(1-m_c)\\). This leads to\n\\[\n\\begin{aligned}\nm_c^* &= \\frac{m_i + m_x}{1 - a^2} \\\\\n&= \\frac{\\bar{a}^2m_r + 2a\\bar{a}(tp + \\bar{t}\\bar{p})}{(1+a)\\bar{a}} \\\\\n&= \\frac{\\bar{a}(p^2 + \\bar{p}^2)  + 2a(tp + \\bar{t}\\bar{p})}{1+a} \\\\\n\\end{aligned}\n\\tag{2}\\]\nwhere the asterisk denotes the choice of the chance correction formula \\(m_c\\) that always makes \\(\\kappa = a^2\\). In unsimplified form, the numerator of the equation above is the expected proportion of matches where there is at least one inaccurate rating, and the denominator is the the rate of non-perfect rating pairs, where at least one of the raters is inaccurate, and they may or may not match. The chance correction is therefore accounting for the accurate ratings by taking them out of the data altogether and then calculating inaccurate matches out of all rating pairs as the probability of by-chance matching.\nThe formulation of \\(m_c^*\\) isn’t directly helpful for computing a kappa because the whole point is to discover \\(a\\), so a formula for \\(m_c\\) that requires knowing \\(a\\) is circular. However, some special cases are of interest. For values of \\(a\\) close to zero, we have \\(m_c^* \\approx p^2 + \\bar{p}^2\\); when most ratings are random, the guessing parameter \\(p\\) is most important. When \\(a\\) is close to one, \\(m_c^* \\approx tp + \\bar{t}\\bar{p}\\); when most ratings are accurate, the inaccurate matches are likely to have only one inaccurate rating.\nIf \\(m_c^2\\) is averaged over a uniform distribution \\(p \\epsilon [0,1]\\), \\(t\\) also drops out so that the chance correction only depends on \\(a\\), the proportion of accurate ratings.\n\\[\n\\int_0^1 \\frac{\\bar{a}(p^2 + \\bar{p}^2)  + 2a(tp + \\bar{t}\\bar{p})}{1+a} dp = \\frac{a+2}{3+3a}.\n\\]\nIf when raters assign inaccurate ratings, if they at least conform to the distribution of true values, so that if \\(t = p\\), then \\(m_c^* = p^2 + \\bar{p}^2\\). We will call them “unbiased” raters when that happens. Finally, if \\(p = 1/2\\) then \\(m_c^*\\) simplifies to \\(1/2\\) as well. That condition will be referred to respectively as “naive” raters, as described below."
  },
  {
    "objectID": "kappa.html#the-fleiss-kappa-proficient-raters",
    "href": "kappa.html#the-fleiss-kappa-proficient-raters",
    "title": "Chapter 3: Kappa Statistics",
    "section": "2 The Fleiss Kappa: Proficient Raters",
    "text": "2 The Fleiss Kappa: Proficient Raters\n[Note: in the asymptotic likelihood app, it seems that in addition to t = p, when p = .5 we have an accurate estimate of a as well–No, that’s only if we use p = .5, not c = .5.]\nThe Fleiss kappa is designed to work over multiple raters and a rating scale of arbitrary length. It assumes an asymptotic form for chance correction, so is most appropriate for large samples. We only consider the binary scale case here. Extension to larger scales is a topic in the discussion section at the end of this essay.\nThe baseline for random ratings for Fleiss is if we took all the observations and randomly shuffled them between subjects. This worse case has accuracy equal to zero, and the rating process devolves to\n\\[\n\\begin{aligned}\nC_{ij} &= T_iA_{ij} + \\bar{A_{ij}}P_{ij} \\\\\n&= P_{ij}\n\\end{aligned}\n\\]\nThis is a simple binomial distribution with probability \\(p\\) , not a mixture of them, and \\(p = \\hat{c}\\) (the observed average fraction of Class 1 ratings) will have maximum likelihood over binomial models. In this case, we have no information about \\(t\\), and any value will do in the model, since it has no effect. The Fleiss kappa uses this random baseline and takes it one step further by assuming that the fraction of true Class 1 cases is the same as the reported fraction, as well, i.e. \\(t = \\hat{c}=p\\). The Fleiss kappa can therefore give poor estimates for \\(a\\) when the raters get the proportion of Class 1 cases wrong.\nFor example, consider a test for a medical condition that has a prevalence of 10%, but the test is in fact returning random results that are half positive and half negative. Then the kappa’s baseline for comparison is wrong, and the resulting estimate for accuracy will be biased.\nA review of the properties of Fleiss kappa can be found in , chapter 18, including kappa’s equivalence to an intraclass correlation coefficient, defined as ICC(1,1) in . In our notation here, the Fleiss kappa is defined as taking \\(m_c = \\hat{c}^2 + \\hat{\\bar{c}}^2\\), where \\(\\hat{c}\\) is the observed proportion of class 1 ratings. The Fleiss kappa is therefore a \\(\\hat{c}\\)-\\(a\\)-\\(\\hat{c}\\) model. The functional form \\(f(x) = x + x^2\\) for \\(x \\epsilon [0,1]\\) is parabolic, with a minimum at \\(x = 1/2\\), where \\(f(x) = 1/2\\) and maxima at \\(x = 0\\) and \\(x = 1\\), when \\(f(x) = 1\\). Therefore, the chance correction term \\(m_c\\) of the Fleiss kappa is opinionated, favoring larger rates of rater error. We would expect that the rater agreement statistic will tend to underestimate rater accuracy, taken to be \\(\\hat{a} = \\sqrt{\\kappa}\\).\nAs noted, the chance correction for Fleiss depends on the observed proportion of Class 1 ratings, \\(\\hat{c}\\). The expected value under the tap model assumptions is \\(E[\\hat{c}] = c = ta + p\\bar{a}\\); ratings of Class 1 result from either true and accurate ratings or from inaccurate ones that happen to guess Class 1.\nWhen \\(p = t\\), the expectation of ratings simplifies to \\(c = pa + p\\bar{a} = p\\). Intuitively, this case causes rater accuracy \\(a\\) to wash out of the observed rate of Class 1 ratings \\(\\hat{c}\\), so that the chance correction can be estimated directly. In the discussion of Equation 1, we saw that when \\(t = p\\), \\(m_c^* = p^2 + \\bar{p}^2\\). Therefore, the Fleiss kappa is equivalent to the chance correction that makes \\(E[\\kappa] = a^2\\) when \\(t = p\\). Under this assumption, when raters make inaccurate ratings, their ratings at least conform to the distribution of true values \\(t\\), and more generally the fraction \\(c\\) of Class 1 ratings is the true fraction. I’ll refer to that case “proficient” raters, since on average their work represents the true distribution of the data, which in the notation here is \\(t = c\\).\nUnder this \\(t = c\\) “proficiency” condition, rater accuracy \\(a\\) is the correlation between the raters’ ratings and the true classifications, \\(\\sqrt{E[\\kappa]} = a = cor(C, T)\\). Additionally, the Fleiss kappa is the intraclass correlation of the ratings. Derivations of these results are found in the appendices, where there is also an alternative derivation of the \\(a = \\sqrt{E[\\kappa}]\\) result.\nEven when \\(t \\ne p\\), when \\(a = 1\\), \\(\\kappa = 1\\), and when \\(a = 0\\), \\(\\kappa = 0\\) or is undefined.\nIn calculations over a grid of possible values for the input parameters in \\((0,1)\\) for each, the Fleiss kappa was rounded up to zero for negative values. When \\(t \\ne p\\), the bias away from \\(a^2\\) was often substantial (the R code to reproduce these results are in the supplementary materials).\n\n\nCode\ndf &lt;- kappa_error(1e6)\n\ndf |&gt; \n  group_by(t, a) |&gt;\n  summarize(error = mean(fleiss_a - a)) |&gt;\n  ggplot(aes(x = t, y = a, z = error)) +\n  geom_contour_filled(breaks = c(-1,-.2, -.1, -.05, 0,.05))  +\n  scale_fill_grey(start = .2, end = .8) +\n  theme_bw() +\n  guides(fill=guide_legend(title=\"Error\"))\n\n\n\n\n\n\n\n\nFigure 1: The Fleiss kappa’s error in the estimate of rater accuracy averaging over expression(p epsilon [0,1]), taken over a dense grid of values in [0,1] for the parameters.\n\n\n\n\n\nThe average of the Fleiss kappa’s estimate of \\(a\\), shown in Figure 1, is pretty good when \\(t\\) is somewhere near .5, across the whole range of true \\(a\\) values. Problems occur for large and small values of \\(t\\). This result doesn’t hold for all values of \\(p\\), just the average. So if we have a large number of raters and imagine that their individual statistics \\(p_j\\) are randomly distributed, there’s some assurance that for reasonable values of \\(t\\), then \\(| \\sqrt{\\kappa_{fleiss}} - a | \\le .05.\\) Recall that under the proficient rater assumption (\\(p = t\\)), the expectation of this kappa is \\(a^2\\).\nAs a soft version of the proficient rater assumption, we can assume that the prior distribution of \\(p\\) is not uniform over \\([0,1]\\), but is represented by a beta distribution \\(\\beta(c_1, c_0)\\).\n[Note: see “decomposing a.rmd” for possible results connecting proficient raters to the decomposed accuracy models]\n[closed form for a exists under Fleiss assumptions, and we have t = p, so there’s a closed form for the likelihood given a data set]"
  },
  {
    "objectID": "kappa.html#s-g-index-naive-raters",
    "href": "kappa.html#s-g-index-naive-raters",
    "title": "Chapter 3: Kappa Statistics",
    "section": "3 S (G-index): Naive Raters",
    "text": "3 S (G-index): Naive Raters\nRecall that the S (G-index) version of kappa assumes that raters “flip coins” when assigning an inaccurate rating, with chance correction \\(m_c = 1/2\\). As noted in the discussion of Equation 1, the choice of \\(p =  1/2\\) corresponds to this choice. Thus, S is a \\(t-a-1/2\\) model. The \\(S\\) statistic will recover \\(a\\) when \\(t = 1/2\\), subject to sampling error, but generally this is not the case. A grid expansion of n uniformly distributed values of the three model parameters \\((t, a, p)\\) was used to calculate the estimation error of rater accuracy, using \\(a = \\sqrt{\\kappa}\\).\n\n\nCode\ndf |&gt; \n  group_by(t, a) |&gt;\n  summarize(error = mean(S_a - a)) |&gt;\n  ggplot(aes(x = t, y = a, z = error)) +\n  geom_contour_filled(breaks = c(0,.05, .1, .2, 1))  +\n  scale_fill_grey(start = .8, end = .2) +\n  theme_bw() +\n  guides(fill=guide_legend(title=\"Error\"))\n\n\n\n\n\n\n\n\nFigure 2: The S statistic’s error in the estimate of rater accuracy averaging over expression(p epsilon [0,1]), taken over a dense grid of values in [0,1] for the parameters.\n\n\n\n\n\nFor small values of \\(a\\), the S statistic’s constant \\(m_c = 1/2\\) underestimates \\(m_c^*\\) and hence inflates kappa. The contours in ?@fig-s-error show that, when averaging over \\(p\\), the S version of kappa badly overestimates \\(\\hat{a}\\) for small values of \\(a\\).When \\(a = 1\\), \\(S = 1\\), and when \\(a = 0\\), S can take any value in [0,1]. For example, when \\((t,a,p) = (0,0,1/2)\\), the S is 0, but when \\((t,a,p) = (1/2,0,1)\\), S is 1.\n[is this model unique in t-a-p like Fleiss is?]"
  },
  {
    "objectID": "kappa.html#ac1",
    "href": "kappa.html#ac1",
    "title": "Chapter 3: Kappa Statistics",
    "section": "4 AC1",
    "text": "4 AC1\nThe AC1 version of kappa developed in uses the same disaggregation of rating agreements found in [equation]. This can be found in Table 4, page 36, where Gwet uses the idea of ratings that are certain (the same as ??’s “for cause”) versus random, so that what I’ve called the accurate match rate \\(m_a = a^2\\) is the sum of Gwet’s counts of accurate matches \\(N_{++.CC} + N_{--.CC}\\) divided by the total number of rating pairs. In terms of the t-a-p model, Gwet assumes that the probability of a by-chance agreement is \\(m_c = m_i + m_x\\), the sum of the cases where both raters make inaccurate ratings and the case where exactly one does. This amounts to assuming \\(a^2\\) is close to zero in \\(m_c^* = (m_i + mx)/(1-a^2)\\). After some approximations, the accidental match rate \\(m_i + mx\\) is assumed to be \\(2c\\bar{c}\\), estimating \\(c\\) from \\(\\hat{c}\\), the observed fraction of \\(C = 1\\). Since \\(1 = (c + \\bar{c})^2 = c^2 + 2c\\bar{c} + \\bar{c}^2\\), the AC1 version of \\(m_c\\) is the complement of the Fleiss version: they sum to one. In effect, matches that Fleiss considers random, AC1 considers non-random, and vice-versa, at least in expectation. One consequence is that \\(AC1 \\ge \\kappa_{fleiss}\\).\nIf the assumptions of AC1 are met,,\n\\[\n\\begin{aligned}\nE[\\kappa] &= \\frac{m_o - m_c}{1 - m_c} \\\\\n&= \\frac{m_a + m_i + m_x - 2c\\bar{c}}{1-2c\\bar{c}} \\\\\n&\\approx \\frac{a^2}{1-2c\\bar{c}} & \\text{(if } m_i + m_x \\approx 2c\\bar{c}\\text{),}\\\\\n\\end{aligned}\n\\tag{2}\\]\nwhich will overestimate \\(a\\), moreso when \\(c \\approx .5\\).\nAs noted in the discussion of the Fleiss kappa, when \\(a\\) is small, the Fleiss chance correction of \\(c^2 + \\bar{c}^2\\) is a good approximation of \\(m_i + m_x\\), which means that the AC1 complement of this, \\(1 - (c^2 + \\bar{c}^2)\\), gives a poor estimate for small values of \\(a\\). Neither version is a good approximation when \\(a\\) is close to one.\n\n\nCode\ndf |&gt; \n  group_by(t, a) |&gt;\n  summarize(error = mean(AC1_a - a)) |&gt;\n  ggplot(aes(x = t, y = a, z = error)) +\n  geom_contour_filled(breaks = c(0,.05, .1, .2, 1)) +\n  scale_fill_grey(start = .8, end = .2) +\n  theme_bw() +\n  guides(fill=guide_legend(title=\"Error\"))\n\n\n\n\n\n\n\n\nFigure 3: The AC1 statistic’s error in the estimate of rater accuracy averaged over a dense grid of values in (0,1) for \\((t,a,p)\\).\n\n\n\n\n\nThe contours in ?@fig-ac1-error shows the median error (solid line), the 25th and 75th percentiles (lighter ribbons), and maximum error (lighter ribbons) for S as a function of rater accuracy. The S version of kappa tends to overestimate rater accuracy when \\(a&lt;.5\\). For example, if \\(t = p = .8\\) with \\(a = .25\\), the AC1 has an expectation of .58, implying an accuracy of .76.\n[is this model unique in t-a-p like Fleiss is?]"
  },
  {
    "objectID": "kappa.html#cohens-kappa",
    "href": "kappa.html#cohens-kappa",
    "title": "Chapter 3: Kappa Statistics",
    "section": "5 Cohen’s Kappa",
    "text": "5 Cohen’s Kappa\nThe original kappa is a variation that exceeds the three-parameter \\(t\\)-\\(a\\)-\\(p\\) model, because each rater gets a custom guessing rate. With two raters, the model becomes \\(t\\)-\\(a\\)-\\(p_1,p_2\\), and the chance match rate is \\(m_c = p_1 p_2 + \\bar{p_1}\\bar{p_2}\\). The kappa is still calculated by comparing the observed match rate \\(m_o\\) and the estimated chance correction, with \\(\\kappa = (m_o - m_c)/(1-m_c)\\).\nIn cases where \\(p_1 = p_2 = t\\), the Cohen kappa is equivalent to the Fleiss kappa with the proficient rater condition, and \\(\\sqrt{E[\\kappa]} = a\\)."
  },
  {
    "objectID": "kappa.html#krippendorffs-alpha",
    "href": "kappa.html#krippendorffs-alpha",
    "title": "Chapter 3: Kappa Statistics",
    "section": "6 Krippendorff’s Alpha",
    "text": "6 Krippendorff’s Alpha\nOn the wiki page, it notes that the metric used is a square of an actual metric, which probably relates to the a^2 thing.\nhttps://en.wikipedia.org/wiki/Krippendorff%27s_alpha\nThere’s a note about the bias in Fleiss. I think the alpha is the small-n version of fleiss with varying numbers of raters. I should code it up."
  },
  {
    "objectID": "kappa.html#conclusions-about-kappa",
    "href": "kappa.html#conclusions-about-kappa",
    "title": "Chapter 3: Kappa Statistics",
    "section": "7 Conclusions about kappa",
    "text": "7 Conclusions about kappa\n\n\nCode\np1 &lt;- df |&gt;  \n  select(t,a,p, Fleiss = fleiss_a, S = S_a, AC1 = AC1_a) |&gt; \n  gather(key = \"kappa\", value = \"value\", -t, -a, -p) |&gt; \n  mutate(Error = sqrt(value) - a) |&gt; \n  select(kappa, Error)\n  \n\np1 |&gt; \n  ggplot(aes(x = Error)) +\n  geom_histogram(binwidth = .1, color = \"white\") +\n  scale_fill_grey() +\n  theme_bw() +\n  facet_wrap(~kappa, nrow = 1) +\n  ylab(\"\")\n\n\n\n\n\n\n\n\nFigure 4: Expected error histograms in estimating rater accuracy, taken over a dense grid of values in [0,1] for the t-a-p parameters$.\n\n\n\n\n\nThe histograms in Figure 4 show the expected error in estimating rater accuracy for selected kappa statistics identified above. In actual use, sampling error will be added to this. The Fleiss kappa has the least error overall. AC1 and S tend to overestimate \\(a\\), and The bias with the Fliess, S, and Cohen kappas come from their assumption that \\(D=0\\) in [equation], which derives from the assumption that the raters are either proficient (Fleiss) or naive (S). Cohen has the additional assumption about individual rater guessing parameters. If those conditions don’t hold in the data (i.e. if the model fit is poor), then rater accuracy will estimated with error depending on how large \\(D\\) actually is. For example, if \\(t = 0.2\\), \\(a = 0.2\\), and \\(p=0\\) (no one ever guesses \\(\\widehat{C_1}\\)), then neither of the kappa assumptions \\(p = 1/2\\) (S) or \\(t = p\\) (Fleiss) are close to true and \\(D = 0.2\\) instead of zero. The S estimate of \\(a\\) is .99 and the Fleiss estimate is .13.\nAlthough the two kappas surveyed can recover rater accuracy when the assumptions are met, they are not robust to large violations of those assumptions. The parameterized rater model presented here is a more general modeling approach to estimating rater accuracy and other parameters of interest.\nUnder the assumptions of the t-a-p model, each of the kappa statistics considered in this section add assumptions. For example, Fleiss adds the assumption \\(p = t\\). This has the effect of reducing the size of the parameter space. For a given data set, it’s then possible to assess the likelihood of each of the kappa choices by maximizing over the respective restricted parameter space. This can reveal which of the assumptions has maximum likelihood, if the desire is to choose between models. However, it’s probably better to not introduce such restrictions unless there’s a good reason to do so."
  },
  {
    "objectID": "kappa.html#the-fleiss-kappa-unbiased-raters",
    "href": "kappa.html#the-fleiss-kappa-unbiased-raters",
    "title": "Chapter 3: Kappa Statistics",
    "section": "2 The Fleiss Kappa: Unbiased Raters",
    "text": "2 The Fleiss Kappa: Unbiased Raters\nThe Fleiss kappa is designed to work over multiple raters and a rating scale of arbitrary length. It assumes an asymptotic form for chance correction, so is most appropriate for large samples. We only consider the binary scale case here. Extension to larger scales is a topic in the discussion section at the end of this essay.\nThe baseline for random ratings for Fleiss is if we took all the ratings and randomly shuffled them between subjects. This worse case has accuracy equal to zero, and the rating process devolves to\n\\[\n\\begin{aligned}\nC_{ij} &= T_iA_{ij} + \\bar{A_{ij}}P_{ij} \\\\\n&= P_{ij}\n\\end{aligned}\n\\]\nThis is a simple binomial distribution with probability \\(p\\) , not a mixture of them, and \\(p = \\hat{c}\\) (the observed average fraction of Class 1 ratings) will have maximum likelihood over binomial models. In this case, we have no information about \\(t\\), and any value will do in the model, since it has no effect. The Fleiss kappa uses this random baseline and takes it one step further by assuming that the fraction of true Class 1 cases is the same as the reported fraction, as well, i.e. \\(t = \\hat{c}=p\\). The Fleiss kappa can therefore give poor estimates for \\(a\\) when the raters get the proportion of Class 1 cases wrong.\nFor example, consider a test for a medical condition that has a prevalence of 10%, but the test is in fact returning random results that are half positive and half negative. Then the kappa’s baseline for comparison is wrong, and the resulting estimate for accuracy will be biased.\nA review of the properties of Fleiss kappa can be found in Fleiss et al. (2013), chapter 18, including kappa’s equivalence to an intraclass correlation coefficient, defined as ICC(1,1) in Shrout & Fleiss (1979). In our notation here, the Fleiss kappa is defined as taking \\(m_c = \\hat{c}^2 + \\hat{\\bar{c}}^2\\), where \\(\\hat{c}\\) is the observed proportion of class 1 ratings. The Fleiss kappa is therefore a \\(\\hat{c}\\)-\\(a\\)-\\(\\hat{c}\\) model. The functional form \\(f(x) = x + x^2\\) for \\(x \\epsilon [0,1]\\) is parabolic, with a minimum at \\(x = 1/2\\), where \\(f(x) = 1/2\\) and maxima at \\(x = 0\\) and \\(x = 1\\), when \\(f(x) = 1\\). Therefore, the chance correction term \\(m_c\\) of the Fleiss kappa is opinionated, favoring larger rates of rater error. We would expect that the rater agreement statistic will tend to underestimate rater accuracy, taken to be \\(\\hat{a} = \\sqrt{\\kappa}\\).\nAs noted, the chance correction for Fleiss depends on the observed proportion of Class 1 ratings, \\(\\hat{c}\\). The expected value under the tap model assumptions is \\(E[\\hat{c}] = c = ta + p\\bar{a}\\); ratings of Class 1 result from either true and accurate ratings or from inaccurate ones that happen to guess Class 1.\nWhen \\(p = t\\), what we’re calling unbiased raters, the expectation of ratings simplifies to \\(c = pa + p\\bar{a} = p\\). Intuitively, this case causes rater accuracy \\(a\\) to wash out of the observed rate of Class 1 ratings \\(\\hat{c}\\), so that the chance correction can be estimated directly. In the discussion of Equation 2, we saw that when \\(t = p\\), \\(m_c^* = p^2 + \\bar{p}^2\\). Therefore, the Fleiss kappa is equivalent to the chance correction that makes \\(E[\\kappa] = a^2\\) when \\(t = p\\). Under this assumption, when raters make inaccurate ratings, their ratings at least conform to the distribution of true values \\(t\\), and more generally the fraction \\(c\\) of Class 1 ratings is the true fraction. I’ll refer to that case “unbiased” raters, since on average their work represents the true distribution of the data, which in the notation here is \\(t = c\\).\nUnder this \\(t = c\\) “unbiased” condition, rater accuracy \\(a\\) is the correlation between the raters’ ratings and the true classifications, \\(\\sqrt{E[\\kappa]} = a = cor(C, T)\\). Additionally, the Fleiss kappa is the intraclass correlation of the ratings. Derivations of these results are found in Appendix A, where there is also an alternative derivation of the \\(a = \\sqrt{E[\\kappa}]\\) result.\nEven when \\(t \\ne p\\), when \\(a = 1\\), \\(\\kappa = 1\\), and when \\(a = 0\\), \\(\\kappa = 0\\) or is undefined."
  },
  {
    "objectID": "package.knit.html",
    "href": "package.knit.html",
    "title": "The TapModel Package",
    "section": "",
    "text": "1 Introduction\nThis is a user guide to the tapModel package, which you can find on github. The guide will show you how to format a data set of ratings and fit a t-a-p model to the data.\nThere are data sets included in the package to use as examples. We’ll use the wine data set. One the tapModel library is loaded, you can access the data set with data(wine).\n\n\nCode\nlibrary(tidyverse)\nlibrary(tapModel)\nlibrary(knitr)\n\n#' Load the included wine data\ndata(wine)\n\n#' It has rating in columns, one for each wine judge. Each row is a wine.\n#' We can convert to a standard long format with the `format_data` function.\nratings &lt;- tapModel::format_data(wine, data_type = \"raters\")\n\n#' To compute the t-a-p model, we need the counts for each class. This is a \n#' dataframe with one row per subjec, with two columns:\n#'        N_r, the number of ratings for that subject\n#'        N_c, the number of ratings in class 1 for that subject\n#'        \n#' If we define Class 1 to be \"acceptable\" wine, with ratings {2,3,4} vs \n#' Class 0 = rating 1, we can find the counts with\ncounts &lt;- tapModel::count_ratings(ratings, c(2,3,4))\n\n#' Compute the solution\ntap &lt;- tapModel::iterative_optim(counts)\n\n#' Show the results\nkable(tap, digits = 2)\n\n\n\n\n\nt\na\np\nll\ndegenerate\n\n\n\n\n0.73\n0.54\n0.78\n-243.36\nFALSE\n\n\n\n\n\nYou can also format the data using tidyverse functions without the helpers.\n\n\nCode\ninclass &lt;- 2:4 # set the range of acceptable ratings\n\n# this will work for the data format where each rater has a column\ncounts &lt;- wine %&gt;%\n  mutate(N_r = rowSums(across(everything(), ~ !is.na(.))),   \n         N_c = rowSums(across(everything(), ~ . %in% inclass))) |&gt; \n  select(N_r, N_c)\n          \ncounts |&gt; \n  head(5) |&gt; \n  kable()\n\n\n\n\n\nN_r\nN_c\n\n\n\n\n4\n5\n\n\n4\n5\n\n\n4\n5\n\n\n4\n4\n\n\n4\n3\n\n\n\n\n\n\n\n2 Ordinal Analysis\nIn addition to a binary comparison, where we split the range of possible ratings into two sets, we can also use the ordinal analysis. This is where we put the ratings in a sensible order, like “strongly disagree” to “strongly agree” for a common survey response, then consider each cut-point between these values. In the case of the wine ratings, we have:\n\nWine rating scale\n\n\nRating Value\nMeaning\n\n\n\n\n1\nNo medal\n\n\n2\nBronze medal\n\n\n3\nSilver medal\n\n\n4\nGold medal\n\n\n\nThere are three natural cut-points:\n\n1|2 divides the 1 ratings from higher ones\n2|3 divides the {1,2} ratings from the {3,4} ratings\n3|4 divides the {1,2,3} ratings from the 4 ratings.\n\nEach cut point defines a binary classification system, which we can estimate the t-a-p parameters for.\nTo use the built-in function for iterating over the cut-points, we need to prepare the data to be in a long format with only two columns SubjectID__ and rating.\n\n\nCode\n#' this will work for the data format where each rater has a column\nratings &lt;- wine %&gt;%\n  mutate(SubjectID__ = row_number()) |&gt; \n  gather(judge, rating, -SubjectID__) |&gt; \n  select(SubjectID__, rating)\n\nratings |&gt; \n  head(5) |&gt; \n  kable()\n\n\n\n\n\nSubjectID__\nrating\n\n\n\n\n1\n3\n\n\n2\n3\n\n\n3\n3\n\n\n4\n3\n\n\n5\n4\n\n\n\n\n\nNow we can apply the function to find estimates for the parameters at each cut-point as well as the Fleiss kappa statistics for each.\n\n\nCode\nparams &lt;- tapModel::get_ordinal_tap(ratings)\n\nkable(params, digits = 2)\n\n\n\n\n\nt\na\np\nll\ndegenerate\nCutPoint\ntype\n\n\n\n\n0.27\n0.54\n0.22\n-243.36\nFALSE\n1|2\nt-a-p\n\n\n0.24\n0.57\n0.24\n-243.47\nFALSE\n1|2\nFleiss\n\n\n0.54\n0.44\n0.59\n-286.12\nFALSE\n2|3\nt-a-p\n\n\n0.57\n0.44\n0.57\n-286.14\nFALSE\n2|3\nFleiss\n\n\n0.65\n0.25\n0.95\n-169.94\nFALSE\n3|4\nt-a-p\n\n\n0.88\n0.35\n0.88\n-170.82\nFALSE\n3|4\nFleiss\n\n\n\n\n\nHere’s how to plot the results.\n\n\nCode\nparams |&gt; \n  select(t,a,p,CutPoint, type) |&gt; \n  gather(param, value, t, a, p) |&gt; \n  ggplot(aes(x = CutPoint, y = value, color = type, group = type)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(. ~ param)\n\n\n\n\n\n\n\n\n\n\n\n3 Bayesian Models\nBayesian estimation using Monte Carlo Markov Chain (MCMC) estimation methods are flexible in allowing for maximum likelihood estimation over any likelihood function you can imagine. This includes the standard t-a–p three-parameter model, but also includes hierarchical models where each subject is assigned a truth value and each rater is assigned an accuracy estimate. The tapModel package provides some limited interface to the most common of these models using library(cmdstanr). Installing that package entails some setup work. Please see the package documentation to install and test your installation before trying out the code below.\nThe use of Stan for Bayesian estimation is also included in the package, with an easy interface for three different models:\n\nt-a-p.stan, a three-parameter t-a-p model\nt-a0a1-p.stan, a four-parameter model that splits the accuracy parameter into \\(a0\\) and \\(a1\\), so that raters can have different accuracies depending on the true classification, and\nt-a0a1-p0p1.stan, a five-parameter version that expands the four-parameter version to also include \\(p0\\) and \\(p1\\), so the guessing probabilities are also conditional on the true classification.\n\nWe can use the counts data frame from the introduction to illustrate.\n\n\nCode\nlibrary(cmdstanr)\nlibrary(LaplacesDemon) # for the Modes() function\n\n#' Get the .stan source code for the chosen model\nstan_model_source &lt;- system.file(\"stan/t-a-p.stan\", package = \"tapModel\")\n\n#' Compile it. This can take a while, but you only need to do it once.\n#' maybe go make a cup of tea\nstan_model &lt;- cmdstanr::cmdstan_model(stan_model_source)\n\n#' Use the convenience function to run the MCMC\n#' This requires counts to be properly formated. \nmodel_draws &lt;- tapModel::fit_tap_model(counts, stan_model) \n\n#' Summarize the statistics\nest_params &lt;- tapModel::get_tap_stats(model_draws)\n\n#' Simplify the draw data into a data fram and then\n#' plot the draw distributions\nmodel_draws |&gt; \n  tapModel::extract_vars() |&gt; \n  tapModel::plot_draw_densities()\n\n\nThe get_tap_stats function returns a data frame with the mean and interquartile range of the draws, as well as an attempt to identify bimodal densities, which can indicate a problem with model fit or non-unique solutions.\n\n\n\n\n\nposition\nvar\navg\np05\np25\nmedian\np75\np95\nmode1\nmode2\n\n\n\n\n2\na\n0.54\n0.45\n0.50\n0.54\n0.58\n0.64\n0.53\n0.55\n\n\n3\np\n0.77\n0.63\n0.73\n0.78\n0.82\n0.88\n0.80\nNA\n\n\n4\nt\n0.73\n0.62\n0.69\n0.73\n0.78\n0.83\n0.73\nNA\n\n\n\n\n\nThe plot_draw_densities function will plot the draws for each parameter, with the mean labeled and the interquartile range shaded. The average log likelihood is indicated with a dotted line.\n\n\n\nParameter estimates for t-a-p on wine data with mean labeled and interquartile range shaded. Average log likelihood is indicated with dotted line."
  },
  {
    "objectID": "paradox.html",
    "href": "paradox.html",
    "title": "Chapter 4: More parameters",
    "section": "",
    "text": "1 Introduction\nProblems with chance-corrected agreement statistics have been studied for a long time, in particular when one class dominates the ratings (classifications). This would happen, for example, in a medical test for a rare condition, where most of the results would be negative. Non-intuitive results for kappa can result from such cases, a situation that has become known as the Kappa Paradox. See Bonett (2022) for one survey of such issues. Attempts have been made to modify the basic kappa formula to accommodate unbalanced classifications. In Byrt et al. (1993), observed agreement is adjusted for bias in rater tendencies (which would be the \\(p\\) parameter here) and for relative prevalence of classes. Krippendorff (2013),illustrates such a hypothetical case (page 484):\n\nSuppose an instrument manufacturer claims to have developed a test to diagnose a rare disease. Rare means that the probability of that disease in a population is small and to have enough cases in the sample, a large number of individuals need to be tested. […] Suppose two separate doctors administer the test to the same 1,000 individuals. Suppose each doctor finds one in 1,000 to have the disease and they agree in 998 cases on the outcome of the test.\n\nThe paradoxical result is that the Fliess kappa for this example is near zero, even though the raters agree on 99.8% of the cases. The reason is that the kappa is defined as the agreement rate minus the agreement rate expected by chance, and the expected agreement rate is very high due to the distribution of classes. Such unbalanced cases are common in practice, leading to the creation of new inter-rater measures like AC1 (Gwet, 2008) to attempt to accommodate this situation, but this solution has been criticized as well (Krippendorff, 2013).\nThe controversy can be summarized in the two points of view:\n\n(pro-kappa) the test has zero agreements on test-positive cases, and is therefore unreliable, therefore \\(\\kappa = 0\\) is correct, versus\n(anti-kappa) almost all the cases are in perfect agreement, therefore the test is reliable, and kappa should be near one.\n\nThe apparent problem can be usefully seen through the lens of the Fleiss kappa. If the baseline for comparison is randomly chosen pairs of ratings, any such set will be identical to the original data set in this case. Since the data is indistinguishable from randomness, the kappa, and hence accuracy should be zero. So in that sense the Fleiss kappa gives the correct result (kappa nearly zero). This conflicts with intuition in that there is a high level of raw agreement in the data.\nReliability statistics are generally concerned with two kinds of differences in ratings. Within subjects, if individual subjects are assigned ratings that don’t have much agreement, this is like measurement error. An ideal measure would return the same rating for the same subject every time. The other kind of difference is between subjects. There we want variation, because if all the subjects get the same rating, the scale doesn’t give us information. In the paradox example, there’s low variation within subjects (low measurement error), but very little variation between subjects, so the scale itself isn’t given us much information.\n\n\n2 Modeling the Paradox\nThe t-a-p model can illuinate this situation. If nearly all the ratings \\(C_{ij}\\) are one value, then the likelihood function over rating sums can be approximately reduced to the likelihood function over individual ratings given by\n\\[ \\text{L}(t,a,p;c_{ij}) \\approx a(t - p) + p \\tag{1}\\]\nso that \\(\\frac{\\partial L}{\\partial t} = a\\), \\(\\frac{\\partial L}{\\partial a} = t- p\\), and \\(\\frac{\\partial L}{\\partial p} = 1 - a\\). Setting these to zero in the usual way to find the maximum, we obtain contradictory solutions to the likelihood maximization problem, since the first and last conditions imply that \\(a=0\\) and \\(a=1\\), respectively. Additionally \\(t = p\\) in Equation 1 requires that raters are unbiased. The contradictory solutions for rater accuracy \\(a\\) reflect the two “paradoxical” positions described above, and these show up empirically if we solve for the unconstrained parameters. The data are nearly under-determined with respect to the full t-a-p model, and the posterior estimate samples show prevarication.\nTo illustrate the dual nature of the maximum likelihood solution, he three-parameter t-a-p model was fitted to N = 1,000 samples with R = 2 (two raters). As described above, 998 of the subjects are assigned Class 0 (negative test result) by both raters, and for the remaining two cases they split, with one Class 1 and one Class 0 each.\n\n\nShow the code\nrefresh = FALSE \n\nif (refresh) {\n  \n  counts &lt;- data.frame(\n                 N_r = 2,\n                 N_c = c(rep(0,980),rep(1,2)))\n  \n  model &lt;- cmdstan_model(\"code/t-a-p.stan\")\n  draws &lt;- tapModel::fit_tap_model(counts, model)\n  write_rds(draws,\"data/Paradox_samples.RDS\")\n\n} else { # to save time\n  \n  draws &lt;- read_rds(\"data/Paradox_samples.RDS\")\n\n}\n\n# plot the draw densities\ndraws |&gt; \n  tapModel::extract_vars() |&gt; \n  tapModel::plot_draw_densities()\n\n\n\n\n\n\n\n\nFigure 1: Results from a t-a-p model on the kappa paradox data, showing the density of estimates.\n\n\n\n\n\nTo see the Stan code used to fit the Bayesian t-a-p model, click the button below.\n\n\nShow the code\n\n//\n//   Stan model specification for fixed rater a and no random effects\n//\n// Learn more about model development with Stan at:\n//\n//    http://mc-stan.org/users/interfaces/rstan.html\n//    https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\n//\n\n\n// functions to make the code simpler below\nfunctions {\n  real p_true(real a, real s) {  // convenience function for binomial probability for \n    return a + (1.0-a)*s;        // subjects that are class 1 in reality\n  }\n  real p_false(real a, real s) {  // convenience function for binomial probability for\n    return (1.0-a)*s;            // subjects that are class 2 in reality\n  }\n  \n}\n\n// The ratings summary (number of 1-ratings per case) and descriptives\ndata {\n  int&lt;lower=0&gt; N;   // number of subjects\n  array[N] int&lt;lower=0&gt; R;   // number of raters fo a given subject\n  array[N] int count;  // count of ratings of category 1 for subject i\n}\n\n// The parameter to estimate\nparameters {\n  real&lt;lower=0, upper = 1&gt; a; // fixed for all raters\n  real&lt;lower=0, upper = 1&gt; p;        // guess rate for class 1 when inaccurate\n  real&lt;lower=0, upper = 1&gt; t;        // true class 1 rate\n}\n\n// The model to be estimated. We model the output\n// count (of 1s) by the binomial mixture described\n// in the paper. S is the fraction of 1-ratings in the whole data set\n// The log_sum_exp function is useful for this--we take the log of each binomial \n// likelihood using built-in functions, and the log_sum_exp function exponentiates,\n// adds, and then takes the log to get the actual likelihood we care about. \n// cf http://modernstatisticalworkflow.blogspot.com/2016/10/finite-mixture-models-in-stan.html\nmodel {\n  a ~ uniform(0,1);\n  t ~ uniform(0,1);\n  p ~ uniform(0,1);\n  \n  for(i in 1:N) {  // for each subject rated\n    target += log_sum_exp(log(t)   + binomial_lpmf(count[i] | R[i], p_true(a,p)),\n                          log(1-t) + binomial_lpmf(count[i] | R[i], p_false(a,p)));\n  }\n}\n\n\n\nThe results in Figure 1 come from a MCMC summary of samples for the parameters. The plot for \\(a\\) shows a bimodal distribution of rater accuracy that slightly favors \\(a \\approx 0\\) over \\(a \\approx 1\\). Also in agreement with the analysis, the mode for \\(p - t\\) is nearly zero since both of those statistics are near zero. The \\(t \\approx 0\\) tells us that the modeled result is that there are very view Class 1 (test positive) cases.\nThe parameter estimates in Figure 1 point us to divergent interpretations of the severely unbalanced data: (1) the accuracy is about zero and the results are due almost entirely to random ratings combined with a severe test bias, or (2) the accuracy is high, but the prevalence of the condition is very small. These two conclusions correspond to the usual philosophical positions in the paradox’s discussion. The graphs in Figure 1 show usefulness of MCMC sampling, and serve as a caution against taking mean estimate values as meaningful without inspecting the distribution first.\n\n\n3 Divergent Accuracy\nThe MCMC estimation for the accuracy parameter vacillates between two poles, which leads to the idea to split Class 1 accuracy from Class 0 accuracy using two parameters for accuracy. This approach was suggested in Cicchetti & Feinstein (1990). It is a straightforward modification to the t-a-p model to bifurcate accuracy by using \\(a_1\\) for the true Class 1 branch of the probability diagram, and \\(a_0\\) for the Class 0 side. The MCMC posterior distributions for those two parameters are shown in Figure 2.\n\n\nShow the code\nrefresh = FALSE \nrefresh = FALSE \n\nif (refresh) {\n  \n  counts &lt;- data.frame(\n                 N_r = 2,\n                 N_c = c(rep(0,980),rep(1,2)))\n  \n  model &lt;- cmdstan_model(\"code/t-a0a1-p.stan\")\n  draws &lt;- tapModel::fit_tap_model(counts, model)\n  write_rds(draws,\"data/Paradox_samples2.RDS\")\n\n} else { # to save time\n  \n  draws &lt;- read_rds(\"data/Paradox_samples2.RDS\")\n\n}\n\n# plot the draw densities\ndraws |&gt; \n  tapModel::extract_vars() |&gt; \n  tapModel::plot_draw_densities()\n\n\n\n\n\n\n\n\nFigure 2: Results from a t-a0,a1-p model on the kappa paradox data, showing the density of the parameter estimates. This one includes separate accuracy parameters for each class: a0 for Class 0 and a1 for Class1.\n\n\n\n\n\nTo see the Stan code used to fit the Bayesian t-a0,a1-p model, click the button below.\n\n\nShow the code\n//\n//   Stan model specification for fixed rater accuracy and no random effects\n//\n// Learn more about model development with Stan at:\n//\n//    http://mc-stan.org/users/interfaces/rstan.html\n//    https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\n//\n\n// functions to make the code simpler below\nfunctions {\n  real p_true(real a, real s) {  // convenience function for binomial probability for \n    return a + (1.0-a)*s;        // subjects that are class 1 in reality\n  }\n  real p_false(real a, real s) {  // convenience function for binomial probability for\n    return (1.0-a)*s;            // subjects that are class 2 in reality\n  }\n  \n}\n\n\n// The ratings summary (number of 1-ratings per case) and descriptives\ndata {\n  int&lt;lower=0&gt; N;   // number of subjects\n  array[N] int&lt;lower=0&gt; R;   // number of raters fo a given subject\n  array[N] int count;  // count of ratings of category 1 for subject i\n}\n\n// The parameter to estimate\nparameters {\n  real&lt;lower=0, upper = 1&gt; a1; // fixed for all raters\n  real&lt;lower=0, upper = 1&gt; a0; // fixed for all raters\n  real&lt;lower=0, upper = 1&gt; p;        // guess rate for class 1 when inaccurate\n  real&lt;lower=0, upper = 1&gt; t;        // true class 1 rate\n  \n}\n\n// The model to be estimated. We model the output\n// count (of 1s) by the binomial mixture described\n// in the paper. S is the fraction of 1-ratings in the whole data set\n// The log_sum_exp function is useful for this--we take the log of each binomial \n// likelihood using built-in functions, and the log_sum_exp function exponentiates,\n// adds, and then takes the log to get the actual likelihood we care about. \n// cf http://modernstatisticalworkflow.blogspot.com/2016/10/finite-mixture-models-in-stan.html\nmodel {\n  a1 ~ uniform(0,1);\n  a0 ~ uniform(0,1);\n  t ~ uniform(0,1);\n  p ~ uniform(0,1);\n  \n  for(i in 1:N) {  // for each subject rated\n    target += log_sum_exp(log(t)   + binomial_lpmf(count[i] | R[i], p_true(a1,p)),\n                          log(1-t) + binomial_lpmf(count[i] | R[i], p_false(a0,p)));\n  }\n}\n\n\n\nHistograms of MCMC draws in Figure 2 for the t-a0,a1-p model reveal the limitations of the three-parameter version. The plots suggest high confidence that \\(a_0\\) is near 1, and somewhat less confidence that \\(a_1\\) is near zero. There is evidence that the accuracy of negative test results is high, but there is no evidence that the accuracy of positive test results is high. A scatterplot of the two parameters as drawn together from the posterior distributions shows that they cluster around the modes of the distributions in Figure 2. In other words, the induced model prefers the case when the accuracy of negative test results is high, but the accuracy of positive test results is low.\n\n\nShow the code\n# make a scatterplot of the two accuracy parameters\ndraws |&gt; \n  tapModel::extract_vars() |&gt; \n  ggplot(aes(x = a1, y = a0)) +\n  stat_density_2d_filled(aes(fill = ..level..)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 3: Scatterplot of the two accuracy parameters from the t-a0,a1-p model, showing that they converge to a high value of Class 0 accuracy and a low value of Class 1 accuracy.\n\n\n\n\n\nThe use of additional parameters to clarify the cases of unbalanced data can be expanded to accommodate a wide range of rater models.\nIt seems contradictory to say that we can classify Class 0 with confidence, but not Class 1, because these seem like two sides of the same coin. However, when there are many Class 0 cases, and only a few Class 1 cases, we could have 99% accuracy in true positives for Class 0 and nearly zero true positives for Class 1. One problem with the traditional kappa measures comes from condensing those two classification statistics into one.\n\n\n4 Individual \\(p\\) parameters\nWe could take this one step further and split \\(p\\) into two parameters that correspond to the two classes. In this t-a0,a1-p0,p1 case, we may get data sets that are not identifiable for label-switching reasons. That is, we may be able to find the maximum likelihood mixture of binomial distributions, but not know which is Class 0 and which is Class 1.\nWith the \\(a_0,a_1,p_0,p_1\\) parameters, label-switching means that we swap \\(\\bar{t}\\) for \\(t\\) and the two corresponding means swap as well. That requires some second set of the parameters \\(a_0',a_1',p_0',p_1'\\) such that\n\\[\n\\begin{aligned}\na_1 + \\bar{a_1}p_1 &= \\bar{a_0}'p_0' \\\\\na_1' + \\bar{a_1}'p_1' &= \\bar{a_0}p_0 \\\\\n\\end{aligned}\n\\tag{2}\\]\nIf Equation 2 holds, we can swap the primed parameters for the non-primes, and exchange \\(t\\) for \\(1-t\\), and the distribution is the same, with the same likelihood. There are four free parameters and two constraints shown above, plus the constraints that all parameters are in [0,1]. A example of a label-switching set is when \\(\\mu_0 = \\bar{a_0}p_0 &gt; a_1 + \\bar{a_1}p_1 = \\mu_1\\), so that there are more Class 1 ratings for Class 0 cases that there are for true Class 1 cases. Suppose \\(a_0 = .5\\), \\(a_1 = .2\\), \\(p_0 = .7\\), and \\(p_1 = .1\\). Then we have \\(\\mu_0 = (.5)(.7) = .35\\) and \\(\\mu_1 = .2 + (.8).1 = .28\\), so the mean of the Class 1 ratings is to the left of the mean of the Class 0 ratings.\nSuch situations point to poor quality data combined to too many parameters. For example, suppose the example above is wine tasting ratings, and it’s easier to for judges to agree on poor wine than excellent wine (this seems to be the case in practice). Suppose further that some of the raters are motivated to over-rate poor wines, pushing up the \\(p_0\\) rate to .7. This rating inflation makes the poor wines seem better than they are, and by comparison the good wines seem worse. From the data, we can’t tell which class is which under these model assumptions and with this data. However, the MCMC results will at least show us a multi-model distribution. In general, splitting the accuracy or randomness parameters should probably only be done for a specific reason, as with the paradox example, where we saw a bimodal distribution for accuracy.\n\n\n5 Estimating Parameters\nThe tapModel library and the advanced version of the interactive application can both be used to generate MCMC results from the t-a-p models expanded to include two \\(a\\) parameters and (optionally) two \\(p\\) parameters. There is an illustration of this in Chapter 8: The t-a-p App.\n\n\n\n\n\nReferences\n\nBonett, D. G. (2022). Statistical inference for g-indices of agreement. Journal of Educational and Behavioral Statistics, 47(4), 438–458.\n\n\nByrt, T., Bishop, J., & Carlin, J. B. (1993). Bias, prevalence and kappa. Journal of Clinical Epidemiology, 46(5), 423–429.\n\n\nCicchetti, D. V., & Feinstein, A. R. (1990). High agreement but low kappa: II. Resolving the paradoxes. Journal of Clinical Epidemiology, 43(6), 551–558.\n\n\nGwet, K. L. (2008). Computing inter-rater reliability and its variance in the presence of high agreement. British Journal of Mathematical and Statistical Psychology, 61(1), 29–48.\n\n\nKrippendorff, K. (2013). Commentary: A dissenting view on so-called paradoxes of reliability coefficients. Annals of the International Communication Association, 36(1), 481–499."
  },
  {
    "objectID": "app.html#basic-functions",
    "href": "app.html#basic-functions",
    "title": "Interactive t-a-p Analysis",
    "section": "",
    "text": "The simplest way to get started is with the tapModel library. It provides these functions:\n\nLoad data from comma-delimited value (CSV) files that are formatted as\n\nOutcome ratings in columns, where multiple traits have rating data, one subject per row, and multiple traits per subject. For example if a jury of reviewers rates a musical performance on style, technique, and musicality, each performer would have multiple rows, each with those three columns.\nRaters in columns, where the same type of jury data can be reformatted to have a Category column (style, technique, musicality), and each rater appears as a single column. In this format each subject only appears on a single row.\nLong format, where the subject ID, optional category, and rating appear as the three columns of the CSV.\n\nSimulate a data set by specifying the t-a-p parameters and sample sizes.\nEstimate t-a-p parameters from a (simulated or real) data set.\nEstimate ordinal t-a-p parameters from a data set. This assumes that the rating scale is sorted alphabetically in the correct order. For example, a numerical survey response scale is usually in the right order, but the labels may not be (“neutral” doesn’t sort in the middle of “strongly agree” and “strongly disagree”). You may need to adjust the rating labels accordingly, e.g. “1 - strongly disagree”, … “5 - strongly agree”."
  },
  {
    "objectID": "app.html#advanced-functions",
    "href": "app.html#advanced-functions",
    "title": "Interactive t-a-p Analysis",
    "section": "",
    "text": "If you launch the app from the github source code and have the stancmdr package installed, some additional features become available. These derive from using Bayesian modeling written in the Stan programming language to make parameter estimates from maximum likelihood models built to reflect variations of the t-a-p model. Markov chain Monte Carlo (MCMC) methods are used to explore the model’s probability distribution, which you can then see within the app. This is advantageous because a parameter estimate may have a bimodal distribution when parameters are not cleanly identifiable. In those cases relying on an average for a parameter estimate is a mistake.\nAnother advantage of MCMC’s numerical simulation is that the basic t-a-p model can be extended to include more parameters without being rigidly tied to the binomial mixture model. Finally, the simple methods used for three-parameter estimation will fail when confronted with many parameters. In the interactive application, there is limited ability to add parameters. For complete flexibility, Stan scripts are provided in the chapter on hierarchical models.\nThe following sections describe the functionality of both the package app and the stand-alone version, with the latter’s additional features marked as “advanced.”"
  },
  {
    "objectID": "app.html#exploring-binary-mixtures",
    "href": "app.html#exploring-binary-mixtures",
    "title": "Interactive t-a-p Analysis",
    "section": "3.1 Exploring Binary Mixtures",
    "text": "3.1 Exploring Binary Mixtures\nIf you’re starting to use the resources on this site to analyze real data sets, it’s a good idea to develop an intuition for the assumptions. The decision tree that starts with the true class, proceeds to accuracy and then randomness, results in a particular pattern of ratings: the collection of true Class 1 subject ratings will look different from the Class 0 collection as long as accuracy is greater than zero. On average, there should be more Class 1 ratings for true Class 1 subjects, and the gap between the average Class 1 and Class 0 ratings will increase as accuracy increases.\nTo see this effect, increase the sample size and number of raters (top two sliders) to the maximum, leave the rest of the sliders alone and generate the data.\n\n\n\nData with large number of raters and all parameters = .5\n\n\nThe table at the top of the display gives statistics for the model specification, which may vary from the actual samples if the Random option is chosen. These correpond to the statistics found in the Kappa chapter. The class proportions in the first row are the \\(c\\) and \\(\\bar{c}\\) statistics, the match rates are \\(a^2\\) and \\(\\bar{a}^2\\), and total match rate is the sum. These last two rows will be different calculations if individual accuracies are used. The random match rate is \\(m_r = p^2 + \\bar{p}^2\\) unless the individual parameters are used. The mixed rate is \\(m_x\\).\nThe shape of the rating distribution is a histogram of the counts per subject of Class 1 ratings per subject. The true Class 1 cases comprise the right bump, since accurate ratings add to the number for each subject half the time (since \\(a = .5\\)). On average, true Class 1 cases are receiving 75/100 ratings of Class 1. The true Class 0 cases–the left bump in the histogram–only ever receive inaccurate ratings of Class 1. The difference between the two peaks is 50, and that’s because 50 = .5(100), or rater accuracy times the number of raters. The more accurate the raters are, the further the two bumps will be apart.\nIf you dial in different specifications for the three parameters, you’ll quickly develop an intuition for how these models work. For example what happens if we leave everything the same, but change to \\(t = .8\\)? You should see that changing the truth parameter only increases the pile of Class 1 ratings; it doesn’t change where they are. This is the “mixture” parameter in the binomial mixture. We already know that \\(a\\) represents the average amount of Class 1 votes between the two bumps in the histogram, but trying out different values of \\(a\\) will allow you to visualize that.\nThe default settings of .5 mean that \\(t = p\\), the unbiased rater case (see the chapter on Kappa for more on that). If you change the sliders so that \\(t \\ne p\\), you can see the effect of bias on the rating distribution.\nFor all of these cases, you can switch to the next tab in the app (the one labeled “t-a-p”) to see if the solver can recover the correct values of the parameters you specified at data generation."
  },
  {
    "objectID": "app.html#feasibility-study",
    "href": "app.html#feasibility-study",
    "title": "Interactive t-a-p Analysis",
    "section": "3.2 Feasibility Study",
    "text": "3.2 Feasibility Study\nThe data simulator can be used for a feasibility study (see power analysis). Suppose a group of graders is to read and evaluate student writing samples as passing or failing. If there are anticipated to be 21 subjects, how many raters for each would we need to be able to assess rater accuracy with a t-a-p model? If we guess that about 80% of the students should be passing, and–based on other data–that rater accuracy is around 50%, we can try varying numbers of raters to see how well the parameters can be recovered.\n\n\n\nSimulated data with 21 subjects, two raters each, 80% true Class 1 rate, and 50% accuracy. The “unbiased” box checked means that p = t = .8. The data set is generated without sampling error in this case.\n\n\nIt’s less obvious in this distribution how we might separate out the Class 1 from Class 0 cases, and the question is whether or not the solver can recover the parameters. If not, then it’s worth considering the design of the anticipated study."
  },
  {
    "objectID": "app.html#additional-parameters",
    "href": "app.html#additional-parameters",
    "title": "Interactive t-a-p Analysis",
    "section": "3.3 Additional Parameters",
    "text": "3.3 Additional Parameters\nAs discussed in the chapter on the Kappa Paradox, it’s possible to expand the t-a-p model to include \\(a\\) and \\(p\\) parameters that are estimated separately for Class 1 and Class 0 cases. These parameters can be set by using the options\n\nUse a0, a1, which creates the two sliders and unlocks the next option:\nUse p0, p1, which creates those two sliders\n\nWith that much flexibility over the probability distribution, it’s possible to create non-identifiable data sets, where the Class 0 mean is larger than the Class 1 mean. In those cases there will effectively be two solutions to the expanded t-a-p model, one with the Class 1 mean to the right (as normal) and one to the left. Any use of the full parameter set should be assumed to be non-identifiable, no matter where the class means lie. Analyzing these models requires the Bayesian methods included in the advanced features. You can, however, generate data from the complex model and then see how the three-parameter t-a-p solver does at finding a plausible solution."
  },
  {
    "objectID": "app.html#interpreting-results",
    "href": "app.html#interpreting-results",
    "title": "Interactive t-a-p Analysis",
    "section": "4.1 Interpreting Results",
    "text": "4.1 Interpreting Results\n\n\n\nModeled distribution (line) compared to empirical distribution (lollipops)\n\n\nThe top display after clicking Compute gives the estimates for the three parameters at the top of a plot. For the wine ratings with Class 1 = {1} and Class 0 = {2, 3, 4} is shown here. That choice is asking the question “how well can the judges distinguish the lowest quality wines from the rest?” The estimate is that 27% of the wines are actually Class 1 (a rating of 1), that rater accuracy is 54%, and that when random assignments are made, Class 1 is chosen 22% of the time. Since \\(t = .27\\) is close to \\(p = .22\\), the ratings are nearly unbiased in the sense discussed in the Kappa chapter.\nThe dashed line in the plot is the expected distribution of Class 1 ratings per subject. The vertical black lines with dots (lollipops) show the actual (empirical) distribution from the data. The extent to which these two agree is a measure of model fit. In this example, there are four raters for each subject (each wine), so there are a maximum of four ratings of Class 1 (the lowest quality rating of 1, as we specified with the selectors). The agreement between model and data looks better for the 0 and 1 counts than for the 2, 3, and 4 counts, implying that the model fit is better for higher ratings (Class 0).\nWe can separate the model’s distributions for the two classes by unchecking the box “show combined distribution.” The box “scale density by t” is checked as well.\n\n\n\nModeled distributions of the two classes\n\n\nThe outclass (Class 0) is modeled by the t-a-p coefficients with a spike at zero, meaning that by far the mostly likely number of Class 1 ratings in cases where the subject (the wine) is truly Class 0 is that no Class 1 ratings are assigned by the four raters. Translating that back to the original question, it means if the wine should, in truth, be rated as 2, 3, or 4 on the scale, it’s quite likely that all four wine judges will assign one of those ratings instead of a 1. In statistics notation, the spike at zero would be written as\n\\[\nPr[\\text{all wine ratings &gt; 1} | \\text{wine is actually 2, 3, or 4 quality}] = .48.\n\\]\nOn the other hand, if the wine is, in truth, a quality rating 1 wine, the ratings are not as unanimous. The most likely case is that three of the four judges will assign a 1 rating (what we’re calling Class 1, or in-class), and it’s a mound-shaped distribution rather than the spike as for Class 0.\nThe average rater accuracy is the difference between the averages for the two distributions shown, after dividing by the number of raters (4). Estimating from the plot, the mean of Class 0 is about .5, and the mean of Class 1 is about 2.7, for a difference of 2.2. Dividing by four gives .55, which is quite close to the numerical estimate of .54.\nIt sounds contradictory, but raters can be better at classifying Class 0 than Class 1, as it seems to be in this case. We could try splitting the accuracy parameter into two separate ones to improve model fit. This is described in the chapter on the Kappa Paradox.\n\n\n\nLikelihood trace for the t parameter\n\n\nThe second plot shows the shape of the log-likelihood for each of the parameters. It ranges over [0,1] for the selected parameter (here it’s \\(t\\)), while holding the other parameters constant at their estimated values (here it is \\(a= .54\\) and \\(p = .22\\)). The red marker shows the model estimate, which should be at the highest point on the graph. The green circle illustrates where the Fleiss kappa solution would be. That assumes unbiased raters (see the chapter on the kappas). Here, there’s not much bias, so the Fleiss kappa estimate is close to the optimal one.\nIf the MCMC results have been generated, the log-likelihood plot will be augmented with an illustration of the confidence interval around the mean value.\n\n\n\nMCMC results\n\n\nUsing the advanced features, a Bayesian estimate for each of the parameters is created, which gives more insights into the convergence properties of the parameter estimates. The top plot in the figure above shows the shading for the middle 5%, 50%, and 95% of the distribution shown in the bottom plot. The bottom plot gives the smoothed density function of the draws from the MCMC exploration of the likelihood space for this parameter. We want to see a normal-like (mound shaped) density, as is the case here. Sometimes this density is pushed up against the edge at zero or one, or can even be bimodal. In those cases, it is probably better to use a mode rather than the mean value for the estimate.\nThe dashed line in the plot is the average log likelihood for each of the values. Generally we’d like to see the peaks coincide."
  },
  {
    "objectID": "kappa.html#footnotes",
    "href": "kappa.html#footnotes",
    "title": "Chapter 3: Kappa Statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot to be confused with the G-index used for ranking citation counts of scholars.↩︎"
  },
  {
    "objectID": "hierarchical.html#binary-truth",
    "href": "hierarchical.html#binary-truth",
    "title": "Chapter 4: Hierarchical Models",
    "section": "3.1 Binary Truth",
    "text": "3.1 Binary Truth\nSo far we have treated truth as a metaphysical property of each subject: we’ve assumed an unknown true classification for each subject. This assumption makes the general t-a-p model easy to analyze by using binomial mixtures because we only need to estimate the fraction of true Class 1 cases, not identify the true class of each case. Some binary mixture solvers do try to identify class membership, like Expectation-Maximization methods, and some Bayesian solvers can accommodate binary parameters, but Stan cannot.\nIn the derivation of the t-a-p model we used \\(T_i\\) to denote the true binary classification of subject \\(i\\), and \\(t\\) to denote the proportion of subjects that are Class 1, which is the average of all the \\(T_i\\) values. The \\(t\\) parameter became the mixture weight (a scalar) in the binomial mixture model. This presented no computational challenges, because each of the three parameters is a scalar in \\([0,1]\\), and the likelihood function is differentiable (smooth), which is a nice property for the solver algorithm to use to find the maximum likelihood.\nWith \\(N\\) subjects, there are \\(2^N\\) possible combinations of true class assignments, so exploring all combinations of them to find maximum likelihood is not feasible for even modestly sized \\(N\\). Moreover, the discrete nature of the binary truth parameters rules out solvers that depend on gradient descent, which maximize a function by exploring its smooth surface. This is a problem for many optimization algorithms (including Stan), which require smoothness to find a maximum value.\nThe combinatorial problem isn’t as bad as it looks for binomial mixtures. For a given number of raters and fixed \\(a\\) and \\(p\\) parameters, the likelihood function for the two class assignments is monotonic in the sense that there will be a cut point beyond which Class 1 is more likely than Class 0, and before the cut point Class 0 is more likely. The best guess for the true class for each subject depends only on the fraction of Class 1 ratings it receives, which is a much smaller dimension than \\(2^N\\). Intuitively, we could simply try all the cut points, optimize the \\(a\\) and \\(p\\) parameters for each choice, and identify the one with the maximum likelihood.\n\n\nCode\n#' Illustrate a t-a-p binomial mixture distribution of Class 1 Counts\na &lt;- .6\np &lt;- .3\npr_c0 &lt;- (1-a)*p \npr_c1 &lt;- a + pr_c0\n\ndistr &lt;- data.frame(N_c = 0:10) |&gt; \n           rowwise() |&gt; \n           mutate(Class0 = dbinom(N_c, 10, pr_c0),\n                  Class1 = dbinom(N_c, 10, pr_c1)) |&gt;\n           gather(Class, Probability, -N_c)\n\ndistr |&gt; \n  ggplot(aes(x = N_c, y = Probability, color = Class)) +\n  geom_point() +\n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = 0:10) +\n  xlab(\"Number of Class 1 Ratings out of 10\")\n\n\n\n\n\n\n\n\nFigure 2: Sample distribution of Class 1 ratings counts, with \\(a = .6\\) and \\(p=.3\\)\n\n\n\n\n\nThe plot in Figure 2 illustrates the cut point idea. The x-axis shows the range of possible agreement between 10 raters, from none of them choosing to rate as Class 1 for a given subject to unanimity among all ten that the subject is Class 1. The two curves show the probabilities for each of the true classes under the t-a-p assumptions. It’s more likely that true Class 1 cases will receive a majority of Class 1 ratings. In fact, true Class 1 cases are slightly more likely to receive four ratings of Class 1 than are true Class 0 cases (the blue marker is slightly higher than the red one at four on the axis). We could then set a rule that if a subject receives fewer than four ratings of Class 1, it is probably Class 0, and otherwise it’s probably Class 1. For any accuracy greater than zero there will be such a cut point.\nThis approach to classifying the true class of cases is necessarily crude, because we don’t have enough information to accurately assign a binary \\(t_i\\) to each case. However, the cut off method may be used after the three t-a-p parameters are estimated, to classify the cases for some use. For example a college may review portfolios of student writing and have three independent readers rate each portfolio as Pass or Fail. To reach a decision, it’s reasonable to pick a cut point, e.g. two of the three raters must rate Pass to accept the portfolio.\nAs an estimation method, the cut point process distorts the likelihood function due to the truncation where the two binomial distributions overlap.\n\n\nCode\n#' Illustrate a t-a-p binomial mixture distribution of Class 1 Counts\na &lt;- .4\np &lt;- .4\npr_c0 &lt;- (1-a)*p \npr_c1 &lt;- a + pr_c0\n\ndistr &lt;- data.frame(N_c = 0:10) |&gt; \n           rowwise() |&gt; \n           mutate(C0 = dbinom(N_c, 10, pr_c0),\n                  C1 = dbinom(N_c, 10, pr_c1),\n                  # apply the cut point \n                  C0_cp = if_else(C0 &gt; C1, C0 + C1, NA),\n                  C1_cp = if_else(C1 &gt; C0, C0 + C1, NA)) |&gt;\n           gather(Class, Probability, -N_c) |&gt; \n           separate(Class, into = c(\"Class\",\"Type\"), sep = \"_\") |&gt; \n           replace_na(list(Type = \"original\"))\n\ndistr |&gt; \n  ggplot(aes(x = N_c, y = Probability, color = Class, linetype = Type)) +\n  geom_point() +\n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = 0:10) +\n  xlab(\"Number of Class 1 Ratings out of 10\")\n\n\n\n\n\n\n\n\nFigure 3: Sample distribution of Class 1 ratings counts, with \\(a = .4\\) and \\(p=.4\\) after a cut point is applied at 4.\n\n\n\n\n\nThe distributions in Figure 3 show the distorting effect of applying a cut point at four ratings of Class 1 with \\(a = p = .4\\). The original distributions are shown in the dashed lines, and the cut point distributions are shown in the solid lines. The cut point process distorts the likelihood function due to the truncation where the two binomial distributions overlap. The cut point process is a form of data reduction, and it can be useful for some purposes, but it’s flawed way to estimate the true class of each case.\nThere are two ways to proceed here. If the use of the ratings is ultimately to apply a cut point to administratively determine the class of each subject, then we could modify the t-a-p model assumptions and solvers to optimize over the truncated distributions shown as solid lines in Figure 3. The problem will be to optimize over a discontinuous function, since the likelihood function will have a jump at the cut point. This is a difficult problem for most optimization algorithms, which depend on smoothness to find a maximum.\nThe other approach is to give up on the idea of estimating binary truth values and to allow the individual parameters to be probabilities instead. This preserves the smoothness of the likelihood function, and allows the use of gradient descent algorithms to find the maximum likelihood. One such method is to average out (marginalize) truth values over the ratings for a subject. That idea is outlined in Team (2022), section 1.10 and implemented in several algorithms found in Carpenter (2008). See Chapter 6: Machine Learning Methods for more on those."
  },
  {
    "objectID": "hierarchical.html#latent-truth",
    "href": "hierarchical.html#latent-truth",
    "title": "Chapter 4: Hierarchical Models",
    "section": "4.1 Latent Truth",
    "text": "4.1 Latent Truth\nSince likelihood is maximized for binary truth, we can take a middle ground and use scalar truth parameters, but use a transformation to nudge the values toward zero or one. One such approach is to use a threshold function, as is used in logistic regression, neural nets, and so forth. This corresponds to the idea from psychological measurement, where a “latent” variable is allowed to have real values, usually with a normal distribution centered at zero, but the value is not directly observed or measured. For example, we might imagine that college students have an abstract trait of “writing ability” that is approximately normally distributed over the universe of students. We then convert from the latent scale to a quasi-discrete one by applying a threshold function \\(\\tau\\) like the logistic function. For example, \\(\\tau(s) = 1/(1+e^{-\\lambda s})\\) has the property that \\(\\tau(0) = 1/2\\) , \\(\\tau(s) \\rightarrow 0\\) as \\(s \\rightarrow -\\infty\\) and \\(\\tau(s) \\rightarrow 1\\) as \\(s \\rightarrow \\infty\\). The steepness of the transition from zero to one (the thresholding) can be adjusted via the parameter \\(\\lambda\\), which can be used in conjunction with the scale (standard deviation) of the distribution used as a prior for the latent variable in a Bayesian analysis.\n\n\nCode\n#' plot the logistic function from [-5,5] with three different thresholding constants \n\ndf &lt;- expand.grid(x = seq(-5,5,.1), lambda = c(1,2,.5)) |&gt; \n      mutate(y = 1/(1+exp(-lambda*x)))\n\ndf |&gt;  ggplot(aes(x = x, y = y, color = as.factor(lambda))) +   \n       geom_line(linewidth = 1) +   \n       theme_bw() +   \n       xlab(\"s\") +   \n       ylab(\"tau(s)\") +   \n       scale_color_discrete(name = \"scaling\\nparameter\")\n\n\n\n\n\n\n\n\nFigure 5: The logistic function, \\(\\tau(s) = 1/(1+e^{-cs})\\), with \\(c = 1, 2, .5\\).\n\n\n\n\n\nUse of the threshold function (sometimes called a sigma function because it looks like an S) with a restrictive prior distribution constrains the domain of the latent variables for computational efficiency. This arrangement allows the truth parameters to take any real value, and the log likelihood function to be differentiable, but it approximates a discrete truth value because applying the transformation forces the values toward zero or one. The modification to the likelihood function is straightforward, with\n\\[ \\ell = -\\sum_{i,j} \\left[ C_{ij}\\log(\\tau_i(s)a_j + \\bar{a_j}p) + \\bar{C_{ij}}\\log(1 - \\tau_i(s)a_j - \\bar{a_j}p) \\right]. \\]\nHere, the \\(\\tau(s)\\) thresholding function has a scaling parameter applied to control the steepness. This approach is similar to item response theory or Rasch models. One advantage of this approach is that it’s possible to include predictor variables in the model, for example to estimate the effect of a rater’s experience (e.g. age, number of ratings) on their accuracy.\n\n\nCode\nrefresh = FALSE \n\nif(refresh){    \n     stan_model_source &lt;- system.file(\"stan/t_i-a-p soft_step.stan\", package = \"tapModel\")      \n     #' Compile it. This can take a while, but you only need to do it once.   \n     #' maybe go make a cup of tea   \n     stan_model &lt;- cmdstanr::cmdstan_model(stan_model_source)      \n     \n     #' Use the convenience function to run the MCMC   \n     #' This requires counts to be properly formatted.    \n     #' Use lambda = 30, the default   \n     model_params &lt;- tapModel::fit_random_tap_model(model_data, stan_model, output = \"params\")    \n     \n     avg_params &lt;- model_params |&gt;       \n       gather(param, value) |&gt;       \n       group_by(param) |&gt;       \n       summarize(avg = mean(value))      \n     \n     write_rds(avg_params, \"examples/evals/eval_params_step.rds\") \n} else {  \n  avg_params &lt;- read_rds(\"examples/evals/eval_params_step.rds\") \n}  \n\nap_params &lt;- avg_params |&gt;             \n  filter(param %in% c(\"a\",\"p\")) |&gt;             \n  spread(param, avg)            \n\ntitle &lt;- str_c(\"Estimates: a = \", round(ap_params$a,2), \" p = \",  round(ap_params$p,2))    \navg_params |&gt;    \n  filter(str_detect(param,\"^t\\\\[\")) |&gt;    \n  ggplot(aes(x = avg)) +   \n  geom_histogram(fill = \"steelblue\", color = \"black\", bins = 20) +   \n  theme_bw() +   \n  ggtitle(title) +   \n  xlab(\"Class 1 Probability\")\n\n\n\n\n\n\n\n\nFigure 6: Using a threshold function for subject truth values.\n\n\n\n\n\nWe could let the \\(\\lambda\\) parameter itself be fitted as part of the MCMC estimation process. In my limited attempts with this idea, there has not been enough selective pressure on the parameter to matter. This is a possible area of future research.\n##Expectation-Maximization\nThe Expectation-Maximization (EM) algorithm is a general method for estimating parameters in models with latent variables. The EM algorithm is a two-step process, where the E-step computes the expected value of unobserved parameters (“latent” variables) given current estimates for “fixed” parameters, and the M-step updates the parameter estimates given the expected values of the latent variables. In the case of individualized \\(t_i\\) parameters, we’d make some estimation of those with fixed values of \\(a\\) and \\(p\\), then treat the resulting \\(t_i\\) parameters as fixed and recompute the optimal \\(a\\) and \\(p\\) values. Then iterate until there’s some convergence. This avoids the problem of having to optimize all the parameters at once.\nThe EM algorithm is guaranteed to converge to a local maximum of the likelihood function, but it is not guaranteed to find the global maximum in general. The t-a-p model with individual truth values is a relatively uncomplicated problem, however.\nThe expression \\(ta + \\bar{a}p\\) in the likelihood function will be maximized when \\(t=1\\), whether or not a threshold function is used. The nature of the t-a-p model therefore prefers discrete truth values. This suggests a modified EM approach where we:\n\nE step. Compute the individual \\(t_i\\) truth parameters using the probabilistic approach, then round them to 0 or 1.\nM step. Compute the new fixed effects using the rounded random effects.\nRepeat until convergence.\n\nI have not yet tried this."
  },
  {
    "objectID": "hierarchical.html#footnotes",
    "href": "hierarchical.html#footnotes",
    "title": "Chapter 4: Hierarchical Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s tempting to convert the scale to display the bits of information per rating that are useful, which is one minus the bits of randomness, or \\(1 - \\log_{\\frac{1}{2}}x = \\log_2x + 1\\). This would correspond to the convention in single-parameter metrics like the kappas, where zero is the worst case and one is the best case. But the general idea of bits of log likelihood has utility beyond this special case, and what we consider random here may be useful information in other contexts (like communication theory). So I’ve stuck with log likelihood as bits of randomness, and one bit means complete randomness.↩︎"
  },
  {
    "objectID": "hierarchical-demo-1.html",
    "href": "hierarchical-demo-1.html",
    "title": "Demo: Hierarchical Truth",
    "section": "",
    "text": "1 Introduction\nIn Chapter 5 we posed the t-a-p model variation with individual \\(t_u\\) parameters for each combination of raters and Class 1 ratings \\((R_u,k_u)\\), with log-likelihood as\n\\[\n\\begin{aligned}\n\\ell(t,a,p) &= \\sum_{u = 1}^{\\text{unique}(R_i,k_i)}n_u \\left[ \\log \\left( t_u\\,\\text{binom}(R_u,k_u,a + \\bar{a}p) +  \\bar{t_u}\\,\\text{binom}(R_u,k_u,\\bar{a}p) \\right)  \\right],\\\\\n\\end{aligned}\n\\tag{1}\\]\nwhere \\(n_u\\) is the number of times the ratings combination \\((R_u,k_u)\\) was observed, and \\(t_u\\) is the expected average of true Class 1 cases within that group. The model is fit to the data by maximizing the likelihood function with the Stan script below.\n\n\nShow the code\n\n//\n//   Model specification for estimating t_u for each (N_r,N_c) type, with\n//   average a and p parameters. \n//\n// Learn more about model development with Stan at:\n//\n//    http://mc-stan.org/users/interfaces/rstan.html\n//    https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\n//\n\n// functions to make the code simpler below\nfunctions {\n  real p_true(real a, real s) {  // binomial probability for\n    return a + (1.0-a)*s;        // subjects that are class 1 in reality\n  }\n  real p_false(real a, real s) {  // binomial probability for\n    return (1.0-a)*s;            // subjects that are class 0 in reality\n  }\n\n}\n\n// The data provided from the ratings\ndata {\n  int&lt;lower=0&gt; N;              // number of rows of data\n  array[N] int&lt;lower=0&gt; N_r;   // number of raters for a count pair\n  array[N] int N_c;            // count of ratings of category 1 for count pair\n  array[N] int&lt;lower=0&gt; n;     // multiplicity of this (N_r, N_c) pair\n}\n\n// Parameters to estimate\nparameters {\n  real&lt;lower=0, upper = 1&gt; a;          // average accuracy \n  real&lt;lower=0, upper = 1&gt; p;          // rate of class 1 ratings when inaccurate\n  array[N] real&lt;lower=0, upper = 1&gt; t; // estimated true class 1 rate for each \n                                       // (N_r, N_c) pair\n}\n\n// We model the count of 1s (Class 1 ratings) by the binomial mixture described\n// in Chapter 5 at kappazoo.com. \n\n// cf http://modernstatisticalworkflow.blogspot.com/2016/10/finite-mixture-models-in-stan.html\nmodel {\n  a ~ uniform(0,1); // flat priors for all parameters, since we are on [0,1]\n  t ~ uniform(0,1);\n  p ~ uniform(0,1);\n\n  for(i in 1:N) {  // for each subject rated\n    target += n[i] * // multiplicity of the (N_r, N_c) pair\n              log_sum_exp( \n                 log(t[i])  + binomial_lpmf(N_c[i] | N_r[i], p_true(a,p)),\n                 log(1-t[i]) + binomial_lpmf(N_c[i] | N_r[i], p_false(a,p)));\n  }\n}\n\n\n\nWe can test this idea with simulated rates that have 200 subjects, 30 raters each, with parameters \\(t = .6\\), \\(a = .3\\), and \\(p = .6\\). We will fit the model to the data and compare the estimated \\(t_u\\) values to the values derived from the average t-a-p model and the rating counts, with\n\\[\n\\begin{aligned}\nPr[T_i = 1|N_r = R_u, N_c = k_u] &= \\frac{Pr[T_i = 1]Pr[N_c = k_u|T_i = 1, N_r = R_u]}{ Pr[N_c = k_u]} \\\\\n&= \\frac{t\\,\\text{Binom}(R_u,k_u,a+\\bar{a}p)}{t\\,\\text{Binom}(R_u,k_u,a+\\bar{a}p) + \\bar{t}\\,\\text{Binom}(R_u,k_u,\\bar{a}p)}.\n\\end{aligned}\n\\tag{2}\\]\n\n\nShow the code\n# simulation\nset.seed(123)\nratings &lt;- tapModel::generate_exact_ratings(N_s = 200, # subjects\n                                            N_r = 30,  # raters\n                                            t = .6, \n                                            a0 = .3, a1 = .3, # a = .3\n                                            p0 = .6, p1 = .6) # p = .6\n\ncounts &lt;- count_ratings(ratings, 1, summarize = TRUE)\n\n#' basic tap\nparams &lt;- tapModel::fit_counts(counts)\n\ntap_probs &lt;- tapModel::estimate_tu(counts, params)\n\n#' estimate t_u using max likelihood ##########################################\nprob_model_params &lt;- tapModel::fit_counts_mcmc(counts, stan_model = \"t_u-a-p\", quiet = TRUE)$params\n\nt_vals &lt;- prob_model_params |&gt;\n          filter(str_detect(var,\"t\\\\[\")) |&gt;\n          pull(mode1)\n\ntap_probs$t_i_mod &lt;- t_vals\n\n# plot ##################################################\ntap_probs |&gt;\n  select(k = N_c, n, t_i = t_u, t_i_mod) |&gt;\n  gather(var, value, -k, -n) |&gt;\n  ggplot(aes(x = k, y = value, color = var)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  ylab(\"Probability\") +\n  xlab(\"Number of Class 1 Ratings\")\n\n\n\n\n\n\n\n\nFigure 1: Conditional probability of a subject being true Class 1 given a number of Class 1 ratings from 30 raters. The comparison shows model-derived values (red) and computed values with t_u parameters (blue).\n\n\n\n\n\nThe red curve in Figure 1 is the expected values for \\(t_i\\) that are derived from Equation 2, which traces a smooth sigmoid. If the count of Class 1 ratings is \\(k&lt;14\\), the subject is almost certainly Class 0 in reality. If the count is \\(k&gt;20\\), the subject is almost certainly Class 1. Between those values, there’s a smooth transition. For example, with 17 Class 1 ratings, half of the subjects are expected to be in each class.\nThe blue curve shows the estimated \\(t_i\\) values from the inferential computation based on Equation 1, as implemented in the Stan code. In the middle of the plot, the transition is more like a step function than a sigmoid. This is a consequence of the likelihood maximization, which favors values of \\(t_u\\) that are close to zero or one. To see that, consider one of the unique ratings combinations \\((R_u,k_u)\\). The likelihood function is\n\\[\nt_u\\,\\text{binom}(R_u,k_u,a + \\bar{a}p) +  \\bar{t_u}\\,\\text{binom}(R_u,k_u,\\bar{a}p).\n\\tag{3}\\]\nUnless the binomial probabilities are each .5, one of them will be larger than the other. Suppose that the calculation is \\(.6 t_u + .4 \\bar{t_u}\\). Since \\(t_u + \\bar{t_u}= 1\\), the likelihood is maximized when \\(t_u = 1\\) and \\(\\bar{t_u} = 0\\). More generally, likelihood will be optimized by setting \\(t_u\\) to 1 when the binomial probability on the left (the case when the true class is 1) is greater than .5, and to 0 when it is less than .5. This is why the blue curve is more like a step function than a sigmoid. When the binomial probabilities in Equation 3 are equal to .5, the posterior distribution for \\(t_u\\) is bimodal, with modes near zero and one.\nThe left and right tails of the blue curve in Figure 1 are biased away from their true values (e.g. on the right it curves down, away from 1). This is due to a limitation in estimating parameters with the Stan model that defines the parameters on \\([0,1]\\). The parameter values are taken from the modes of the posterior distributions to mitigate this problem (using averages is much worse), but it’s still evident here. One approach to this problem is to use a variable transformation, as will be taken up in the Chapter 5 discussion of Bayesian methods (not currently complete).\nThe main conclusion from this exercise is that we are better off using the model’s expected values for \\(t_u\\), which are used to assign each subject a \\(t_i\\) value, rather than allowing \\(t_u\\) to be free parameters in the model. This implies a two-step process for hierarchical models, where we first estimate the average t-a-p parameters, use those to estimate the \\(t_u\\) values, and then consider other parameters as desired. That approach can be thought of as a single step in an Expectation-Maximization algorithm."
  },
  {
    "objectID": "hierarchical.html#uniform-raters",
    "href": "hierarchical.html#uniform-raters",
    "title": "Chapter 4: Hierarchical Models",
    "section": "6.1 Uniform Raters",
    "text": "6.1 Uniform Raters\nWe can start with a data set where all the raters and subjects have the same average parameters, subject only to sampling error. Ideally the model comparison will tell us that adding parameters for raters is unnecessary. To test, we’ll use a fairly large rating set to minimize the effect of sampling error. The data set is generated with 500 subjects and 20 raters each, with \\(t = .5\\), \\(a = .7\\), and \\(p = .4\\).\n\n\nShow the code\nratings &lt;- tapModel::generate_sample_ratings(N_s = 500, N_r = 20,\n                                             t = .5, a0 = .7, a1 = .7,\n                                             p0 = .4, p1 = .4)\n\nparams &lt;- tapModel::iterative_optim(as_counts(ratings))\n\nparams |&gt; select(t, a, p) |&gt; kable(digits = 2)\n\n\n\n\nTable 3: Model fit for uniform raters, showing the fitted parameters for the average t-a-p model with simulated ratings where t = .5, a = .7, and p = .4.\n\n\n\n\n\n\nt\na\np\n\n\n\n\n0.49\n0.7\n0.41\n\n\n\n\n\n\n\n\nThe model estimates are good in Table 3, so we can proceed to the next step of inducing the individual parameters for raters and subjects.\n\n\nShow the code\nm &lt;- tapModel::hierarchical_tap_model(ratings, mcmc = FALSE)\n\nm$rater_params |&gt; \n  filter(rater_id &lt; 11) |&gt; \n  select(rater_id, a = a_re, p = p_re) |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 4: Model fit for uniform raters, showing the first ten fitted rater parameters for the individual t-a-p model with simulated ratings where t = .5, a = .7, and p = .4.\n\n\n\n\n\n\nrater_id\na\np\n\n\n\n\n1\n0.72\n0.44\n\n\n2\n0.73\n0.31\n\n\n3\n0.73\n0.34\n\n\n4\n0.76\n0.36\n\n\n5\n0.70\n0.49\n\n\n6\n0.62\n0.42\n\n\n7\n0.70\n0.35\n\n\n8\n0.70\n0.49\n\n\n9\n0.71\n0.52\n\n\n10\n0.73\n0.36\n\n\n\n\n\n\n\n\nThe last half of the raters in Table 4 are left off to save space, but the table is representative of the correspondance between the average and individual parameters. The only difference should be the sampling error when each rater generates 500 ratings. The coefficients in the table are close to the average values of .7 and .4, respectively. For real data, we can’t do this test, of course, because we don’t have the exact parameter values.\n\n\nShow the code\nm$plot\n\n\n\n\n\n\n\n\nFigure 2: Model fit for uniform raters, showing the simulated distribution of bits per rating for the average and individual (random effects, or ‘re’) t-a-p models with simulated ratings where t = .5, a = .7, and p = .4. The red line shows the bits per rating for the average model, and the blue line shows the bits per rating for the individual rating model. These values are given in the title.\n\n\n\n\n\nThe model fit in units of randomness is shown in Figure 2. The horizontal scale for model fit is bits/rating as discussed in the previous section. For both the average (three-parameter) model and the rating model, 1000 simulated ratings sets are generated from the proposed parameters, and the model fit recomputed. The density plots for these two simulations are shown, with red being the “random effects” model with individual rater parameters, and blue being the “fixed effects” model with average t-a-p parameters. These two densities overlap so much in the plot that they are nearly indistinguishable. This suggests that switching to an individual rater model is not necessary for this data set, and probably overfits the data.\nThe vertical dashed lines, which also overlap, show the model fit for the given data set (not the simulated ones) at .61 bits per rating. Its placement in the density plot shows that the data set is well within the range of randomness for the two models, which is an indication of reasonableness.\nIf we did not know the source of this ratings data set, we could conclude that the three-parameter model is the right one to use, and that there is evidence that the ratings are non-random, with 1-.61 = .39 bits of useful information per rating."
  },
  {
    "objectID": "hierarchical.html#ptsd-data",
    "href": "hierarchical.html#ptsd-data",
    "title": "Chapter 4: Hierarchical Models",
    "section": "6.2 PTSD Data",
    "text": "6.2 PTSD Data\nWe can now apply the same process to the PTSD data set, which has unknown true parameter values. We already generated parameter estimates for raters in Table 2, so we can skip to step 4 in the program outlined above.\n\n\nShow the code\ndata(ptsd)\n# convert Q2 to long form\nratings &lt;- ptsd |&gt;\n  filter(Question == \"Q2\") |&gt;\n  select(-Question) |&gt;\n  gather(RaterID, Rating, -SubjectID) |&gt;\n  mutate(rater_id = as.integer(str_extract(RaterID,\"\\\\d+\")),\n         rating  = Rating - 1) |&gt;\n  select(subject_id = SubjectID, rating, rater_id)\n\nm &lt;- tapModel::hierarchical_tap_model(ratings, mcmc = FALSE)\n\nm$plot\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nUnlike the case in Figure 2, the model fit for the PTSD data set in ?@tbl-ptsd1 shows that the individual rater model is a much better fit than the average model. The bits per rating for the individual model is .46, while the average model is .57. The data set is closer to the individual model than the average model, which suggests that the individual model is a better fit. In both cases, the dashed line showing the bits/rating for the actual data is considerably to the left of the mean of the distribution of simulated ratings. That suggests that both models are overfitting the data, but the individual (random effects/hierarchical) model is still an attractive alternative to the average model.\nThe justification for an individual parameter model here is solid, since the raters are individual people, and likely to have varying opinions about the protocols in relation to evidence. We would expect some raters to be more accurate than others.\nTo test the robustness of the individual model fit, we can simulate new data sets from the estimated parameters and then see how well we can recover the parameters. This is computationally expensive for large data sets, so I’ll only run 100 of these.\n\n\nShow the code\nrater_params &lt;- m$rater_params |&gt; \n                select(rater_id, a = a_re, p = p_re)\n\nsubject_params &lt;- m$subject_params |&gt; \n                  select(subject_id, t = t_re)\n\nsim_params &lt;- tapModel::rater_parameter_error(ratings, rater_params, subject_params, n_sim = 100)\n\nrater_order &lt;- sim_params |&gt; \n  group_by(rater_id) |&gt; \n  summarize(a= mean(a_sim)) |&gt; \n  pull(a)\n\nsim_params |&gt; \n  left_join(rater_params, by = \"rater_id\") |&gt;\n  mutate(rater_id = factor(rater_id, levels = order(rater_order))) |&gt; \n  ggplot(aes(x = a_sim, y = rater_id, group = rater_id)) +\n  geom_boxplot() +\n  geom_point(aes(x = a), color = \"red\") +\n  theme_bw() +\n  xlab(\"Simulated Rater Accuracy\")\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\nThe results in Figure 4 show that the individual rater parameters are well recovered from the simulated data sets. The red markers show the proposed accuracy estimates for each rater, and the boxplots show the distribution of recovered values after simulating new ratings and recovering the parameters. We can do the same for the \\(p_j\\) parameters, and the results are similarly accurate.\nOne limitation of this approach is that the truth parameters \\(t_i\\) are assumed fixed throughout. We can relax that condition by using Bayesian methods. At the least, this “round trip” through simulation and parameter estimation shows that the model is self-consistent on the given data set. The estimated parameters are probably slightly over-fit to the data, given the vertical placement of the actual data set in Figure 3 (blue dashed line relative to the blue density plot). This could potentially be addressed by using a form of regularization, to simulataneously estimate an average \\(a\\) an \\(a_j\\) offsets to that average. This requires more sophistication and Bayesian models.\nThe evidence from the hierarchical model suggests that raters 1, 4, and 5 are significantly less accurate than the others. Such information could be useful for researchers and practitioners to improve rater accuracy."
  },
  {
    "objectID": "hierarchical.html#likelihood-as-information",
    "href": "hierarchical.html#likelihood-as-information",
    "title": "Chapter 5: Hierarchical Models",
    "section": "5.1 Likelihood as Information",
    "text": "5.1 Likelihood as Information\nSince the ratings are binary, we can use ideas from information theory to make sense of the log likelihood. From Shannon (1948), the amount of information in a binary data stream is \\(I = x \\log_{\\frac{1}{2} } x\\) per event with probability \\(x\\), where the information is measured in bits (binary digits). If \\(x = 1/2\\) is the fraction of 1s in the data stream, then \\(I = \\log_{\\frac{1}{2} } 1/2= 1\\) bit per symbol sent.\n\n\n\n\n\n\nNote\n\n\n\nI will use the convention that \\(\\log x\\) means \\(\\log_{\\frac{1}{2} } x\\), without specifying the base each time. This reads oddly for those used to integer log bases, since the log of any probability with base greater than one is negative. Hence there are a lot of expressions that start with negative signs in the usual likelihood formulations. The advantage of log base 1/2 is that we don’t need the negative signs to get a result in bits. Another important difference is that when we maximize likelihood, we minimize the number of informational bits per rating (again, because of the lack of a minus sign).\n\n\nFor a t-a-p model, the average rate of Class 1 assignments is \\(c = ta + \\bar{a}p\\), and heuristically the expected information per rating is hence \\(I_{tap} = c \\log c + \\bar{c} \\log \\bar{c}\\). If all the ratings are of the same type (Class 1, say), the expression becomes zero; no information can be transmitted if the signal never changes. The maximum information rate is when \\(c = 1/2\\), which is \\(I_{tap} = 1\\) bit per rating.\nThe information \\(I_{tap}\\) has an approximate relationship to likelihood, which is the probability of seeing the provided data set under the model assumptions. To see that, consider the accumulation of log likelihood \\(\\ell\\) by comparing the actual ratings to their probabilities under a given t-a-p model, with a sum over each classification \\(\\hat{C}_{ij}\\) by rater \\(j\\) of subject \\(i\\). From Equation 3 we have\n\\[\n\\begin{aligned}\n\\ell_{\\text{rating}}(\\text{ratings}|\\text{model}) &=  \\log \\prod_{i,j} Pr[C_{ij} = \\hat{C}_{ij}] \\\\\n&= \\sum_{i,j} \\log Pr[C_{ij} = \\hat{C}_{ij}] \\\\\n&= \\sum_{\\hat{C}_{ij}= 1} \\log c+ \\sum_{\\hat{C}_{ij}= 0}  \\log \\bar{c}\\\\\n& \\approx N(c \\log c + \\bar{c} \\log \\bar{c}) \\\\\n&= N I_{tap}.\n\\end{aligned}\n\\tag{5}\\]\nHere, the probability \\(Pr[C_{ij} = \\hat{C}_{ij}]\\) in the fitted t-a-p model is \\(c = ta + \\bar{a}p\\) when raters assign \\(\\hat{C}_{ij} = 1\\) to a subject, and and \\(\\bar{c}\\) for \\(\\hat{C}_{ij} = 0\\). \\(N\\) is the number of ratings. If the model represents the data, the fraction of \\(\\hat{C}_{ij}= 1\\) is \\(c\\), and there will be approximately \\(Nc\\) ratings of Class 1. From Equation 5 it’s intuitive to compare t-a-p models on the common scale \\(I_{tap}\\), bits of information per rating. This isn’t technically correct, because unless the ratings are completely random, correlations within subjects interfere with the information stream analogy. We can see this most directly with the likelihood from Equation 1, where only counts per subject matter. For example, if three out of four raters assign a Class 1 rating with probability \\(c = 1/2\\), the log likelihood for that subject is\n\\[\n\\begin{aligned}\n\\ell &= \\log \\binom{4}{1}c^3\\bar{c} \\\\\n&= \\log 4 + 4 \\log \\frac{1}{2} \\\\\n&= -2 + 4 = 2 \\text{ bits}\n\\end{aligned}\n\\]\nThe within-subject variation absorbs half of the four bits that the four ratings nominally provide. A similar effect is present in calculations from Equation 3. If one subject has 1000 raters and another subject has 5, the information of the 1000th rating of the first subject is intuitively less informative than the fifth rating from the second. Similarly, a rating from a rater with accuracy zero isn’t as informative as a rater with accuracy one. Accordingly, for hierarchical models we could consider weighting ratings according to such factors.\n\n\n\n\n\n\nResearch Project\n\n\n\nHow might we modify Equation 3 to assign weights to ratings according to the relative amount of information provided?\n\n\nIn practice, we compute \\(\\ell_{\\text{rating}}(\\cdot)\\) from Equation 3, and then divide by the number of ratings to get the bits per rating to get \\(I_{tap}\\). This can be done even for the subject-likelihood average t-a-p models computed from Equation 1 by assigning each of the (possibly anonymous) raters the same average accuracy and random assignment parameters."
  },
  {
    "objectID": "hierarchical.html#signal-and-noise",
    "href": "hierarchical.html#signal-and-noise",
    "title": "Chapter 5: Hierarchical Models",
    "section": "5.2 Signal and Noise",
    "text": "5.2 Signal and Noise\nIn addition to splitting the ratings by the rating of 0 or 1 as in Equation 5, we could also split them by their accuracy. With probability \\(a\\) a rating will be accurate, in which case we have an information stream with characteristics determined by \\(t\\). In the complementary case, it’s determined by \\(p\\). Then we can sum over the ratings to extract two kinds of information with\n\\[\n\\begin{aligned}\nI_{\\text{signal}} &= \\frac{1}{N} \\sum_{i,j}^N a_{ij}(t_i \\log t_i + \\bar{t}_i \\log \\bar{t}_i) \\\\\nI_{\\text{noise}} &= \\frac{1}{N} \\sum_{i,j}^N \\bar{a}_{ij}(p_{ij} \\log p_{ij} + \\bar{p}_{ij}\\log \\bar{p}_{ij}).\n\\end{aligned}\n\\]\nNote that for unbiased raters (\\(t=p= c\\)), \\(I_{\\text{signal}} + I_{\\text{noise}} = c \\log c + \\bar{c} \\log \\bar{c} = I_{tap}\\), but in general the sum of signal and noise won’t equal the average likelihood \\(I_{tap}\\). The information per bit of two communications channels is not generally additive when we combine them. As an example, a channel of all zeros and a channel of all ones each have zero information, but if we combine the two the result will be positive.\nThe names “signal” and “noise” are borrowed from communication theory, but they can be misunderstood here. The maximum signal occurs when raters are perfectly accurate and samples are split evenly between true Class 1 and Class 0 cases. It’s not always possible or desirable to have evenly balanced classes, for example with a medical test that detects a rare condition. Such unbalanced data sets are common in ratings (see Chapter 4: The Kappa Paradox)."
  },
  {
    "objectID": "hierarchical.html#example-uniform-raters",
    "href": "hierarchical.html#example-uniform-raters",
    "title": "Chapter 4: Hierarchical Models",
    "section": "6.1 Example: Uniform Raters",
    "text": "6.1 Example: Uniform Raters\nAs a simple illustration of model assessment, I’ll start with a simulated data set where all the raters and subjects have the same average parameters, subject only to sampling error. The data set is generated with 600 subjects and 10 raters each (the same as the ptsd data, which we’ll consider next), with \\(t = .5\\), \\(a = .7\\), and \\(p = .2\\). The raters are significantly biased away from Class 1. One purpose of this example is to inspire some confidence in the method, since we already know the parameter values. Another purpose is to show how we can investigate a parameter set by simulating values from it to make sure we can recover them. This can be done, for example, after estimating parameters from a real rating set.\n\n6.1.1 Step 1: Are estimated parameters reasonable?\nHere, we know the real values of the parameters, so our expectations are exact. We can compute the three-parameter model with the functions provided in the tapModel package.\n\n\nShow the code\nset.seed(1234)\n\nratings &lt;- tapModel::generate_sample_ratings(N_s = 200, N_r = 10,\n                                             params = list(t = .5, a = .65, p = .8),\n                                             details = TRUE)\n\nparams &lt;- tapModel::fit_counts(as_counts(ratings))\n\nparams |&gt; select(t, a, p) |&gt; kable(digits = 2)\n\n\n\n\nTable 3: Model fit for 20 uniform raters on 200 subjects, showing the fitted parameters for the average t-a-p model with simulated ratings where t = .5, a = .65, and p = .8.\n\n\n\n\n\n\nt\na\np\n\n\n\n\n0.51\n0.66\n0.8\n\n\n\n\n\n\n\n\nThe parameters don’t look degenerate (close to zero or one).\n\n\n6.1.2 Step 2. Where is the DK Horizon?\nThe next step is to find the Dunning-Kruger threshold for this data configuration. Recall that this is a reframing of the null hypothesis \\(p\\)-value to emphasize ignorance.\n\n\nShow the code\ntapModel::simulate_exact_fit(200,10,n_sim = 50, output = \"plot\")\n\n\n\n\n\n\n\n\nFigure 2: Ranges of accuracy estimates for nominal t = p = .5 samples.\n\n\n\n\n\nThe boxplots in Figure 2 show that the DK-horizon is low, maybe around \\(a=.15\\) as the point past which we can’t trust accuracy estimates. If we want, we can put numbers to this by simulating \\(a=0\\) to see how large the accuracy estimates can be from sampling error.\n\n\nShow the code\ndk &lt;- dk_horizon(200, 10, n_sim = 200)\ndk |&gt; \n  as.tibble() |&gt;  \n  mutate(Threshold = names(dk)) |&gt; \n  select(Threshold , `max(a_est)` = value) |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 4: Estimated percentiles for accuracy, given 600 subjects with ten raters each, when accuracy is actually zero.\n\n\n\n\n\n\nThreshold\nmax(a_est)\n\n\n\n\n50%\n0.00\n\n\n75%\n0.08\n\n\n90%\n0.11\n\n\n95%\n0.13\n\n\n98%\n0.15\n\n\n\n\n\n\n\n\nFor a data set of this size, 95% of the randomly generated ratings resulted in an accuracy parameter estimate of .15 or less. It’s reasonable to be suspicious of parameter estimates that have \\(a\\) below that threshold. We might be being fooled by sampling and estimation error into thinking the raters are much more accurate than they actually are. In this case, with an estimate of about .65 (the true value), we easily pass the DK-threshold test.\n\n\n6.1.3 Step 3. Do we need a hierarchical model?\nNext we assess the likelihood in units of bits/rating to compare the three-parameter model to the more detailed hierarchical model.\n\n\nShow the code\nrating_params &lt;- tapModel::fit_ratings(ratings)\n\ntapModel::model_fit_comparison(rating_params, model_names = c(\"hierarchical\",\"average\"))$plot\n\n\n\n\n\n\n\n\nFigure 3: Model fit for uniform raters, showing the simulated distribution of bits per rating for the average and hierarchical t-a-p models with simulated ratings where t = .5, a = .7, and p = .2. The vertical lines shows the bits per rating for each model with the original data set, and the densities are for data sets simulated from the respective parameters.\n\n\n\n\n\nThe model fit in Figure 3 shows overlapping distributions of log likelihoods for the hierarchical and average (three-parameter) t-a-p models. Recall that better fit for a model means a smaller value of bits/rating (hence larger likelihood). Here there’s no difference between the two models, so we can conclude that the hierarchical model is adding parameters with no benefit. In this case, we would choose the average three-parameter t-a-p model as the most parsimonious description of the data. Because this is simulated data, we know that’s the correct conclusion.\n\n\n6.1.4 Summary\nFor this example, we already knew the paramete specification, and the goodness-of-fit process confirmed that the estimate was probably trustable. Moreover, it confirmed that there’s no advantage to choosing a more complicated hierarchical model.\nI chose the sample size and t-a-p parameters for this example because they also correspond to the PTSD data’s specifications. We’ll now turn to that real data set."
  },
  {
    "objectID": "hierarchical.html#example-ptsd-data",
    "href": "hierarchical.html#example-ptsd-data",
    "title": "Chapter 4: Hierarchical Models",
    "section": "6.2 Example: PTSD Data",
    "text": "6.2 Example: PTSD Data\nThe PTSD data set has unknown true parameter values. This is not my research data, so I don’t have intuition about what reasonable answers might look like.\n\n6.2.1 Step 1. Estimate t-a-p parameters\nThe first step is to estimate the average t-a-p parameters and compare to the DK-threshold, which we already calculated for the uniform raters in Table 4.\n\n\nShow the code\ndata(ptsd)\n\n# convert Q2 to long form\nratings &lt;- ptsd |&gt;\n  filter(Question == \"Q2\") |&gt;\n  select(-Question) |&gt;\n  gather(RaterID, Rating, -SubjectID) |&gt;\n  mutate(rater_id = as.integer(str_extract(RaterID,\"\\\\d+\")),\n         rating  = Rating - 1) |&gt;\n  select(subject_id = SubjectID, rating, rater_id)\n\nparams &lt;- tapModel::fit_counts(as_counts(ratings))\n\nparams |&gt; select(t, a, p) |&gt; kable(digits = 2)\n\n\n\n\nTable 5: t-a-p model fit the PTSD data.\n\n\n\n\n\n\nt\na\np\n\n\n\n\n0.54\n0.65\n0.81\n\n\n\n\n\n\n\n\nWe already calculated the DK-horizon (step 2) in the previous example. The accuracy estimate of \\(a=.65\\) is signficantly higher than the DK-threshold of .15 that we saw in Table 4.\n\n\n6.2.2 Step 3. Do we need a hierarchical model?\nIf we include parameters for subjects and raters, does model fit improve?\n\n\nShow the code\n# generate individual parameters for subjects, raters, ratings\nrating_params &lt;- tapModel::fit_ratings(ratings)\n\nset.seed(123)\ntapModel::model_fit_comparison(rating_params, model_names = c(\"hierarchical\",\"average\"))$plot\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\nUnlike the case in Figure 3, the model fit for the PTSD data set in Figure 4 shows that the hierarchical rater model is a much better fit than the average model. The bits per rating for the individual model is .46, while the average model is .57. Raters probably vary in their accuracy and bias since they are classifying complex evidence. If we had the original data we could examine in detail the classifications made by the raters with lowest and highest accuracy, for example by getting more ratings from a few experts. This kind of information could be useful in improving classifications generally.\nIn both distributions in Figure 4, the dashed line showing the bits/rating for the actual data is somewhat to the left of the mean of the distribution of the bits/rating from the simulated ratings. The parameters are generated from the given data, so we’d expect that the original data set would be a somewhat better fit for the model than data that’s randomly generated from the parameters. That’s the case here, but this “overfitting” subjectively doesn’t look too bad. It would be different if the dashed blue line were way off to the left of the distribution. That would indicate a degree of overfitting (or other problem) that we should follow up on.\n\n\n6.2.3 Step 4: Can we recover the parameters from simulations?\nTo test the robustness of the individual model fit, we can simulate new data sets from the hierarchcial parameters and then see how well we can recover the parameters. This is computationally expensive for large data sets, so I’ll only run 50 of these.\n\n\nShow the code\nset.seed(123)\nerror_est &lt;- estimate_rater_parameter_error(rating_params,n_sim = 50)\nerror_est$plot_a\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nThe results in Figure 5 show that the individual rater parameters are well recovered from the simulated data sets. The red markers show the proposed accuracy estimates for each rater, and the boxplots show the distribution of recovered values after simulating new ratings and recovering the parameters. We can do the same for the \\(p_j\\) parameters.\n\n\nShow the code\nerror_est$plot_p\n\n\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\nThe plots in Figure 6 show that the estimates are robust, and that raters 1,4,5, and 10 are quite biased toward assigning Class 1. In practice, this feedback can likely be used to improve ratings.\n\n\n6.2.4 Summary\nThe above analysis shows what the PTSD ratings have impressive accuracy if the model assumptions are met (primarily independence of raters). A hierarchical model is called for, and it shows that the raters have a range of characteristics that might be useful in improving classifications in real-world settings. In general, the PTSD data shows that we can extract much more meaningful information from the ratings than is possible with the kappa statistics. Note, however, that we only get the full benefit of these new methods if we have rater identification for each rating."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Untitled",
    "section": "",
    "text": "Code\nflowchart LR\n  A{raw_ratings} --&gt;|format_raw_ratings|B(ratings)\n  B --&gt; |as_counts|D[counts]\n  D --&gt; |fit_counts|F[params]\n  B --&gt; |fit_ratings|E(rating_params)\n\n\n\n\n\n\nflowchart LR\n  A{raw_ratings} --&gt;|format_raw_ratings|B(ratings)\n  B --&gt; |as_counts|D[counts]\n  D --&gt; |fit_counts|F[params]\n  B --&gt; |fit_ratings|E(rating_params)\n\n\n\n\nFigure 1: Data flow diagram for ratings"
  },
  {
    "objectID": "fit.html",
    "href": "fit.html",
    "title": "Chapter 6: Assessing Model Fit",
    "section": "",
    "text": "This chapter discusses general ideas about avoiding errors in analysis and then moves on to specific tools for inter-rater statistics. My general remarks are opinionated, and should be taken as a point of view, not a definitive way to think about analysis. Seek other opinions. You can skip the introduction if you want to get straight to the methods. You’ll see that we have much more analytical ability if the rater data has rater IDs. The PTSD data set is used to illustrate methods to assess model fit and error estimation.\n\n\nThere are a lot of ways that data analysis can lead us to the wrong conclusion. In analysis I did of a private college’s graduation rates, I discovered that students who got no financial aid from the college were much more likely to drop out. I confidently wrote a proposal showing that increasing financial aid awards judiciously would pay for itself and lead to higher graduation rates, so everyone wins. The enrollment VP interrupted my enthusiastic presentation to tell me that when a students leave, the financial aid office zeros the aid amount in the database after the fact. The reverse causality obviously invalidated my findings. The point of this story is that we should think about the validity of results more broadly than just the statistics we obtain from the data. For a practical treatment of this attitude, see The Scout Mindset by Julia Galef.\nMuch of the statistics taught in 101 classes concerns hypothesis testing, with \\(p\\)-values and so on, that address problems like\n\\[\nPr(\\text{our data}|\\text{bad inputs}),\n\\]\nby which I mean the probability that we are seeing this data set because something went wrong with measurement or data preparation, etc. In this case, the statistical properties of the data are irrelevant. When I was young I made the mistake of browsing my uncle’s medical reference books and discovered that I had symptoms for a number of dreaded diseases. If confusion is a reliable symptom of a stroke, then Pr(confusion|stroke) is fairly high, but for a healthy 14-year-old Pr(stroke|confusion) fortunately is not! If we estimate Pr(stroke|confusion), we should have an idea about Pr(stroke) before we get too excited. Since we probably can’t estimate Pr(bad inputs) in the expression called out above, this isn’t a statistical question as much as a mindset and a judgment call. As a precautionary meausure, we can entertain the possibility that our rating data is merely coin-flips (i.e. “had a stroke”) and compute how likely our data set is under those circumstances. This is equivalent to a zero-accuracy t-a-p model\n\\[\nPr(\\text{ratings data}|t= 1/2, a = 0, p = 1/2).\n\\] This computation gives a lower threshold for accuracy, which I called the Dunning-Kruger horizon. If the estimated t-a-p parameters from the data are safely above this, we can conclude that the chance our data “had a stroke” and is equivalent to coin flips is low. There still might be data integrity issues, but they are not catastrophic. Another kind of analysis is more direct, asking\n\\[\nPr(\\text{erroneous conclusion}|\\text{data and analysis}).\n\\] Such questions can be cast within the set of statistical assumptions we work with to estimate model parameters, like independence of samples and the shape of the probability distribution (a binomial mixture in the case of binary ratings). If we estimate average rater accuracy \\(a = .3\\), we can then ask for a reasonable range of nearby values that might be true given the natural variability of the data. In Bayesian statistics these are called “credible Intervals” to contrast them with “confidence intervals.” We might find that while the average is \\(a=.3\\), there’s a good chance that \\(a&lt;.2\\), which is important to know when conveying results.\nThe surveying profession has a long tradition of classifying errors into types. One description gives the categories of error as personal, instrumental, and natural. I’ll quote from that source in the sections below.\n\n\n\nError may also arise due to want of perfection of human sight in observing and of touch in manipulating instruments. For example, an error may be there in taking the level reading or reading and angle on the circle of a theodolite. Such errors are known as personal errors.\n\nGathering and preparing data for analysis is fraught with potential for mistakes. We might get columns mixed up, treat blanks incorrectly, use an outdated version of a file, misinterpret a definition, have a bug in our code, and so on. It’s common to get data with column headers that don’t mean what they seem to mean, or have exceptions. Like a column for grade point averages that has zeros instead of blanks for missing data. A famous example of an analysis that went wrong is the Excel spreadsheet error in an influential economics article, where a blunder led to bad conclusions.\nIn addition to outright blunders, we can get the wrong answer because we want to. We often prefer one answer over another when we do an analysis. It’s nice to get a clear result that will give us that glow of understanding the world and maybe a pay raise. It’s less fun to find that the analysis isn’t useful and we wasted our time. So we might overlook an inquiry that could cast doubt on a “good” result, but pursue it vigorously for a “bad” one. This is a subtle version of [wireheading](https://en.wikipedia.org/wiki/Wirehead_(science_fiction), taking a shortcut to rewards. Galef’s book, mentioned earlier, is one of a genre of books addressing such cognitive biases. Also see Think Again by Adam Grant, Predictably Irrational by Dan Ariely, and Thinking fast and slow by Daniel Kahneman.\nEven if we avoid the reward shortcuts and other biases in our own work, we still have to deal with biases in the people who will use our results. Benn Stancil has a useful observation about this on his substack\n\nSome decisions are made relatively slowly, with intention, and based on reasoned analysis. But most decisions are made quickly and instinctively, based on heuristics, experience, and some rough estimation of how the company works.\n\nHe argues that analysts mostly produce technical work and stop there, when we should be trying to affect the heuristics of decision-making. To be more effective, analysts should try to update the mental maps of how things work. As an example, in private colleges the “discount rate” is the fraction of tuition that waived upon entry, and it hovers around 50% these days. Over the last decades increasing discount rates are seen as fiscally irresponsible, and the subject of much board angst. However, it’s easy to show that discount rates must increase as a consequence of the business model. There’s a persistant gap here between how things work and the perception of how things work, which causes bad decisions to be made. It’s purely cultural, a baked-in bias about the world that so prevelant that it’s self-perpetuating. That’s the kind of thing that analysts are up against, and it’s ubiquitous.\nIt can be uncomfortable to make the leap from a statistical estimate to a mental map, because it’s no longer deterministic. But in fact, our statistical analysis isn’t deterministic either; we make lots of subjective choices in the process. We’re going to advertise the limitations of the study as the lawyerly fine print. Use that license to venture beyond the statistics into meaning. Darwin (and his advocates) established evolution without knowing what mechanism caused it (genetic heritability).\nMy recommendation is to approach the personal error problem with these strategies. First, we must protect our work from blunders as much as possible. Fully document work as you do it by using quarto documents) or something similar to combine code and text in a transparent and reproducible form. Take pride in the presentation of the work, even if it’s just for yourself, because that attention to detail carries over to the analysis itself. Write the report for yourself in five years. I like to include an appendix with a description of the analytical method and related statistics, like how to intepret logistic regression coefficients. When coding, I like R’s package documentation for writing functions, and use it even when I’m not writing a package, because the declaration of parameters and results at the top of the function definition forces me to think through what I’m doing before I write the code. Write unit tests when it’s likely to prevent bad things. In addition to the analytical report, create an audit report that summarizes the data in various ways, so you can quickly see if something’s amiss.\nAll of this seems like extra work compared to the way I used to do things. I’d start in Excel or SPSS and compute like mad, looking for some morsel of interestingness, pasting graphs and tables into a Word document occasionally. If I found something that looked useful, I had no way to reproduce what I’d done, and no way to know if there was a mistake in that long chain of reasoning. The discipline of writing good reports and creating a data audit will greatly improve your understanding of the data and the kind of analysis you’re using. A nice report makes it natural to share with domain experts to assess the reasonableness of the findings. The experts will make good suggestions, find problems you aren’t aware of, and you may get them interested in your project. That’s a good way to learn about their mental maps and look for ways to improve them.\n\n\n\n\nSurveying error may arise due to imperfection or faulty adjustment of the instrument with which measurement is being taken. For example, a tape may be too long or an angle measuring instrument may be out of adjustment. Such errors are known as instrumental errors.\n\nThis describes a difference between the real world and our measurements of it, a misrepresentation of the state of the universe that may bias the results or lead to dubious conclusions. The article on the spreadsheet error cited earlier is entitled “That Reinhart and Rogoff Committed a Spreadsheet Error Completely Misses the Point,” arguing that the technical blunder was irrelevant because of the instrumental error, e.g.\n\nThe idea that GDP and debt statistics from mid-19th century Spain should be an empirical basis for modern economic policy – with an accuracy as fine-tuned as two percent of GDP - is ludicrous on its face.\n\nThere seems to be a cognitive bias to accept without question substitutes that are available as proxies for the thing we actually want to understand. I’ll call that the representation trap. For example, the authors of Academically Adrift accepted the results of a test[^ Collegiate Learning Assessment, or CLA.] on “critical thinking” as a basis for their claims that students weren’t learning much in college. Even a casual look at that test shows that it has no overlap with most college majors (biology, English, engineering, etc.). So the test doesn’t primarily measure what students learn in college. Moreover, the authors ignore the strong correlation between the “critical thinking” test and intelligence tests. If the psychologists are correct that IQ is relatively fixed, then we wouldn’t expect much change in it. This is not to claim that their conclusions are wrong, just that their method doesn’t offer much support for their conclusions[^ The reception by the press showed there wasn’t much critical thinking going on among those presumably college-educated reporters, so maybe they proved their point after all.], probably because of this “substitution bias.” This problem seems to be ubiquitous in higher education.\nSome amount of instrumental error is probably inevitable. We don’t have a perfect representation of the universe, and the further we get from physical dimensions, the worse that representation is. We can measure how much a person weights with very little instrumentation error, but if we want to measure how happy they are, it’s different. If we fall into the substitution trap, we just ask them how happy they are on a scale of one to ten. It’s fine to use that data, but we shouldn’t treat it as if it perfectly represents the glow of happiness we associate with the word [^ See Dan Gilbert and others for actual research on happiness.]. The first step would be to see if the measure is reliable, e.g. by using inter-rater statistics. If the measures can be shown to be non-random, then there are techniques developed by the educational testing experts (Brennan et al., 2006) to see if we can figure how how this measure relates to others. Does the happiness measure correlate with lifespan or income? That aligns with expectations. Does it correlate with drug use? Asking such questions can create a web inter-relationships that clarify meaning. It’s easy to forget that this is how the natural sciences converged to reliable measures and theories. I recommend reading that history. Maybe start with Steven Shapin’s Scientific Revolution.\n\n\n\n\nError in surveying may also be due to variations in natural phenomena such as temperature, humidity, gravity, wind, refraction and magnetic declination. If they are not properly observed while taking measurements, the results will be incorrect. For example, a tape may be 20 meters at 200C but its length will change if the field temperature is different.\n\nBesides blunders and biases, the representation trap, and reliability and validity concerns, there’s the practical nature of modeling. Our goal is to reduce the complexity of the data to a meaningful, usually parsimoneous, description. That reduction is a kind of data compression, and it entails compromise. As such, it’s akin to story-telling, where we don’t include every detail, don’t show every photo of our trip, and maybe adjust the truth slightly in service of the dramatic arc.\nAn analog of natural errors in surveying is estimation or residual error in regression models. Once our model has squeezed all the pattern out of the data, the random-looking bits that are left are the natural error. The error can have some pattern left in it, which means our model can be improved, or the error may be “random,” usually meaning normally distributed with mean zero. For the remainder of the discussion, I’ll use the more familiar term “residual error.” The analysis of residuals is comfortably within the domain of statistics. One danger is that we oversell the apparent precision of statistical measures of fit and forget about the other types of error discussed earlier. This is particularly true when we communicate results to non-statisticians, who may over-value the math, mistaking arcane-seeming formulations for truth.\nUsually we want to minimize residual error by finding the right model. But this is a trade-off between describing the exact data set we have and describing the imagined source of the data. This is commonly called the bias-variance tradeoff, and leads to regularization, balancing local explanations with global ones, e.g. with a hierarchical Bayesian model (McElreath, 2020).\nIn our work, natural error is often sampling error, the left-over part after we try to explain the finite ratings with our fitted model (residuals). Whereas instrumental error is a representation problem between the data and the model we map it to, the natural error is the part of the data that can’t be accounted for even with a perfectly good model. These are related. For example, if we predict college outcomes like graduation using student GPAs (cumulative grade averages), we need to use a polynomial expression for GPA, because the relationship isn’t linear (e.g. using a logistic model). If we naively use a simple linear expression for GPA, the predictions (fitted values from the regression) will be biased for high-GPA students, and the “natural errors,” or residuals will have a distinct shape to them. In fact, that’s one way to tell that the model needs to be adjusted. The point is that natural error and instrumental error are considered together as part of the statistical analysis, whereas personal error is entirely different.\nFor t-a-p models, \\(1-a = \\bar{a}\\) is the chance of random assignment, which can be considered a measure of natural error.\nOne reason to do validity checking, including assessing model fit, is to avoid conclusions that are misleading, maybe even the opposite of the truth. See Gelman & Carlin (2014) for a discussion of “type-S” and “type-M” errors. The nature of statistics is that we don’t know anything for certain, so we often try to quantify the chance of a wrong conclusion given our data and model, or Pr(bad conclusion|data, model). The idea is to “do no harm” by avoiding conclusions that leave our understanding worse off than before.\n\n\n\n\nThe following steps may help you think through the analysis of binary t-a-p models and reduce Pr(bad conclusion).\n\nCheck Assumptions. Validity testing begins with assumptions about the data-gathering process, so it’s essential to seek expertise with the data type to avoid personal errors. If you didn’t generate the data, talk to whoever did. Ask questions that will build confidence that the encodings are valid, the data format reflects your understanding, and the statistical assumptions. In the formal t-a-p model, the most important assumption to check is the independence of raters. If a group of faculty who are scoring student writing portfolios are sitting around a table discussing the essays, the ratings are not independent. Optionally, use prior experience or literature to write down your expectations for \\(t\\) and \\(a\\). This is so that your ‘surprise factor’ doesn’t get biased by seeing the results first.\nCompute the three-parameter t-a-p solution with params = fit_counts(as_counts(ratings)) and inspect for reasonableness (given your priors) and degeneracy (any coefficient of zero or one).\nCompute the Dunning-Kruger horizon for your data configuration using the number of subjects as \\(N_s\\) and average raters per subject as \\(N_r\\). This gives a value for accuracy, below which any estimate is suspect. Suppose the DK number comes back as .3, using a 98% threshold, and the accuracy estimate for your data is .25. We should double-check the validity of the data to make sure it’s not being scrambled inadvertantly.\n\n2.1 If the estimated accuracy is below the DK horizon, fit the model using the MCMC method and inspect the distribution of posterior samples for accuracy. Look for a reasonable range of values that represents uncertainty. If this includes values that are unacceptable, then you may not be able to proceed further. It’s an indication that you need more samples or more reliable ratings or more raters, in some combination. You can simulate these combinations as a kind of power test.\n\nUse the t-a-p parameters to estimate the \\(t_u\\) truth probabilities and rater \\(a_j\\) and \\(p_j\\) parameters using rating_params = fit_ratings(ratings) and compare the model fit for this hierarchical model to the three-parameter version using model_fit_comparison(rating_params)$plot. This uses the estimated parameters to simulate new ratings to see if the original data set looks like the simulated ones, an indication that the model represents the data. We’re looking for two things here. One is whether the hierarchical model has better fit (fewer bits/rating) than the average model. If not, we should probably stick with the three-parameter model to avoid overfitting. The second thing to inspect is the relationship to the simulated likelihood (bits/rating) and the dashed line indicating the likelihood for the actual data set. Ideally this line will fall well within the bell shape of the simulation distribution. That indicates that the original data is not unusual with respect to the data sets generated by those parameters.\nUse the rating_params, which have all the hierarchical coefficients, to generate new sample ratings and then see if the solver can properly estimate those coefficients. You can generate boxplots for accuracy with estimate_rater_parameter_error(rating_params)$plot_a. This gives us an idea of how much the sampling error combined with estimation error affect the estimates.\nOptionally use Bayesian models to further refine parameters and their likely distributions. The shinystan package has interactive tools for detailed analysis.\n\nThis process isn’t intended to produce a single statistic that tells us if the model is “good” or not. Rather, it’s intended to be a work flow that allows an analyst to develop an intuition about the data and models to aid in judgments about applications of the ratings. At all points, it’s necessary to apply judgment to test the results for reasonableness with respect to the subjects, raters, and rating processes that produced the data.\n\n\n\nAs a simple illustration of model assessment, I’ll start with a simulated data set where all the raters and subjects have the same average parameters, subject only to sampling error. The data set is generated with 600 subjects and 10 raters each (the same as the ptsd data, which we’ll consider next), with \\(t = .5\\), \\(a = .7\\), and \\(p = .2\\). The raters are significantly biased away from Class 1. One purpose of this example is to inspire some confidence in the method, since we already know the parameter values. Another purpose is to show how we can investigate a parameter set by simulating values from it to make sure we can recover them. This can be done, for example, after estimating parameters from a real rating set.\n\n\nHere, we know the real values of the parameters, so our expectations are exact. We can compute the three-parameter model with the functions provided in the tapModel package.\n\n\nShow the code\nset.seed(1234)\n\nratings &lt;- tapModel::generate_sample_ratings(N_s = 200, N_r = 10,\n                                             params = list(t = .5, a = .65, p = .8),\n                                             details = TRUE)\n\nparams &lt;- tapModel::fit_counts(as_counts(ratings))\n\nparams |&gt; select(t, a, p) |&gt; kable(digits = 2)\n\n\n\n\nTable 1: Model fit for 20 uniform raters on 200 subjects, showing the fitted parameters for the average t-a-p model with simulated ratings where t = .5, a = .65, and p = .8.\n\n\n\n\n\n\nt\na\np\n\n\n\n\n0.51\n0.66\n0.8\n\n\n\n\n\n\n\n\nThe parameters don’t look degenerate (close to zero or one).\n\n\n\nThe next step is to find the Dunning-Kruger threshold for this data configuration. Recall that this is a reframing of the null hypothesis \\(p\\)-value to emphasize ignorance.\n\n\nShow the code\ntapModel::simulate_exact_fit(200,10,n_sim = 50, output = \"plot\")\n\n\n\n\n\n\n\n\nFigure 1: Ranges of accuracy estimates for nominal t = p = .5 samples.\n\n\n\n\n\nThe boxplots in Figure 1 show that the DK-horizon is low, maybe around \\(a=.15\\) as the point past which we can’t trust accuracy estimates. If we want, we can put numbers to this by simulating \\(a=0\\) to see how large the accuracy estimates can be from sampling error.\n\n\nShow the code\ndk &lt;- dk_horizon(200, 10, n_sim = 200)\ndk |&gt; \n  as.tibble() |&gt;  \n  mutate(Threshold = names(dk)) |&gt; \n  select(Threshold , Accuracy = value) |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 2: Estimated percentiles for accuracy, given 600 subjects with ten raters each, when accuracy is actually zero.\n\n\n\n\n\n\nThreshold\nAccuracy\n\n\n\n\n50%\n0.00\n\n\n75%\n0.08\n\n\n90%\n0.11\n\n\n95%\n0.13\n\n\n98%\n0.15\n\n\n\n\n\n\n\n\nFor a data set of this size, 98% of the randomly generated ratings resulted in an accuracy parameter estimate of .15 or less. It’s reasonable to be suspicious of parameter estimates that have \\(a\\) below that threshold. We might be being fooled by sampling and estimation error into thinking the raters are much more accurate than they actually are. In this case, with an estimate of about .65 (the true value), we easily pass the DK-threshold test.\n\n\n\nNext we assess the likelihood in units of bits/rating to compare the three-parameter model to the more detailed hierarchical model.\n\n\nShow the code\nrating_params &lt;- tapModel::fit_ratings(ratings)\n\ntapModel::model_fit_comparison(rating_params, model_names = c(\"hierarchical\",\"average\"))$plot\n\n\n\n\n\n\n\n\nFigure 2: Model fit for uniform raters, showing the simulated distribution of bits per rating for the average and hierarchical t-a-p models with simulated ratings where t = .5, a = .7, and p = .2. The vertical lines shows the bits per rating for each model with the original data set, and the densities are for data sets simulated from the respective parameters.\n\n\n\n\n\nThe model fit in Figure 2 shows overlapping distributions of log likelihoods for the hierarchical and average (three-parameter) t-a-p models. Recall that better fit for a model means a smaller value of bits/rating (hence larger likelihood). Here there’s no difference between the two models, so we can conclude that the hierarchical model is adding parameters with no benefit. In this case, we would choose the average three-parameter t-a-p model as the most parsimonious description of the data. Because this is simulated data, we know that’s the correct conclusion.\n\n\n\nFor this example, we already knew the parameter specification, and the goodness-of-fit process confirmed that the estimate was probably trustable. Moreover, it confirmed that there’s no advantage to choosing a more complicated hierarchical model.\nI chose the sample size and t-a-p parameters for this example because they also correspond to the PTSD data’s specifications. We’ll now turn to that real data set.\n\n\n\n\nThe PTSD data set has unknown true parameter values. This is not my research data, so I don’t have intuition about what reasonable answers might look like.\n\n\nThe first step is to estimate the average t-a-p parameters and compare to the DK-threshold.\n\n\nShow the code\ndata(ptsd)\n\n# convert Q2 to long form\nratings &lt;- ptsd |&gt;\n  filter(Question == \"Q2\") |&gt;\n  select(-Question) |&gt;\n  gather(RaterID, Rating, -SubjectID) |&gt;\n  mutate(rater_id = as.integer(str_extract(RaterID,\"\\\\d+\")),\n         rating  = Rating - 1) |&gt;\n  select(subject_id = SubjectID, rating, rater_id)\n\nparams &lt;- tapModel::fit_counts(as_counts(ratings))\n\nparams |&gt; select(t, a, p) |&gt; kable(digits = 2)\n\n\n\n\nTable 3: t-a-p model fit the PTSD data.\n\n\n\n\n\n\nt\na\np\n\n\n\n\n0.54\n0.65\n0.81\n\n\n\n\n\n\n\n\nWe already calculated the DK-horizon (step 2) in the previous example. The accuracy estimate of \\(a=.65\\) is signficantly higher than the DK-threshold of .15 that we saw in Table 2.\n\n\n\nIf we include parameters for subjects and raters, does model fit improve?\n\n\nShow the code\n# generate individual parameters for subjects, raters, ratings\nrating_params &lt;- tapModel::fit_ratings(ratings)\n\nset.seed(123)\ntapModel::model_fit_comparison(rating_params, model_names = c(\"hierarchical\",\"average\"))$plot\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nUnlike the case in Figure 2, the model fit for the PTSD data set in Figure 3 shows that the hierarchical rater model is a much better fit than the average model. The bits per rating for the individual model is .46, while the average model is .57. Raters probably vary in their accuracy and bias since they are classifying complex evidence. If we had the original data we could examine in detail the classifications made by the raters with lowest and highest accuracy, for example by getting more ratings from a few experts. This kind of information could be useful in improving classifications generally.\nIn both distributions in Figure 3, the dashed line showing the bits/rating for the actual data is somewhat to the left of the mean of the distribution of the bits/rating from the simulated ratings. The parameters are generated from the given data, so we’d expect that the original data set would be a somewhat better fit for the model than data that’s randomly generated from the parameters. That’s the case here, but this “overfitting” subjectively doesn’t look too bad. It would be different if the dashed blue line were way off to the left of the distribution. That would indicate a degree of overfitting (or other problem) that we should follow up on.\n\n\nShow the code\ndata(ptsd)\n\nraw_ratings &lt;- ptsd\n\n# get the ratings for question 2\nformatted_ratings &lt;- tapModel::format_raw_ratings(raw_ratings,\n                                        rating_cols = str_c(\"Rater \",1:10),\n                                        rating_colnames_type = \"rater\",\n                                        subject_id_col = \"SubjectID\",\n                                        rater_id_col = NULL,\n                                        prune = FALSE) \n\nratings &lt;- formatted_ratings |&gt; \n  filter(Question == \"Q2\") |&gt; \n  select(-Question) |&gt; \n  tapModel::as_binary_ratings(in_class = \"2\")\n  \n# compute the rating parameters, t_i, a_j, p_j\nrating_params &lt;- tapModel::fit_ratings(ratings)\n\n# extract the rater parameters\nrating_params |&gt; \n  group_by(rater_id) |&gt; \n  summarize(c_j = mean(rating),\n            a_j = first(a),\n            p_j = first(p),\n            bias = mean(p - t)) |&gt; \n  arrange(a_j) |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 4: Rater parameters, showing the ID of the rater from the original PTSD data with individual rater statistics: c_j is the rate of class 1 assignments; a_j and p_j are the t-a-p model parameters for this rater; and bias is mean(p_j - t_i).\n\n\n\n\n\n\nrater_id\nc_j\na_j\np_j\nbias\n\n\n\n\nRater 1\n0.84\n0.36\n1.00\n0.46\n\n\nRater 5\n0.80\n0.39\n0.96\n0.42\n\n\nRater 4\n0.80\n0.43\n0.99\n0.45\n\n\nRater 2\n0.52\n0.65\n0.48\n-0.05\n\n\nRater 9\n0.66\n0.65\n0.87\n0.33\n\n\nRater 7\n0.54\n0.75\n0.54\n0.00\n\n\nRater 6\n0.46\n0.76\n0.19\n-0.35\n\n\nRater 3\n0.56\n0.79\n0.64\n0.10\n\n\nRater 8\n0.59\n0.84\n0.87\n0.33\n\n\nRater 10\n0.60\n0.87\n0.98\n0.44\n\n\n\n\n\n\n\n\n\n\n\nTo test the robustness of the individual model fit, we can simulate new data sets from the hierarchcial parameters and then see how well we can recover the parameters. This is computationally expensive for large data sets, so I’ll only run 50 of these.\n\n\nShow the code\nset.seed(123)\nerror_est &lt;- estimate_rater_parameter_error(rating_params,n_sim = 50)\nerror_est$plot_a\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\nThe results in Figure 4 show that the individual rater parameters are well recovered from the simulated data sets. The red markers show the proposed accuracy estimates for each rater, and the boxplots show the distribution of recovered values after simulating new ratings and recovering the parameters. We can do the same for the \\(p_j\\) parameters.\n\n\nShow the code\nerror_est$plot_p\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nThe plots in Figure 5 show that the estimates are robust, and that raters 1,4,5, and 10 are quite biased toward assigning Class 1. In practice, this feedback can likely be used to improve ratings.\nThe PTSD data set is interesting because the same raters used three different methods to assess the PTSD condition for the same subjects. This gives us an opportunity to see how the rater accuracy estimates generalize. We might guess that an inaccurate rater would be inaccurate regardless of the particular measure being used.\n\n\nShow the code\ndata(ptsd)\n\nraw_ratings &lt;- ptsd\n\n# get the ratings for question 2\nformatted_ratings &lt;- tapModel::format_raw_ratings(raw_ratings,\n                                        rating_cols = str_c(\"Rater \",1:10),\n                                        rating_colnames_type = \"rater\",\n                                        subject_id_col = \"SubjectID\",\n                                        rater_id_col = NULL,\n                                        prune = FALSE) \n\n# Method 1\nmethod_1 &lt;- formatted_ratings |&gt; \n  filter(Question == \"Q1\") |&gt; \n  select(-Question) |&gt; \n  tapModel::as_binary_ratings(in_class = \"2\") |&gt; \n  tapModel::fit_ratings() |&gt; \n  group_by(rater_id) |&gt; \n  summarize(c_j = mean(rating),\n            a_j = first(a),\n            p_j = first(p),\n            bias = mean(p - t)) |&gt; \n  mutate(Method = \"1\")\n\n# Method 2\nmethod_2 &lt;- formatted_ratings |&gt; \n  filter(Question == \"Q2\") |&gt; \n  select(-Question) |&gt; \n  tapModel::as_binary_ratings(in_class = \"2\") |&gt; \n  tapModel::fit_ratings() |&gt; \n  group_by(rater_id) |&gt; \n  summarize(c_j = mean(rating),\n            a_j = first(a),\n            p_j = first(p),\n            bias = mean(p - t)) |&gt; \n  mutate(Method = \"2\")\n\n# Method 3\nmethod_3 &lt;- formatted_ratings |&gt; \n  filter(Question == \"Q3\") |&gt; \n  select(-Question) |&gt; \n  tapModel::as_binary_ratings(in_class = \"2\") |&gt; \n  tapModel::fit_ratings() |&gt; \n  group_by(rater_id) |&gt; \n  summarize(c_j = mean(rating),\n            a_j = first(a),\n            p_j = first(p),\n            bias = mean(p - t)) |&gt; \n  mutate(Method = \"3\")\n\nrater_stats &lt;- rbind(method_1, method_2, method_3)\n\nrater_stats |&gt; \n  mutate(rater_id = reorder(rater_id, a_j)) |&gt;\n  gather(parameter, estimate, -rater_id, -Method) |&gt; \n  filter(parameter %in% c(\"a_j\",\"c_j\")) |&gt; \n  ggplot(aes(x = estimate, y = rater_id, color = Method)) +\n  geom_point() +\n  facet_wrap(~parameter) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 6: Comparison of rater accuracy over three methods of assessing PTSD, where a_j is the estimated rater accuracy for each method, and c_j is the rater’s average Class 1 rate.\n\n\n\n\n\nThe pattern of \\(a_j\\) estimates over the three evaluation types in Figure 6 show that rater accuracy is consistant, for example with rater 1 consistently among the least accurate, and rater 8 among the best. We can also see that method 2 tends to generate the most accurate results over all the raters. Finally, we can attribute part of the inaccuracy for the low-accuracy raters to over-assigning Class 1. Note that method 2 has more discrimination generally, with smaller raters of Class 1 (the green dots in the right panel are closer to .5).\n\n\n\nThe above analysis shows what the PTSD ratings have impressive accuracy if the model assumptions are met (primarily independence of raters). A hierarchical model is called for, and it shows that the raters have a range of characteristics that might be useful in improving classifications in real-world settings. In particular, some raters are notably more accurate than others. The third method of scoring seems to inflate Class 1 ratings, reducing accuracy generally. The PTSD data analysis shows that we can extract much more meaningful information from the ratings than is possible with the traditional kappa statistics. Note, however, that we only get the full benefit of these new methods if we have rater identification for each rating."
  },
  {
    "objectID": "fit.html#introduction",
    "href": "fit.html#introduction",
    "title": "Chapter 6: Assessing Model Fit",
    "section": "",
    "text": "There are a lot of ways that data analysis can lead us to the wrong conclusion. In analysis I did of a private college’s graduation rates, I discovered that students who got no financial aid from the college were much more likely to drop out. I confidently wrote a proposal showing that increasing financial aid awards judiciously would pay for itself and lead to higher graduation rates, so everyone wins. The enrollment VP interrupted my enthusiastic presentation to tell me that when a students leave, the financial aid office zeros the aid amount in the database after the fact. The reverse causality obviously invalidated my findings. The point of this story is that we should think about the validity of results more broadly than just the statistics we obtain from the data. For a practical treatment of this attitude, see The Scout Mindset by Julia Galef.\nMuch of the statistics taught in 101 classes concerns hypothesis testing, with \\(p\\)-values and so on, that address problems like\n\\[\nPr(\\text{our data}|\\text{bad inputs}),\n\\]\nby which I mean the probability that we are seeing this data set because something went wrong with measurement or data preparation, etc. In this case, the statistical properties of the data are irrelevant. When I was young I made the mistake of browsing my uncle’s medical reference books and discovered that I had symptoms for a number of dreaded diseases. If confusion is a reliable symptom of a stroke, then Pr(confusion|stroke) is fairly high, but for a healthy 14-year-old Pr(stroke|confusion) fortunately is not! If we estimate Pr(stroke|confusion), we should have an idea about Pr(stroke) before we get too excited. Since we probably can’t estimate Pr(bad inputs) in the expression called out above, this isn’t a statistical question as much as a mindset and a judgment call. As a precautionary meausure, we can entertain the possibility that our rating data is merely coin-flips (i.e. “had a stroke”) and compute how likely our data set is under those circumstances. This is equivalent to a zero-accuracy t-a-p model\n\\[\nPr(\\text{ratings data}|t= 1/2, a = 0, p = 1/2).\n\\] This computation gives a lower threshold for accuracy, which I called the Dunning-Kruger horizon. If the estimated t-a-p parameters from the data are safely above this, we can conclude that the chance our data “had a stroke” and is equivalent to coin flips is low. There still might be data integrity issues, but they are not catastrophic. Another kind of analysis is more direct, asking\n\\[\nPr(\\text{erroneous conclusion}|\\text{data and analysis}).\n\\] Such questions can be cast within the set of statistical assumptions we work with to estimate model parameters, like independence of samples and the shape of the probability distribution (a binomial mixture in the case of binary ratings). If we estimate average rater accuracy \\(a = .3\\), we can then ask for a reasonable range of nearby values that might be true given the natural variability of the data. In Bayesian statistics these are called “credible Intervals” to contrast them with “confidence intervals.” We might find that while the average is \\(a=.3\\), there’s a good chance that \\(a&lt;.2\\), which is important to know when conveying results.\nThe surveying profession has a long tradition of classifying errors into types. One description gives the categories of error as personal, instrumental, and natural. I’ll quote from that source in the sections below.\n\n\n\nError may also arise due to want of perfection of human sight in observing and of touch in manipulating instruments. For example, an error may be there in taking the level reading or reading and angle on the circle of a theodolite. Such errors are known as personal errors.\n\nGathering and preparing data for analysis is fraught with potential for mistakes. We might get columns mixed up, treat blanks incorrectly, use an outdated version of a file, misinterpret a definition, have a bug in our code, and so on. It’s common to get data with column headers that don’t mean what they seem to mean, or have exceptions. Like a column for grade point averages that has zeros instead of blanks for missing data. A famous example of an analysis that went wrong is the Excel spreadsheet error in an influential economics article, where a blunder led to bad conclusions.\nIn addition to outright blunders, we can get the wrong answer because we want to. We often prefer one answer over another when we do an analysis. It’s nice to get a clear result that will give us that glow of understanding the world and maybe a pay raise. It’s less fun to find that the analysis isn’t useful and we wasted our time. So we might overlook an inquiry that could cast doubt on a “good” result, but pursue it vigorously for a “bad” one. This is a subtle version of [wireheading](https://en.wikipedia.org/wiki/Wirehead_(science_fiction), taking a shortcut to rewards. Galef’s book, mentioned earlier, is one of a genre of books addressing such cognitive biases. Also see Think Again by Adam Grant, Predictably Irrational by Dan Ariely, and Thinking fast and slow by Daniel Kahneman.\nEven if we avoid the reward shortcuts and other biases in our own work, we still have to deal with biases in the people who will use our results. Benn Stancil has a useful observation about this on his substack\n\nSome decisions are made relatively slowly, with intention, and based on reasoned analysis. But most decisions are made quickly and instinctively, based on heuristics, experience, and some rough estimation of how the company works.\n\nHe argues that analysts mostly produce technical work and stop there, when we should be trying to affect the heuristics of decision-making. To be more effective, analysts should try to update the mental maps of how things work. As an example, in private colleges the “discount rate” is the fraction of tuition that waived upon entry, and it hovers around 50% these days. Over the last decades increasing discount rates are seen as fiscally irresponsible, and the subject of much board angst. However, it’s easy to show that discount rates must increase as a consequence of the business model. There’s a persistant gap here between how things work and the perception of how things work, which causes bad decisions to be made. It’s purely cultural, a baked-in bias about the world that so prevelant that it’s self-perpetuating. That’s the kind of thing that analysts are up against, and it’s ubiquitous.\nIt can be uncomfortable to make the leap from a statistical estimate to a mental map, because it’s no longer deterministic. But in fact, our statistical analysis isn’t deterministic either; we make lots of subjective choices in the process. We’re going to advertise the limitations of the study as the lawyerly fine print. Use that license to venture beyond the statistics into meaning. Darwin (and his advocates) established evolution without knowing what mechanism caused it (genetic heritability).\nMy recommendation is to approach the personal error problem with these strategies. First, we must protect our work from blunders as much as possible. Fully document work as you do it by using quarto documents) or something similar to combine code and text in a transparent and reproducible form. Take pride in the presentation of the work, even if it’s just for yourself, because that attention to detail carries over to the analysis itself. Write the report for yourself in five years. I like to include an appendix with a description of the analytical method and related statistics, like how to intepret logistic regression coefficients. When coding, I like R’s package documentation for writing functions, and use it even when I’m not writing a package, because the declaration of parameters and results at the top of the function definition forces me to think through what I’m doing before I write the code. Write unit tests when it’s likely to prevent bad things. In addition to the analytical report, create an audit report that summarizes the data in various ways, so you can quickly see if something’s amiss.\nAll of this seems like extra work compared to the way I used to do things. I’d start in Excel or SPSS and compute like mad, looking for some morsel of interestingness, pasting graphs and tables into a Word document occasionally. If I found something that looked useful, I had no way to reproduce what I’d done, and no way to know if there was a mistake in that long chain of reasoning. The discipline of writing good reports and creating a data audit will greatly improve your understanding of the data and the kind of analysis you’re using. A nice report makes it natural to share with domain experts to assess the reasonableness of the findings. The experts will make good suggestions, find problems you aren’t aware of, and you may get them interested in your project. That’s a good way to learn about their mental maps and look for ways to improve them.\n\n\n\n\nSurveying error may arise due to imperfection or faulty adjustment of the instrument with which measurement is being taken. For example, a tape may be too long or an angle measuring instrument may be out of adjustment. Such errors are known as instrumental errors.\n\nThis describes a difference between the real world and our measurements of it, a misrepresentation of the state of the universe that may bias the results or lead to dubious conclusions. The article on the spreadsheet error cited earlier is entitled “That Reinhart and Rogoff Committed a Spreadsheet Error Completely Misses the Point,” arguing that the technical blunder was irrelevant because of the instrumental error, e.g.\n\nThe idea that GDP and debt statistics from mid-19th century Spain should be an empirical basis for modern economic policy – with an accuracy as fine-tuned as two percent of GDP - is ludicrous on its face.\n\nThere seems to be a cognitive bias to accept without question substitutes that are available as proxies for the thing we actually want to understand. I’ll call that the representation trap. For example, the authors of Academically Adrift accepted the results of a test[^ Collegiate Learning Assessment, or CLA.] on “critical thinking” as a basis for their claims that students weren’t learning much in college. Even a casual look at that test shows that it has no overlap with most college majors (biology, English, engineering, etc.). So the test doesn’t primarily measure what students learn in college. Moreover, the authors ignore the strong correlation between the “critical thinking” test and intelligence tests. If the psychologists are correct that IQ is relatively fixed, then we wouldn’t expect much change in it. This is not to claim that their conclusions are wrong, just that their method doesn’t offer much support for their conclusions[^ The reception by the press showed there wasn’t much critical thinking going on among those presumably college-educated reporters, so maybe they proved their point after all.], probably because of this “substitution bias.” This problem seems to be ubiquitous in higher education.\nSome amount of instrumental error is probably inevitable. We don’t have a perfect representation of the universe, and the further we get from physical dimensions, the worse that representation is. We can measure how much a person weights with very little instrumentation error, but if we want to measure how happy they are, it’s different. If we fall into the substitution trap, we just ask them how happy they are on a scale of one to ten. It’s fine to use that data, but we shouldn’t treat it as if it perfectly represents the glow of happiness we associate with the word [^ See Dan Gilbert and others for actual research on happiness.]. The first step would be to see if the measure is reliable, e.g. by using inter-rater statistics. If the measures can be shown to be non-random, then there are techniques developed by the educational testing experts (Brennan et al., 2006) to see if we can figure how how this measure relates to others. Does the happiness measure correlate with lifespan or income? That aligns with expectations. Does it correlate with drug use? Asking such questions can create a web inter-relationships that clarify meaning. It’s easy to forget that this is how the natural sciences converged to reliable measures and theories. I recommend reading that history. Maybe start with Steven Shapin’s Scientific Revolution.\n\n\n\n\nError in surveying may also be due to variations in natural phenomena such as temperature, humidity, gravity, wind, refraction and magnetic declination. If they are not properly observed while taking measurements, the results will be incorrect. For example, a tape may be 20 meters at 200C but its length will change if the field temperature is different.\n\nBesides blunders and biases, the representation trap, and reliability and validity concerns, there’s the practical nature of modeling. Our goal is to reduce the complexity of the data to a meaningful, usually parsimoneous, description. That reduction is a kind of data compression, and it entails compromise. As such, it’s akin to story-telling, where we don’t include every detail, don’t show every photo of our trip, and maybe adjust the truth slightly in service of the dramatic arc.\nAn analog of natural errors in surveying is estimation or residual error in regression models. Once our model has squeezed all the pattern out of the data, the random-looking bits that are left are the natural error. The error can have some pattern left in it, which means our model can be improved, or the error may be “random,” usually meaning normally distributed with mean zero. For the remainder of the discussion, I’ll use the more familiar term “residual error.” The analysis of residuals is comfortably within the domain of statistics. One danger is that we oversell the apparent precision of statistical measures of fit and forget about the other types of error discussed earlier. This is particularly true when we communicate results to non-statisticians, who may over-value the math, mistaking arcane-seeming formulations for truth.\nUsually we want to minimize residual error by finding the right model. But this is a trade-off between describing the exact data set we have and describing the imagined source of the data. This is commonly called the bias-variance tradeoff, and leads to regularization, balancing local explanations with global ones, e.g. with a hierarchical Bayesian model (McElreath, 2020).\nIn our work, natural error is often sampling error, the left-over part after we try to explain the finite ratings with our fitted model (residuals). Whereas instrumental error is a representation problem between the data and the model we map it to, the natural error is the part of the data that can’t be accounted for even with a perfectly good model. These are related. For example, if we predict college outcomes like graduation using student GPAs (cumulative grade averages), we need to use a polynomial expression for GPA, because the relationship isn’t linear (e.g. using a logistic model). If we naively use a simple linear expression for GPA, the predictions (fitted values from the regression) will be biased for high-GPA students, and the “natural errors,” or residuals will have a distinct shape to them. In fact, that’s one way to tell that the model needs to be adjusted. The point is that natural error and instrumental error are considered together as part of the statistical analysis, whereas personal error is entirely different.\nFor t-a-p models, \\(1-a = \\bar{a}\\) is the chance of random assignment, which can be considered a measure of natural error.\nOne reason to do validity checking, including assessing model fit, is to avoid conclusions that are misleading, maybe even the opposite of the truth. See Gelman & Carlin (2014) for a discussion of “type-S” and “type-M” errors. The nature of statistics is that we don’t know anything for certain, so we often try to quantify the chance of a wrong conclusion given our data and model, or Pr(bad conclusion|data, model). The idea is to “do no harm” by avoiding conclusions that leave our understanding worse off than before."
  },
  {
    "objectID": "fit.html#a-validation-process-for-binary-ratings",
    "href": "fit.html#a-validation-process-for-binary-ratings",
    "title": "Chapter 6: Assessing Model Fit",
    "section": "",
    "text": "The following steps may help you think through the analysis of binary t-a-p models and reduce Pr(bad conclusion).\n\nCheck Assumptions. Validity testing begins with assumptions about the data-gathering process, so it’s essential to seek expertise with the data type to avoid personal errors. If you didn’t generate the data, talk to whoever did. Ask questions that will build confidence that the encodings are valid, the data format reflects your understanding, and the statistical assumptions. In the formal t-a-p model, the most important assumption to check is the independence of raters. If a group of faculty who are scoring student writing portfolios are sitting around a table discussing the essays, the ratings are not independent. Optionally, use prior experience or literature to write down your expectations for \\(t\\) and \\(a\\). This is so that your ‘surprise factor’ doesn’t get biased by seeing the results first.\nCompute the three-parameter t-a-p solution with params = fit_counts(as_counts(ratings)) and inspect for reasonableness (given your priors) and degeneracy (any coefficient of zero or one).\nCompute the Dunning-Kruger horizon for your data configuration using the number of subjects as \\(N_s\\) and average raters per subject as \\(N_r\\). This gives a value for accuracy, below which any estimate is suspect. Suppose the DK number comes back as .3, using a 98% threshold, and the accuracy estimate for your data is .25. We should double-check the validity of the data to make sure it’s not being scrambled inadvertantly.\n\n2.1 If the estimated accuracy is below the DK horizon, fit the model using the MCMC method and inspect the distribution of posterior samples for accuracy. Look for a reasonable range of values that represents uncertainty. If this includes values that are unacceptable, then you may not be able to proceed further. It’s an indication that you need more samples or more reliable ratings or more raters, in some combination. You can simulate these combinations as a kind of power test.\n\nUse the t-a-p parameters to estimate the \\(t_u\\) truth probabilities and rater \\(a_j\\) and \\(p_j\\) parameters using rating_params = fit_ratings(ratings) and compare the model fit for this hierarchical model to the three-parameter version using model_fit_comparison(rating_params)$plot. This uses the estimated parameters to simulate new ratings to see if the original data set looks like the simulated ones, an indication that the model represents the data. We’re looking for two things here. One is whether the hierarchical model has better fit (fewer bits/rating) than the average model. If not, we should probably stick with the three-parameter model to avoid overfitting. The second thing to inspect is the relationship to the simulated likelihood (bits/rating) and the dashed line indicating the likelihood for the actual data set. Ideally this line will fall well within the bell shape of the simulation distribution. That indicates that the original data is not unusual with respect to the data sets generated by those parameters.\nUse the rating_params, which have all the hierarchical coefficients, to generate new sample ratings and then see if the solver can properly estimate those coefficients. You can generate boxplots for accuracy with estimate_rater_parameter_error(rating_params)$plot_a. This gives us an idea of how much the sampling error combined with estimation error affect the estimates.\nOptionally use Bayesian models to further refine parameters and their likely distributions. The shinystan package has interactive tools for detailed analysis.\n\nThis process isn’t intended to produce a single statistic that tells us if the model is “good” or not. Rather, it’s intended to be a work flow that allows an analyst to develop an intuition about the data and models to aid in judgments about applications of the ratings. At all points, it’s necessary to apply judgment to test the results for reasonableness with respect to the subjects, raters, and rating processes that produced the data."
  },
  {
    "objectID": "fit.html#example-uniform-raters",
    "href": "fit.html#example-uniform-raters",
    "title": "Chapter 6: Assessing Model Fit",
    "section": "",
    "text": "As a simple illustration of model assessment, I’ll start with a simulated data set where all the raters and subjects have the same average parameters, subject only to sampling error. The data set is generated with 600 subjects and 10 raters each (the same as the ptsd data, which we’ll consider next), with \\(t = .5\\), \\(a = .7\\), and \\(p = .2\\). The raters are significantly biased away from Class 1. One purpose of this example is to inspire some confidence in the method, since we already know the parameter values. Another purpose is to show how we can investigate a parameter set by simulating values from it to make sure we can recover them. This can be done, for example, after estimating parameters from a real rating set.\n\n\nHere, we know the real values of the parameters, so our expectations are exact. We can compute the three-parameter model with the functions provided in the tapModel package.\n\n\nShow the code\nset.seed(1234)\n\nratings &lt;- tapModel::generate_sample_ratings(N_s = 200, N_r = 10,\n                                             params = list(t = .5, a = .65, p = .8),\n                                             details = TRUE)\n\nparams &lt;- tapModel::fit_counts(as_counts(ratings))\n\nparams |&gt; select(t, a, p) |&gt; kable(digits = 2)\n\n\n\n\nTable 1: Model fit for 20 uniform raters on 200 subjects, showing the fitted parameters for the average t-a-p model with simulated ratings where t = .5, a = .65, and p = .8.\n\n\n\n\n\n\nt\na\np\n\n\n\n\n0.51\n0.66\n0.8\n\n\n\n\n\n\n\n\nThe parameters don’t look degenerate (close to zero or one).\n\n\n\nThe next step is to find the Dunning-Kruger threshold for this data configuration. Recall that this is a reframing of the null hypothesis \\(p\\)-value to emphasize ignorance.\n\n\nShow the code\ntapModel::simulate_exact_fit(200,10,n_sim = 50, output = \"plot\")\n\n\n\n\n\n\n\n\nFigure 1: Ranges of accuracy estimates for nominal t = p = .5 samples.\n\n\n\n\n\nThe boxplots in Figure 1 show that the DK-horizon is low, maybe around \\(a=.15\\) as the point past which we can’t trust accuracy estimates. If we want, we can put numbers to this by simulating \\(a=0\\) to see how large the accuracy estimates can be from sampling error.\n\n\nShow the code\ndk &lt;- dk_horizon(200, 10, n_sim = 200)\ndk |&gt; \n  as.tibble() |&gt;  \n  mutate(Threshold = names(dk)) |&gt; \n  select(Threshold , Accuracy = value) |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 2: Estimated percentiles for accuracy, given 600 subjects with ten raters each, when accuracy is actually zero.\n\n\n\n\n\n\nThreshold\nAccuracy\n\n\n\n\n50%\n0.00\n\n\n75%\n0.08\n\n\n90%\n0.11\n\n\n95%\n0.13\n\n\n98%\n0.15\n\n\n\n\n\n\n\n\nFor a data set of this size, 98% of the randomly generated ratings resulted in an accuracy parameter estimate of .15 or less. It’s reasonable to be suspicious of parameter estimates that have \\(a\\) below that threshold. We might be being fooled by sampling and estimation error into thinking the raters are much more accurate than they actually are. In this case, with an estimate of about .65 (the true value), we easily pass the DK-threshold test.\n\n\n\nNext we assess the likelihood in units of bits/rating to compare the three-parameter model to the more detailed hierarchical model.\n\n\nShow the code\nrating_params &lt;- tapModel::fit_ratings(ratings)\n\ntapModel::model_fit_comparison(rating_params, model_names = c(\"hierarchical\",\"average\"))$plot\n\n\n\n\n\n\n\n\nFigure 2: Model fit for uniform raters, showing the simulated distribution of bits per rating for the average and hierarchical t-a-p models with simulated ratings where t = .5, a = .7, and p = .2. The vertical lines shows the bits per rating for each model with the original data set, and the densities are for data sets simulated from the respective parameters.\n\n\n\n\n\nThe model fit in Figure 2 shows overlapping distributions of log likelihoods for the hierarchical and average (three-parameter) t-a-p models. Recall that better fit for a model means a smaller value of bits/rating (hence larger likelihood). Here there’s no difference between the two models, so we can conclude that the hierarchical model is adding parameters with no benefit. In this case, we would choose the average three-parameter t-a-p model as the most parsimonious description of the data. Because this is simulated data, we know that’s the correct conclusion.\n\n\n\nFor this example, we already knew the parameter specification, and the goodness-of-fit process confirmed that the estimate was probably trustable. Moreover, it confirmed that there’s no advantage to choosing a more complicated hierarchical model.\nI chose the sample size and t-a-p parameters for this example because they also correspond to the PTSD data’s specifications. We’ll now turn to that real data set."
  },
  {
    "objectID": "fit.html#example-ptsd-data",
    "href": "fit.html#example-ptsd-data",
    "title": "Chapter 6: Assessing Model Fit",
    "section": "",
    "text": "The PTSD data set has unknown true parameter values. This is not my research data, so I don’t have intuition about what reasonable answers might look like.\n\n\nThe first step is to estimate the average t-a-p parameters and compare to the DK-threshold.\n\n\nShow the code\ndata(ptsd)\n\n# convert Q2 to long form\nratings &lt;- ptsd |&gt;\n  filter(Question == \"Q2\") |&gt;\n  select(-Question) |&gt;\n  gather(RaterID, Rating, -SubjectID) |&gt;\n  mutate(rater_id = as.integer(str_extract(RaterID,\"\\\\d+\")),\n         rating  = Rating - 1) |&gt;\n  select(subject_id = SubjectID, rating, rater_id)\n\nparams &lt;- tapModel::fit_counts(as_counts(ratings))\n\nparams |&gt; select(t, a, p) |&gt; kable(digits = 2)\n\n\n\n\nTable 3: t-a-p model fit the PTSD data.\n\n\n\n\n\n\nt\na\np\n\n\n\n\n0.54\n0.65\n0.81\n\n\n\n\n\n\n\n\nWe already calculated the DK-horizon (step 2) in the previous example. The accuracy estimate of \\(a=.65\\) is signficantly higher than the DK-threshold of .15 that we saw in Table 2.\n\n\n\nIf we include parameters for subjects and raters, does model fit improve?\n\n\nShow the code\n# generate individual parameters for subjects, raters, ratings\nrating_params &lt;- tapModel::fit_ratings(ratings)\n\nset.seed(123)\ntapModel::model_fit_comparison(rating_params, model_names = c(\"hierarchical\",\"average\"))$plot\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nUnlike the case in Figure 2, the model fit for the PTSD data set in Figure 3 shows that the hierarchical rater model is a much better fit than the average model. The bits per rating for the individual model is .46, while the average model is .57. Raters probably vary in their accuracy and bias since they are classifying complex evidence. If we had the original data we could examine in detail the classifications made by the raters with lowest and highest accuracy, for example by getting more ratings from a few experts. This kind of information could be useful in improving classifications generally.\nIn both distributions in Figure 3, the dashed line showing the bits/rating for the actual data is somewhat to the left of the mean of the distribution of the bits/rating from the simulated ratings. The parameters are generated from the given data, so we’d expect that the original data set would be a somewhat better fit for the model than data that’s randomly generated from the parameters. That’s the case here, but this “overfitting” subjectively doesn’t look too bad. It would be different if the dashed blue line were way off to the left of the distribution. That would indicate a degree of overfitting (or other problem) that we should follow up on.\n\n\nShow the code\ndata(ptsd)\n\nraw_ratings &lt;- ptsd\n\n# get the ratings for question 2\nformatted_ratings &lt;- tapModel::format_raw_ratings(raw_ratings,\n                                        rating_cols = str_c(\"Rater \",1:10),\n                                        rating_colnames_type = \"rater\",\n                                        subject_id_col = \"SubjectID\",\n                                        rater_id_col = NULL,\n                                        prune = FALSE) \n\nratings &lt;- formatted_ratings |&gt; \n  filter(Question == \"Q2\") |&gt; \n  select(-Question) |&gt; \n  tapModel::as_binary_ratings(in_class = \"2\")\n  \n# compute the rating parameters, t_i, a_j, p_j\nrating_params &lt;- tapModel::fit_ratings(ratings)\n\n# extract the rater parameters\nrating_params |&gt; \n  group_by(rater_id) |&gt; \n  summarize(c_j = mean(rating),\n            a_j = first(a),\n            p_j = first(p),\n            bias = mean(p - t)) |&gt; \n  arrange(a_j) |&gt; \n  kable(digits = 2)\n\n\n\n\nTable 4: Rater parameters, showing the ID of the rater from the original PTSD data with individual rater statistics: c_j is the rate of class 1 assignments; a_j and p_j are the t-a-p model parameters for this rater; and bias is mean(p_j - t_i).\n\n\n\n\n\n\nrater_id\nc_j\na_j\np_j\nbias\n\n\n\n\nRater 1\n0.84\n0.36\n1.00\n0.46\n\n\nRater 5\n0.80\n0.39\n0.96\n0.42\n\n\nRater 4\n0.80\n0.43\n0.99\n0.45\n\n\nRater 2\n0.52\n0.65\n0.48\n-0.05\n\n\nRater 9\n0.66\n0.65\n0.87\n0.33\n\n\nRater 7\n0.54\n0.75\n0.54\n0.00\n\n\nRater 6\n0.46\n0.76\n0.19\n-0.35\n\n\nRater 3\n0.56\n0.79\n0.64\n0.10\n\n\nRater 8\n0.59\n0.84\n0.87\n0.33\n\n\nRater 10\n0.60\n0.87\n0.98\n0.44\n\n\n\n\n\n\n\n\n\n\n\nTo test the robustness of the individual model fit, we can simulate new data sets from the hierarchcial parameters and then see how well we can recover the parameters. This is computationally expensive for large data sets, so I’ll only run 50 of these.\n\n\nShow the code\nset.seed(123)\nerror_est &lt;- estimate_rater_parameter_error(rating_params,n_sim = 50)\nerror_est$plot_a\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\nThe results in Figure 4 show that the individual rater parameters are well recovered from the simulated data sets. The red markers show the proposed accuracy estimates for each rater, and the boxplots show the distribution of recovered values after simulating new ratings and recovering the parameters. We can do the same for the \\(p_j\\) parameters.\n\n\nShow the code\nerror_est$plot_p\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nThe plots in Figure 5 show that the estimates are robust, and that raters 1,4,5, and 10 are quite biased toward assigning Class 1. In practice, this feedback can likely be used to improve ratings.\nThe PTSD data set is interesting because the same raters used three different methods to assess the PTSD condition for the same subjects. This gives us an opportunity to see how the rater accuracy estimates generalize. We might guess that an inaccurate rater would be inaccurate regardless of the particular measure being used.\n\n\nShow the code\ndata(ptsd)\n\nraw_ratings &lt;- ptsd\n\n# get the ratings for question 2\nformatted_ratings &lt;- tapModel::format_raw_ratings(raw_ratings,\n                                        rating_cols = str_c(\"Rater \",1:10),\n                                        rating_colnames_type = \"rater\",\n                                        subject_id_col = \"SubjectID\",\n                                        rater_id_col = NULL,\n                                        prune = FALSE) \n\n# Method 1\nmethod_1 &lt;- formatted_ratings |&gt; \n  filter(Question == \"Q1\") |&gt; \n  select(-Question) |&gt; \n  tapModel::as_binary_ratings(in_class = \"2\") |&gt; \n  tapModel::fit_ratings() |&gt; \n  group_by(rater_id) |&gt; \n  summarize(c_j = mean(rating),\n            a_j = first(a),\n            p_j = first(p),\n            bias = mean(p - t)) |&gt; \n  mutate(Method = \"1\")\n\n# Method 2\nmethod_2 &lt;- formatted_ratings |&gt; \n  filter(Question == \"Q2\") |&gt; \n  select(-Question) |&gt; \n  tapModel::as_binary_ratings(in_class = \"2\") |&gt; \n  tapModel::fit_ratings() |&gt; \n  group_by(rater_id) |&gt; \n  summarize(c_j = mean(rating),\n            a_j = first(a),\n            p_j = first(p),\n            bias = mean(p - t)) |&gt; \n  mutate(Method = \"2\")\n\n# Method 3\nmethod_3 &lt;- formatted_ratings |&gt; \n  filter(Question == \"Q3\") |&gt; \n  select(-Question) |&gt; \n  tapModel::as_binary_ratings(in_class = \"2\") |&gt; \n  tapModel::fit_ratings() |&gt; \n  group_by(rater_id) |&gt; \n  summarize(c_j = mean(rating),\n            a_j = first(a),\n            p_j = first(p),\n            bias = mean(p - t)) |&gt; \n  mutate(Method = \"3\")\n\nrater_stats &lt;- rbind(method_1, method_2, method_3)\n\nrater_stats |&gt; \n  mutate(rater_id = reorder(rater_id, a_j)) |&gt;\n  gather(parameter, estimate, -rater_id, -Method) |&gt; \n  filter(parameter %in% c(\"a_j\",\"c_j\")) |&gt; \n  ggplot(aes(x = estimate, y = rater_id, color = Method)) +\n  geom_point() +\n  facet_wrap(~parameter) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 6: Comparison of rater accuracy over three methods of assessing PTSD, where a_j is the estimated rater accuracy for each method, and c_j is the rater’s average Class 1 rate.\n\n\n\n\n\nThe pattern of \\(a_j\\) estimates over the three evaluation types in Figure 6 show that rater accuracy is consistant, for example with rater 1 consistently among the least accurate, and rater 8 among the best. We can also see that method 2 tends to generate the most accurate results over all the raters. Finally, we can attribute part of the inaccuracy for the low-accuracy raters to over-assigning Class 1. Note that method 2 has more discrimination generally, with smaller raters of Class 1 (the green dots in the right panel are closer to .5).\n\n\n\nThe above analysis shows what the PTSD ratings have impressive accuracy if the model assumptions are met (primarily independence of raters). A hierarchical model is called for, and it shows that the raters have a range of characteristics that might be useful in improving classifications in real-world settings. In particular, some raters are notably more accurate than others. The third method of scoring seems to inflate Class 1 ratings, reducing accuracy generally. The PTSD data analysis shows that we can extract much more meaningful information from the ratings than is possible with the traditional kappa statistics. Note, however, that we only get the full benefit of these new methods if we have rater identification for each rating."
  }
]